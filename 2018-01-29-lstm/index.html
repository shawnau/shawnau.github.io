<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>循环神经网络的前世今生 - Laniakea</title><meta name="Description" content="xxuan&#39;s blog"><meta property="og:title" content="循环神经网络的前世今生" />
<meta property="og:description" content="
新年第一篇, 和永无止境的八月一样循环的神经网络(其实是存货" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://xxuan.cc/2018-01-29-lstm/" /><meta property="og:image" content="https://xxuan.cc/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-01-29T15:27:59+00:00" />
<meta property="article:modified_time" content="2023-02-14T23:37:00+08:00" /><meta property="og:site_name" content="xxuan.cc" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://xxuan.cc/logo.png"/>

<meta name="twitter:title" content="循环神经网络的前世今生"/>
<meta name="twitter:description" content="
新年第一篇, 和永无止境的八月一样循环的神经网络(其实是存货"/>
<meta name="application-name" content="Laniakea">
<meta name="apple-mobile-web-app-title" content="Laniakea"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://xxuan.cc/2018-01-29-lstm/" /><link rel="prev" href="https://xxuan.cc/2017-12-30-binarytreetraverse/" /><link rel="next" href="https://xxuan.cc/2018-02-03-hmm/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "循环神经网络的前世今生",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/xxuan.cc\/2018-01-29-lstm\/"
        },"genre": "posts","keywords": "LSTM, Neural Network, NLP","wordcount":  2996 ,
        "url": "https:\/\/xxuan.cc\/2018-01-29-lstm\/","datePublished": "2018-01-29T15:27:59+00:00","dateModified": "2023-02-14T23:37:00+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "xiaoxuan"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Laniakea">Laniakea</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/" title="存档脑细胞"> 文章 </a><a class="menu-item" href="/playground/" title="玩耍算法题的地方"> 练习场 </a><a class="menu-item" href="/code/" title="存放大概率过期的代码"> 码农 </a><a class="menu-item" href="/tags/" title="标签"> Tags </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Laniakea">Laniakea</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="存档脑细胞">文章</a><a class="menu-item" href="/playground/" title="玩耍算法题的地方">练习场</a><a class="menu-item" href="/code/" title="存放大概率过期的代码">码农</a><a class="menu-item" href="/tags/" title="标签">Tags</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">循环神经网络的前世今生</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>xiaoxuan</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2018-01-29">2018-01-29</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;2996 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;6 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#standard-rnn">Standard RNN</a></li>
    <li><a href="#lstm">LSTM</a>
      <ul>
        <li><a href="#calculate-cell-state">Calculate Cell State</a></li>
        <li><a href="#transmit-cell-state">Transmit Cell State</a></li>
        <li><a href="#long-term--short-term">Long Term &amp; Short Term</a></li>
        <li><a href="#input-forget-output-gate--cell-state">Input, Forget, Output Gate &amp; Cell State</a></li>
        <li><a href="#peephole-connection">Peephole Connection</a></li>
      </ul>
    </li>
    <li><a href="#lstms-calculation">LSTM&rsquo;s Calculation</a>
      <ul>
        <li><a href="#feedforward">FeedForward</a></li>
        <li><a href="#backward-propagation">Backward Propagation</a></li>
      </ul>
    </li>
    <li><a href="#gru">GRU</a>
      <ul>
        <li><a href="#reset--update-gate">Reset &amp; Update Gate</a></li>
        <li><a href="#remember--forget-in-one-shot">Remember &amp; Forget in One Shot</a></li>
      </ul>
    </li>
    <li><a href="#pytorch-implementation">Pytorch implementation</a>
      <ul>
        <li><a href="#model">Model</a></li>
        <li><a href="#batch">Batch</a></li>
      </ul>
    </li>
    <li><a href="#appendix">Appendix</a>
      <ul>
        <li><a href="#gradient-of-activate-funtions">Gradient of Activate Funtions</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="http://my-imgshare.oss-cn-shenzhen.aliyuncs.com/RNN3.png"
        data-srcset="http://my-imgshare.oss-cn-shenzhen.aliyuncs.com/RNN3.png, http://my-imgshare.oss-cn-shenzhen.aliyuncs.com/RNN3.png 1.5x, http://my-imgshare.oss-cn-shenzhen.aliyuncs.com/RNN3.png 2x"
        data-sizes="auto"
        alt="http://my-imgshare.oss-cn-shenzhen.aliyuncs.com/RNN3.png"
        title="http://my-imgshare.oss-cn-shenzhen.aliyuncs.com/RNN3.png" />
新年第一篇, 和永无止境的八月一样循环的神经网络(其实是存货</p>
<h2 id="standard-rnn">Standard RNN</h2>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png"
        data-srcset="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png, http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png 1.5x, http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png 2x"
        data-sizes="auto"
        alt="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png"
        title="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" /></p>
<p>Standard RNN结构如上图, 主要涉及:</p>
<ol>
<li>hidden state的更新</li>
<li>输出值y的计算</li>
</ol>
<p>(本文忽略偏置项)
$$
\begin{aligned}
h^{(t)} &amp;= tanh(Wx^{(t)} + Uh^{(t-1)}) \\
y^{(t)} &amp;= softmax(Vh^{(t)})
\end{aligned}
$$</p>
<h2 id="lstm">LSTM</h2>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png"
        data-srcset="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png, http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png 1.5x, http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png 2x"
        data-sizes="auto"
        alt="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png"
        title="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" /></p>
<p>LSTM(Long Short-Term Memory)是RNN的变种, 他的诞生要追溯到1997年. Hochreiter S &amp; Schmidhuber J (2000) 在Neural Computation上提出了该网络结构. 没错, 是Neural Computation XD
LSTM意在解决RNN无法传递长程状态的缺陷, 其核心概念是Cell State</p>
<h3 id="calculate-cell-state">Calculate Cell State</h3>
<p>利用cell state来携带长程信息, 记为$c^{(t)}$. 在当前时间步$t$利用输入$x^{(t)}$和前一步的$h^{(t-1)}$产生该时刻的长程信息${\hat c}^{(t)}$, 该层也可以称作cell state层:
$$
\hat c^{(t)} = tanh(W_c x^{(t)} + U_c h^{(t-1)})
\tag{1.1}
$$</p>
<h3 id="transmit-cell-state">Transmit Cell State</h3>
<ol>
<li>添加长程信息的频率应该是很低的, 因此设置<strong>输入门</strong>$g_{in}$挑选长程信息加入cell state: $g_{in} \otimes {\hat c}^{(t)}$</li>
<li>如果长程信息设为$c^{(t)} = c^{(t-1)} + g_{in} \otimes {\hat c}^{(t)}$, 总是保留旧信息$c^{(t-1)}$会让$c^{(t)}$迅速饱和导致梯度爆炸, 无法传递长程信息. 因此需要设置<strong>遗忘门</strong>$g_{f}$控制旧cell state的去留: $g_{f} \otimes c^{(t-1)}$
$$c^{(t)} = g_{f} \otimes c^{(t-1)} + g_{in} \otimes {\hat c}^{(t)} \tag{1.2}$$</li>
<li>最后利用<strong>输出门</strong>$g_{out}$从cell state中选择合适的输出:
$$ h^{(t)} =  g_{out} \otimes f(c^{(t)}) \tag{1.3}$$</li>
</ol>
<h3 id="long-term--short-term">Long Term &amp; Short Term</h3>
<p>什么是LSTM中的Long Term &amp; Short Term?</p>
<ol>
<li>回溯cell state的计算图, 发现$c^{(t)}$的更新只涉及到element wise乘法和加法, 梯度可以稳定传播, 属于长程(Long Term)信息</li>
<li>回溯hidden state的计算图, 发现$h^{(t)}$的计算取决于$c^{(t)} $, 而$c^{(t)}$的计算取决于$Uh^{(t-1)}$, 故反向传播需要跨越$U$. 和RNN一样易导致梯度消失, $h^{(t)}$属于短程(Short Term)信息</li>
</ol>
<h3 id="input-forget-output-gate--cell-state">Input, Forget, Output Gate &amp; Cell State</h3>
<p>输入门, 遗忘门, 输出门和Cell State都是根据当前输入$x^{(t)}$和前一步输出$h^{(t-1)}$决定, 这是可以并行计算的四层神经网络, 他们的权重用下标加以区分
$$
\begin{aligned}
\hat c^{(t)} &amp;= tanh(W_c x^{(t)} + U_c h^{(t-1)}) \\
g_{in} &amp;= sigmoid(W_i x^{(t)} + U_i h^{(t-1)}) \\
g_f &amp;= sigmoid(W_f x^{(t)} + U_f h^{(t-1)}) \\
g_{out} &amp;= sigmoid(W_o x^{(t)} + U_o h^{(t-1)}) \\
\end{aligned}
\tag{1.4}
$$</p>
<p>这四层网络可以直接用一个矩阵表示:</p>
<p>$$
\begin{aligned}
\begin{bmatrix}
{\hat c}_r^{(t)} \\
{\hat g}_{in}^{(t)} \\
{\hat g}_f ^{(t)} \\
{\hat g}_{out}^{(t)}
\end{bmatrix}
&amp;=
\begin{bmatrix}
W_c &amp; U_c \\
W_i &amp; U_i \\
W_f &amp; U_f \\
W_o &amp; U_o
\end{bmatrix}
\cdot
\begin{bmatrix}
x^{(t)} \\
h^{(t-1)}
\end{bmatrix}
= W \cdot I^{(t)}
\end{aligned}
\tag{1.5}
$$</p>
<p>其中${\hat c_r}^{(t)}$,  $\hat g_{in}$,  $\hat g_f$,  $\hat g_{out}$代表被激活之前的仿射运算结果, 以下为简洁省略门的时间标记${(t)}$</p>
<h3 id="peephole-connection">Peephole Connection</h3>
<p>四个门的开闭目前仅取决于当前输入$x^{(t)}$和前一步输出$h^{(t-1)}$, 而依上文所叙, $h^{(t-1)}$属于短程信息, 那么所有门控尤其是输出门并不能从长程信息中获益, 这不利于输出门保留长程信息. 因此将前一步的Cell State引入门单元的计算可以提高网络性能.  这项技术依然是LSTM的提出者3年后的工作, <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener noreffer ">Gers &amp; Schmidhuber (2000)</a>.</p>
<p>此时的计算方式为:</p>
<p>$$
\begin{aligned}
\begin{bmatrix}
{\hat c_r}^{(t)} \\
{\hat g_{in}}^{(t)} \\
{\hat g_f} ^{(t)} \\
{\hat g_{out}}^{(t)}
\end{bmatrix}
&amp;=
\begin{bmatrix}
W_c &amp; U_c &amp; V_c \\
W_i &amp; U_i &amp; V_i \\
W_f &amp; U_f &amp; V_f \\
W_o &amp; U_o &amp; V_o
\end{bmatrix}
\cdot
\begin{bmatrix}
x^{(t)} \\
h^{(t-1)} \\
c^{(t-1)}
\end{bmatrix}
\end{aligned}
$$</p>
<h2 id="lstms-calculation">LSTM&rsquo;s Calculation</h2>
<h3 id="feedforward">FeedForward</h3>
<p>假设输入向量为 $\vec x \in R^n$, 隐变量$\vec h \in R^d$, 则$(1.5)$中, $W_* \in R^{d \times n}$, $U_* \in R^{d \times d}$, $W \in R^{4d \times (n+d)}$</p>
<p>$$
\begin{bmatrix}
{\hat c_r}^{(t)} \\
{\hat g_i} \\
{\hat g_f} \\
{\hat g_o}
\end{bmatrix}
= \begin{bmatrix}
W_c &amp; U_c \\
W_i &amp; U_i \\
W_f &amp; U_f \\
W_o &amp; U_o
\end{bmatrix}
\cdot
\begin{bmatrix}
x^{(t)} \\
h^{(t-1)}
\end{bmatrix}
= W \cdot I^{(t)}
\tag{2.1}
$$</p>
<p>$$
\begin{bmatrix}
{\hat c}^{(t)} \\
g_i \\
g_f  \\
g_o
\end{bmatrix}
= \begin{bmatrix}
tanh({\hat c}_r^{(t)}) \\
sigmoid(\hat g_i) \\
sigmoid(\hat g_f)  \\
sigmoid(\hat g_o)
\end{bmatrix}
\tag{2.2}
$$</p>
<p>$$
c^{(t)} = g_{f} \otimes c^{(t-1)} + g_i \otimes {\hat c}^{(t)}
\tag{2.3}
$$</p>
<p>$$
h^{(t)} =  g_o \otimes tanh(c^{(t)}) \tag{2.4}
$$</p>
<h3 id="backward-propagation">Backward Propagation</h3>
<p>以下$\delta x$皆代表$\frac{\partial L}{\partial x}$, 其中$L(y, \hat y)$为损失函数</p>
<ol>
<li>由$(2.4)$, 给定$\delta h^{(t)}$, 求$\delta g_o$ , $\delta c^{(t)}$
$$
\begin{aligned}
\delta g_o &amp;= \frac{\partial h^{(t)}}{\partial g_o} \delta h^{(t)} \\
&amp;= tanh(c^{(t)}) \otimes \delta h^{(t)}
\end{aligned}
\tag{2.5}
$$
$$
\begin{aligned}
\delta c^{(t)} &amp;= \frac{\partial h^{(t)}}{\partial  tanh(c^{(t)})} \frac{\partial  tanh(c^{(t)})}{\partial c^{(t)}} \delta h^{(t)} \\
&amp;= g_o \otimes (1 - tanh^2(c^{(t)})) \otimes \delta h^{(t)}
\end{aligned}
\tag{2.6}
$$</li>
<li>由$(2.3)$, 给定$\delta c^{(t)}$, 求$\delta g_{f}$, $\delta c^{(t-1)}$, $\delta g_i$, $\delta {\hat c}^{(t)}$
$$
\begin{aligned}
\delta g_i &amp;= {\hat c}^{(t)} \otimes \delta c^{(t)} \\
\delta g_f &amp;= c^{(t-1)} \otimes \delta c^{(t)} \\
\delta {\hat c}^{(t)} &amp;= g_i \otimes \delta c^{(t)} \\
\delta c^{(t-1)} &amp;= g_f \otimes \delta c^{(t)} \\
\end{aligned}
\tag{2.7}
$$</li>
<li>由$(2.2)$, 给定$\delta {\hat c}^{(t)}$, $\delta g_i$, $\delta g_{f}$, $\delta g_o$, 求$\delta {\hat c}_r^{(t)}$, $\delta \hat g_i$, $\delta \hat g_f$, $\delta \hat g_o$
$$
\begin{aligned}
\delta \hat g_i &amp;=g_i(1 - g_i) \otimes \delta g_i \\
\delta \hat g_f &amp;=g_f(1 - g_f) \otimes \delta g_f \\
\delta \hat g_o &amp;=g_o(1 - g_o) \otimes \delta g_o \\
\delta {\hat c}_r^{(t)} &amp;= (1 - tanh^2({\hat c}^{(t)})) \otimes \delta {\hat c}^{(t)}
\end{aligned}
\tag{2.8}
$$</li>
<li>由$(2.1)$, 给定$\delta {\hat c}_r^{(t)}$, $\delta \hat g_i$, $\delta \hat g_f$, $\delta \hat g_o$, 求$\delta W^{(t)}$, $\delta h^{(t-1)}$
$$
\delta W^{(t)} = \delta z^{(t)} \cdot (I^{(t)})^T \\
\delta I^{(t)} = W^T \cdot \delta z^{(t)} \\
$$</li>
<li>假设一共有$t$个时间步, 前向传播完毕之后更新权重
$$
\begin{aligned}
\delta W &amp;= \sum_{t=1}^{T}\delta W^{(t)} \\
W &amp;= W - \epsilon \cdot \delta W
\end{aligned}
\tag{2.9} $$</li>
</ol>
<p>有了Auto Grad, 我想大部分情况下你都不需要手推LSTM的BP&hellip;</p>
<h2 id="gru">GRU</h2>
<blockquote>
<p>贫穷限制了我们的计算能力</p>
</blockquote>
<p>简而言之LSTM需要传递长短两条状态, 而GRU(Gate Recurrent Unit)只需要一个状态, 但性能和LSTM相似, 且需要更少的参数, 只需要三组系数(两个门和一个状态). 该结构是Chung, Junyoung, et al. (2014)的工作</p>
<h3 id="reset--update-gate">Reset &amp; Update Gate</h3>
<p>GRU有两个门: Reset门$g_r$和Update门$g_z$, 由当前输入$x^{(t)}$和前一步输出$h^{(t-1)}$决定
$$
\begin{aligned}
\begin{bmatrix}
\hat g_r \\
\hat g_z
\end{bmatrix}
&amp;=
\begin{bmatrix}
W_r &amp; U_r \\
W_z &amp; U_z
\end{bmatrix}
\cdot
\begin{bmatrix}
x^{(t)} \\
h^{(t-1)}
\end{bmatrix} \\
\begin{bmatrix}
g_r \\
g_z
\end{bmatrix}
&amp;=
\begin{bmatrix}
\sigma(g_r) \\
\sigma(g_z)
\end{bmatrix}
\end{aligned}
\tag{3.1}
$$</p>
<h3 id="remember--forget-in-one-shot">Remember &amp; Forget in One Shot</h3>
<p>GRU的信息传递也涉及两步
$$
h&rsquo; = tanh(\begin{bmatrix} W_h &amp; U_h\end{bmatrix} \cdot \begin{bmatrix} x^{(t)} \\ g_r \otimes h^{(t-1)} \end{bmatrix})
\tag{3.2}
$$</p>
<p>$$
h^{(t)} = g_z \otimes h^{(t-1)} + (1 - g_z) \otimes h'
\tag{3.3}
$$
其中$(3.2)$步涉及到选择性地添加将上一步的隐状态$h^{(t-1)}$和输入$x^{(t)}$, $(3.3)$步涉及到同时进行遗忘$(g_z \otimes h^{(t-1)})$和记忆$((1 - g_z) \otimes h&rsquo;)$, 而遗忘和记忆都通过$g_z$进行, 且归一化为1</p>
<h2 id="pytorch-implementation">Pytorch implementation</h2>
<h3 id="model">Model</h3>
<p>这里实现了一个LSTMTagger, 参考<a href="http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html" target="_blank" rel="noopener noreffer ">Sequence Models and Long-Short Term Memory Networks</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LSTMTagger</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 初始化隐藏层, 有两组状态, 即h和c</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">                <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embed</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_label</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">LSTMTagger</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 先将单词embed为embed_dim维向量</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embed</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 再通过LSTM</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 每个输出通过一个线性+激活层输出为num_label维概率分布</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden2tag</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">log_softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为每一句输入初始化h和c</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># word embedding</span>
</span></span><span class="line"><span class="cl">        <span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的out就是每一步输出的hidden state, 而self.hidden是最后一步的输出, 即out[-1] == self.hidden</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tag_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden2tag</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">log_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">tag_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">log_prob</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="batch">Batch</h3>
<ul>
<li>Mini Batch Training
<code>train.shape == (n_batch, batch_size, embedding_dim)</code>
假设一篇文档长为10000个单词, 输出标记每个单词的词性, 即有10000个输出. 设<code>batch_size=50</code>, 则式$(2.9)$中$T=50$, 每传播50个单位计算一次梯度并更新, 一共迭代<code>n_batch=10000/50=200</code>次, 每个单词被embed为<code>embedding\_dim</code>维, 计算细节如下(部分伪代码, 不是python):</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">200</span><span class="p">,</span>  <span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span>\<span class="n">_batch</span> <span class="kn">from</span> <span class="mi">1</span> <span class="n">to</span> <span class="mi">200</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">   <span class="c1"># feedforward</span>
</span></span><span class="line"><span class="cl">   <span class="k">for</span> <span class="n">step</span> <span class="kn">from</span> <span class="mi">1</span> <span class="n">to</span> <span class="mi">50</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">       <span class="n">hidden</span><span class="p">,</span> <span class="n">outputs</span><span class="p">[</span><span class="n">i_batch</span><span class="p">][</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="n">i_batch</span><span class="p">][</span><span class="n">step</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">   <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i_batch</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">i_batch</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">   <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># backprop</span>
</span></span><span class="line"><span class="cl">   <span class="c1"># accumulate gradient</span>
</span></span><span class="line"><span class="cl">   <span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">   <span class="k">for</span> <span class="n">step</span> <span class="kn">from</span> <span class="mi">1</span> <span class="n">to</span> <span class="mi">50</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">       <span class="n">dW</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">step</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span><span class="p">()</span>  <span class="c1"># pytorch accumulate gradient automatically along timesteps</span>
</span></span><span class="line"><span class="cl">       <span class="n">grad</span> <span class="o">+=</span> <span class="n">dW</span><span class="p">[</span><span class="n">i_ts</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">   <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>Variable Length Training: Using <code>pack_padded_sequence()</code>
假设训练集为3句话, 分别包括<code>(3, 2, 4)</code>个单词, 每个单词进行2维 embedding, 原则上需要一句话一个batch进行训练, 但每句话长度不一样, 无法固定<code>batch_size</code>:</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">train = [
</span></span><span class="line"><span class="cl">   [[1, 2], [3, 4], [5, 6]],
</span></span><span class="line"><span class="cl">   [[7, 8], [9, 10]],
</span></span><span class="line"><span class="cl">   [[4, 3], [6, 5], [2, 1], [0, 0]]
</span></span><span class="line"><span class="cl">]
</span></span></code></pre></td></tr></table>
</div>
</div><p>此时可以使用padding对齐所有变长的句子, 参考<a href="https://discuss.pytorch.org/t/batch-processing-with-variable-length-sequences/3150" target="_blank" rel="noopener noreffer ">pytorch论坛的讨论</a></p>
<h2 id="appendix">Appendix</h2>
<h3 id="gradient-of-activate-funtions">Gradient of Activate Funtions</h3>
<p>$$
\begin{aligned}
y &amp;= sigmoid(x) =  \frac{1}{1 + e^x} \\
y&rsquo; &amp;= y (1 - y)
\end{aligned}
\tag{a.1}
$$</p>
<p>$$
\begin{aligned}
y &amp;= tanh(x) =  \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
y&rsquo; &amp;= 1 - y^2
\end{aligned}
\tag{a.2}
$$</p>
<h2 id="reference">Reference</h2>
<ol>
<li><a href="http://arunmallya.github.io/writeups/nn/lstm/index.html#/" target="_blank" rel="noopener noreffer ">LSTM Forward and Backward Pass</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener noreffer ">Understanding LSTM Networks</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/30465140" target="_blank" rel="noopener noreffer ">Step-by-step to LSTM: 解析LSTM神经网络设计原理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32481747" target="_blank" rel="noopener noreffer ">人人都能看懂的GRU</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28297161" target="_blank" rel="noopener noreffer ">三次简化一张图: 一招理解LSTM/GRU门控机制</a></li>
</ol>
<blockquote>
<p>to be continued&hellip;</p>
</blockquote></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2023-02-14&nbsp;<a class="git-hash" href="https://github.com/shawnau/hugo_blog/commit/2c2d05cdd3d4ee6a956693e6d97a6277b10ee5c0" target="_blank" title="commit by xiaoxuan(shawn_au@outlook.com) 2c2d05cdd3d4ee6a956693e6d97a6277b10ee5c0: update website">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>2c2d05c</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/2018-01-29-lstm/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://xxuan.cc/2018-01-29-lstm/" data-title="循环神经网络的前世今生" data-hashtags="LSTM,Neural Network,NLP"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://xxuan.cc/2018-01-29-lstm/" data-hashtag="LSTM"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://xxuan.cc/2018-01-29-lstm/" data-title="循环神经网络的前世今生"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://xxuan.cc/2018-01-29-lstm/" data-title="循环神经网络的前世今生"><i data-svg-src="/lib/simple-icons/icons/line.min.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://xxuan.cc/2018-01-29-lstm/" data-title="循环神经网络的前世今生"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/lstm/">LSTM</a>,&nbsp;<a href="/tags/neural-network/">neural network</a>,&nbsp;<a href="/tags/nlp/">NLP</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/2017-12-30-binarytreetraverse/" class="prev" rel="prev" title="Binary Tree Traverse"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Binary Tree Traverse</a>
            <a href="/2018-02-03-hmm/" class="next" rel="next" title="Hidden Markov Model">Hidden Markov Model<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.102.2">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2016 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">xiaoxuan</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="https://xxuan-cc.disqus.com/embed.js" defer></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
