[{"categories":null,"content":"经过一系列艰苦的考古, 找回了当时七牛云挂掉之后备份的阿里云图床. 这么多年来兢兢业业为我当图床还没跑路, 甚至也没花一分钱 (至今账上还有2块钱), 比隔壁的什么牛厉害多了(参考被坑经历). 很高兴过去写的文章都回来了! 下一步就是进一步装修(比如图床全部转移到 github 上并且用 cdn 加速), 以及写新的文章啦! 上一次修整博客还远在17年. 但是人不能被懒惰战胜! 这次做的事情: 换主题: LoveIt (见页底) 迁移文章. 修复了很多 latex 渲染错误, 但是依然还有一部分没仔细修复的存在. 很多公式我自己都看不懂了, 所以得复习复习. ","date":"2023-02-14","objectID":"/2023-02-14-refresh/:0:0","tags":null,"title":"图床迁移和旧文章恢复!","uri":"/2023-02-14-refresh/"},{"categories":null,"content":"一类01矩阵形成的‘岛屿’问题","date":"2022-09-08","objectID":"/playground/islands/","tags":["dfs"],"title":"岛屿问题","uri":"/playground/islands/"},{"categories":null,"content":"总结了一些01矩阵组成的岛屿问题. 基本上可以通过dfs解决, 但也可以用并查集处理. 其中200是我做的第一道并查集题, 印象很深 200 岛屿数量: 最基本的dfs框架, 不需要返回值 1254 统计封闭岛屿的数目: 200的变体, 特判一下与边界接壤的岛屿, 把他们去掉 463 岛屿的周长: dfs需要返回周长 695 岛屿的最大面积: dfs需要返回面积 827 最大人工岛: dfs在标记岛屿的同时返回面积 参考: 岛屿类问题的通用解法、DFS 遍历框架 ","date":"2022-09-08","objectID":"/playground/islands/:0:0","tags":["dfs"],"title":"岛屿问题","uri":"/playground/islands/"},{"categories":null,"content":"观察者模式简介, 以及c#的event-handler模式带来的便利","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"观察者模式是常见的设计模式之一. 为了便于理解, 考虑一种常见的场景: 被观察者是报刊, 观察者是订阅报刊的人. 一旦有新闻, 订阅者都可以收到报刊的通知. ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:0:0","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"定义消息, 观察者和被观察者 ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:1:0","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"消息与EventArgs 第一步是定义消息的格式, 以及观察者和被观察者的行为. public class NotifyEventArgs : EventArgs { public string Info { get; set; } = string.Empty; public DateTime Time { get; set; } } 首先定义消息, 这里继承了EventArgs. C#在语言上就提供了对消息的的支持, 其中 EventArgs 抽象了消息. 为了简单起见, 这里消息就只包含内容和发出的时间两部分. EventHandler 抽象了一个delegate, 也就是处理消息的逻辑, 即观察者收到消息之后需要做的事情. ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:1:1","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"定义观察者 public interface IObserver { public void NotifyEventHandler(object? notifier, NotifyEventArgs args); } 观察者只需要做一件事, 就是收到消息之后做他想做的事情. 这里函数签名与EventHandler一致, 被观察者会把它自身传给第一个参数供观察者使用, 第二个参数是NotifyEventArgs, 就是前面定义的消息内容. ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:1:2","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"定义被观察者 public interface IObservable { public void Register(IObserver observer); public void OnNotified(NotifyEventArgs args); } 被观察者需要做两件事: Register: 订阅观察者 需要通知观察者的时候, 调用OnNotified, 并且传入消息 ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:1:3","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"实现观察者 public class SubscriberAlice : IObserver { public string Name = \"Alice\"; public void NotifyEventHandler(object? notifier, NotifyEventArgs args) { Console.WriteLine($\"{this.Name} received {args.Info} from {notifier?.ToString()} at {args.Time}.\"); } } public class SubscriberBob : IObserver { public string Name = \"Bob\"; public void NotifyEventHandler(object? notifier, NotifyEventArgs args) { Console.WriteLine($\"{this.Name} received {args.Info} from {notifier?.ToString()} at {args.Time}.\"); } } 这里构造了两个观察者, Alice和Bob, 他们做的事情就是把收到的消息打印出来. 利用notifier和args, 我们可以看到消息内容, 时间和发送消息的对象. ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:2:0","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"实现被观察者 public class Observable : IObservable { internal event EventHandler\u003cNotifyEventArgs\u003e NotifyEventHandlers = delegate {}; public virtual void OnNotified(NotifyEventArgs args) =\u003e NotifyEventHandlers(this, args); public void Register(IObserver observer) { this.NotifyEventHandlers += observer.NotifyEventHandler; } public void Publish() { var args = new NotifyEventArgs { Info = $\"[Breaking News!]\", Time = DateTime.Now }; OnNotified(args); } public override string ToString() =\u003e \"New York Times\"; } NotifyEventHandlers 可以视为一个集合, 保存了所有观察者注册的行为(delegate), 在需要发送消息的时候, 所有行为都会被依次调用, 这样观察者们会被依次通知. OnNotified调用NotifyEventHandlers里注册的行为. Register将观察者的行为添加到NotifyEventHandlers中. Publish构造EventArgs并触发OnNotified, 发送消息给所有观察者. ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:3:0","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"启动程序 class Program { static void Main(string[] args) { // 初始化被观察者 var notifier = new Observable(); // 初始化观察者 var alice = new SubscriberAlice(); var bob = new SubscriberBob(); // 注册观察者 notifier.Register(alice); notifier.Register(bob); // 通知观察者 notifier.Publish(); } } 程序输出 Alice received [Breaking News!] from New York Times at 8/30/2022 3:37:14 PM. Bob received [Breaking News!] from New York Times at 8/30/2022 3:37:14 PM. ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:4:0","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"Reference Observer Design Pattern | Microsoft Docs How to: Raise and Consume Events | Microsoft Docs ","date":"2022-08-30","objectID":"/2022-08-30-observer-pattern/:4:1","tags":null,"title":"设计模式 - 观察者模式(Observer Pattern)","uri":"/2022-08-30-observer-pattern/"},{"categories":null,"content":"臭名昭著的接雨水以及其常用解法","date":"2021-08-11","objectID":"/playground/trapping-rain-water/","tags":null,"title":"Trapping Rain Water","uri":"/playground/trapping-rain-water/"},{"categories":null,"content":"https://leetcode.com/problems/trapping-rain-water/ 设第$i$个bar的高度为$h_i$, 其蓄水量为$v_i$, 一共有$n$个bar, ","date":"2021-08-11","objectID":"/playground/trapping-rain-water/:0:0","tags":null,"title":"Trapping Rain Water","uri":"/playground/trapping-rain-water/"},{"categories":null,"content":"暴力解 对于每一个位置$i$, 考虑它头顶有多少水. 很容易发现它左边最高和右边最高的两个bar中比较低的那个决定了水位高度, 再减去当前bar的高度就是水的体积. 也就是: $$v_i = \\min (\\max\\limits_{0\\leq l\u003ci} h_l, \\max\\limits_{i\u003cr\u003cn} h_r) - h_i$$ 对所有位置求和就得到了结果. 注意处理负数情况为0. 时间复杂度 $O(n^2)$. 可以发现如果提前维护好每一个位置左边和右边最大的bar, 就可以 $O(n)$ 时间复杂度解决本题. ","date":"2021-08-11","objectID":"/playground/trapping-rain-water/:1:0","tags":null,"title":"Trapping Rain Water","uri":"/playground/trapping-rain-water/"},{"categories":null,"content":"“水位\"法 对此, 还有一个非常直观的的解法： 想象水从两边流向中间, 水位在逐渐上升, 水会逐渐淹没比较低的bar,并且在淹没其之后流向相邻的bar, 从两边逐渐向中间蔓延. 设水位高度为$level$, 并且水流向了$h_j$. $level \u003e h_j$, 水位比bar高, 水会淹没$h_j$, 此时$j$位置可以拦截的水量即为$v_j = level - h_j$. 证明: 无论水是从左边还是右边流向位置$j$的, 它一定得先漫过左右两边最高的bar中比较低的bar, 否则不能流到此位置. $level \\leq h_j$, 那么水平面遇到了更高的bar, 它不会容纳水, 即$v_j = 0$. 因为水从某一侧来就说明$h_j$的高度比那一侧最高的bar更高. ","date":"2021-08-11","objectID":"/playground/trapping-rain-water/:2:0","tags":null,"title":"Trapping Rain Water","uri":"/playground/trapping-rain-water/"},{"categories":null,"content":"优先队列解 维护水位高度和bar组成的优先队列, 初始时水位为0, 左右两边的bar入队列. 随着水位的上升, 有: 最小值出队列, 设为i, 同时更新水位 level = max(level, height[i]) 探索与i相邻的bar, 设为j, 若未访问过, 压入队列, 累加该位置的蓄水量 v += level - height[j] 重复1~2步骤直至队列为空. 可以发现, 这就是一个从边界开始的bfs, 对每个元素计算其蓄水量并累加得到结果. int trap(vector\u003cint\u003e\u0026 height) { int n = height.size(), level = 0; int volume = 0; vector\u003cint\u003e visited(n); priority_queue\u003cpair\u003cint, int\u003e\u003e q; // 首尾入栈 q.push({-height[0], 0}); q.push({-height[n-1], n-1}); visited[0] = 1; visited[n-1] = 1; while (!q.empty()) { auto i = q.top(); q.pop(); level = max(level, -i.first); for (int j : {i.second-1, i.second+1}) { if (j \u003e= 0 \u0026\u0026 j \u003c n \u0026\u0026 !visited[j]) { visited[j] = 1; if (height[j] \u003c level) { volume += level - height[j]; } q.push({-height[j], j}); } } } return volume; } 这里用了几个小技巧. 一个是优先队列比较pair\u003cint, int\u003e中的第一个元素, 所以把高度放在前, 把index放在后. 其次是默认的优先队列是最大堆, 为了变成最小堆, 我们压入负的高度. ","date":"2021-08-11","objectID":"/playground/trapping-rain-water/:2:1","tags":null,"title":"Trapping Rain Water","uri":"/playground/trapping-rain-water/"},{"categories":null,"content":"双指针解 双指针$l, r$从两边向中间遍历, 双指针其实就是水位法中优先队列里的两个元素. 双指针的移动顺序, 其实就是水浸没bar的顺序. 所以逻辑是完全一致的. class Solution { public: int trap(vector\u003cint\u003e\u0026 height) { int l = 0; int r = height.size() - 1; // l和r对应队列中的index int lmax = 0; int rmax = 0; int v = 0; // lmax和rmax代表了左右两边水位的高度, 对应level while (l \u003c= r) { int hl = height[l]; int hr = height[r]; // hl和hr对应队列中的height if (hl \u003c= hr) { // 左边水位比较低, 先处理左边 lmax = max(lmax, hl); // 更新水位 if (lmax \u003e hl) { v += lmax - hl; // 水位浸没hl } l++; // 对应hl入队列 } else { // 右边水位比较低, 先处理右边 rmax = max(rmax, hr); if (rmax \u003e hr) { v += rmax - hr; // 水位浸没hr } r--; // 对应hr入队列 } } return v; } }; ","date":"2021-08-11","objectID":"/playground/trapping-rain-water/:2:2","tags":null,"title":"Trapping Rain Water","uri":"/playground/trapping-rain-water/"},{"categories":null,"content":"二维拓展 Extend: https://leetcode.com/problems/trapping-rain-water-ii/ 这道题拓展到了2维，其实使用\"水位法\"依然可解。 想象水从四周流向中间，水位逐渐上升，每当水位升至与某一个$h_i$齐平的时候，它就会\"溢出\"并且蔓延到和$h_i$接壤的位置，此时需要检查水可不可以\"漫\"过去。如果接壤的$h_j \u003c h_i$，$h_j$之上可以容纳水的体积就确定为$h_i - h_j$. 因为对$j$来说，在已经把它包围的拓扑里，最低的值是$h_i$ 所以和上一题把两边的bar入栈类似的, 先把最外围一圈入栈, 然后和1维情况一样了. 演示动画 // 顺时针遍历周围的元素 int drow[4] = {0, -1, 0, 1}; int dcol[4] = {-1, 0, 1, 0}; int trapRainWater(vector\u003cvector\u003cint\u003e\u003e\u0026 h) { int m = h.size(); int n = h[0].size(); int level = 0; int volume = 0; priority_queue\u003cpair\u003cint, pair\u003cint, int\u003e\u003e\u003e q; // {-height, {row, col}} vector\u003cvector\u003cint\u003e\u003e visited(m, vector\u003cint\u003e(n)); // 最外层一圈入堆 for (int i = 0; i \u003c m; i++) { for (int j = 0; j \u003c n; j++) { if (i == 0 || j == 0 || i == m-1 || j == n-1) { q.push({-h[i][j], {i, j}}); visited[i][j] = 1; } } } while (!q.empty()) { auto cur = q.top(); q.pop(); level = max(level, -cur.first); for (int i = 0; i \u003c 4; i++) { // 遍历邻居 int r2 = cur.second.first + drow[i]; int c2 = cur.second.second + dcol[i]; if (r2 \u003e= 0 \u0026\u0026 r2 \u003c m \u0026\u0026 c2 \u003e=0 \u0026\u0026 c2 \u003c n \u0026\u0026 !visited[r2][c2]) { int h2 = h[r2][c2]; if (h2 \u003c level) { volume += level - h2; } q.push({-h2, {r2, c2}}); visited[r2][c2] = 1; } } } return volume; } ","date":"2021-08-11","objectID":"/playground/trapping-rain-water/:3:0","tags":null,"title":"Trapping Rain Water","uri":"/playground/trapping-rain-water/"},{"categories":null,"content":" Time to mix drinks and change lives ","date":"2021-07-04","objectID":"/2021-07-04-va11/:0:0","tags":null,"title":"赛博朋克, 调酒师与现实","uri":"/2021-07-04-va11/"},{"categories":null,"content":"VA-11 HALL-A VA-11 HALL-A是2016年6月21日登陆Steam的一款单人AVG游戏, 在游戏里你需要做的事情很少. 简单来说, 就是调酒, 听客人吐槽以及吐槽客人, 捎带每天晚上去购物满足主角Jill的消费欲, 以及赚够足够的房租不至于流落街头. 因为一些游戏之内以及之外的故事, 它意外地获得了非常好的评价, 以及很多meme. 在此就不展开了, 有兴趣可以自己搜搜(毕竟本文也不是来推销游戏的) 单纯从游戏内容来看, 8bit风格十足的美术和音乐, 鲜活的人物刻画, 以及充斥着整个游戏的(语文不好不知道如何形容, 4chan味?)构造出一个十分有沉浸感的世界. 当然, 既然是赛博朋克, 反乌托邦那套世界观拿来就用了. 巧在现代互联网社区的阴暗角落(没错,又是4chan)里充斥着的元素和这样的世界观异常相配, 产生了奇妙的化学反应, 这样虚拟世界/网络世界/现实的模糊也是沉浸感的来源. The cyberpunk world was born for criticism, which is far from even a fine world. There’s bitter behind the guys created the game, and it’s not their fault. The idea of cyberpunk is so famous. I’m guessing some big names would feel sting in their backs while other poor guys creating, contributing, and being a part of this world. 之所以对这个游戏有如此深刻的印象, 还是要回到现实, 看看当时发生了啥. 好在我有玩完游戏写review的习惯, 翻到大概是在2018年7月15日在steam写的, 根据时间来回忆一下\"操蛋的现实\"指的都是啥. 很高兴能暂时脱离这操蛋的现实,在vallhala里呆一会, 体验他们更操蛋的生活. 这很赛博朋克. 希望有一家这样的酒吧可以去, 虽然我不能喝酒, 但可以Sugar Rush嘛 最后, 太真实了, 买买买还是很容易导致交不起房租的! 下面是私货 2018年7月, 研究生二年级暑假, 我刚好结束为期1年的实习, 离开上海回家, 准备秋招踏上社畜之路. 那个时候完全没有职场经验, 属于走进面试间两腿就打颤的水平, 很方. (搞笑的是后来面多了才知道根本没有面试间, 成百上千人挤在一起 搬个小桌子和你面对面都很奢侈了) 不过刚到家屁股还没坐热, 我爹晕倒在了家里, 感觉这种关头人会觉得事情一下子就全都来的很快: 打120, 进急诊室, 医生就让我签责任书, 使用溶栓药. 因为这玩意会清除血栓, 但也有一定概率造成脑出血, 所以我要为我的决定负责. 面对的是自己的亲爹, 这种感觉难以描述. 打完效果未卜转到南京省院, 在医院打地铺通宵等医生消息. 好在之后恢复得很好, 甚至都没有留下什么后遗症, 并且也没有再复发过. 对坚持锻炼的我爹报以崇高的敬意. 后来我在准备面试的闲暇时间打完了这游戏, 经历了这一切求职时心态反而很好, 感觉自己也成长了一些, 不过代价还是太大了, 不如不要. 游戏如果足够好, 就能让人暂时从现实脱身, 躲在安全的壳儿里神游一会(其实好电影也一样). 四处奔波面试的我大概是很珍惜这段时间了. 想起自己16年本科毕业之后不想继续读基础学科了, 也逃避就业, 一个人在南大和园租了个毛坯房, 闷头准备了一年考研, 当时印象最深的是看完了7部jojo, 玩了几遍Undertale. 白天见的人太少了有点恍惚, 晚上就去GTA5里上街瞎逛, 感觉都是很相似的经历. 这部作品有意无意藏了很多星际牛仔的梗. Rad Shiba(有提到是高智商写程序的犬类) -\u003e Ein Jamie曾经提到的传奇杀手外号Black Dog -\u003e Jet的外号 Alma -\u003e Ed(肤色很像, 都是黑客, 身材嘛…who knows?) 有一款酒名为Crevice Spike 月球爆炸了!(cb世界观) 作为cb粉真的被爽到了. ","date":"2021-07-04","objectID":"/2021-07-04-va11/:1:0","tags":null,"title":"赛博朋克, 调酒师与现实","uri":"/2021-07-04-va11/"},{"categories":null,"content":"Ben’s Secret ","date":"2021-07-04","objectID":"/2021-07-04-va11/:2:0","tags":null,"title":"赛博朋克, 调酒师与现实","uri":"/2021-07-04-va11/"},{"categories":null,"content":"缘起 去年我在b站发现一位调酒师UP神秘金针菇先生发了游戏里酒的复刻(当然不能真按游戏里的配方来, 需要要调酒师自己去按照酒的属性创作, 所以不会调酒就别瞎折腾了), 发现酒吧在南京, 就收藏了希望哪天有机会去体验一下. 所以, 2021年6月的最后一个周末, 冒着夜里的小雨, 我终于有机会前去拜访一下. 这还是我第一次去酒吧…本人基本上不喝酒, 以前在实验室闻了太多乙醇, 和巴普洛夫的狗子一样训练出了ptsd. 哈哈 酒吧门口的招牌. 其实藏得很深, 门口是烤肉店. 很有设计感的招牌 排排坐就满啦. 有全套eva和王牌酒保漫画可看 贞本义行的EVA漫画 贞本义行老师的EVA. 漫画作者和我一样是个丽党, 我喜欢这个版本的结局. 王牌酒保是调酒师入行的引子 王牌酒保是调酒师入行的引子呢. 不过依他所言, 现实要是和漫画里一样, 酒吧就要关门咯. 所以说漫画就是漫画, 啊哈哈. ","date":"2021-07-04","objectID":"/2021-07-04-va11/:2:1","tags":null,"title":"赛博朋克, 调酒师与现实","uri":"/2021-07-04-va11/"},{"categories":null,"content":"赛博小酒馆 幸运地找到了UP主, 表明了是他的粉丝!(其实是假粉, 视频都没看完, 后来果然暴露了, 哈哈), 点了两杯女性化的酒 - Sugar Rush, Fluffy Dream以及一杯黄油啤酒 (拍照过于激动还打翻了一杯, 罪过罪过) 经典的Sugar Rush, 不含酒精, 杯子真的很迷你, 有那种女性化的可爱感觉 Fluffy Dream, 微酸, 很柔和的口感, 清爽 黄油啤酒, 肉桂和麦芽香气让这款酒特别上头 其实还有好几件意外惊喜没能多拍点记录下来(只顾吃喝了), 恰逢酒吧一位客人过生日, 我还收到了一份马卡龙和蛋糕, 希望过生日的小伙也能收到我的祝福哈. ","date":"2021-07-04","objectID":"/2021-07-04-va11/:2:2","tags":null,"title":"赛博朋克, 调酒师与现实","uri":"/2021-07-04-va11/"},{"categories":null,"content":"胶片 给老板拍了一张Portra400, 可惜酒吧有点儿暗, 外加当时机器还没完全修好, 我自觉拍得不够完美, 希望神秘金针菇先生不介意就好哈. 顺便也给Sugar Rush拍了一张. 虽然有点暗但是很有氛围感 调酒师很亲切, 可爱的娃娃脸 如果有机会, 下次一定带灯和脚架来拍. 感谢茜茜, 感谢调酒师先生以及酒吧里的客人们, 让我有了这次奇妙而难忘的经历. 尾声悄悄话 2021年了, 我的经历还要以图文这朴素的方式记录, 而不是2077里的超梦, 科技发展也太慢了吧? 什么时候能去火星啊. 望天. ","date":"2021-07-04","objectID":"/2021-07-04-va11/:2:3","tags":null,"title":"赛博朋克, 调酒师与现实","uri":"/2021-07-04-va11/"},{"categories":null,"content":"LeetCode 174 ","date":"2020-03-01","objectID":"/playground/dungeon-game/:0:0","tags":["dynamic programming"],"title":"Dungeon Game","uri":"/playground/dungeon-game/"},{"categories":null,"content":"A False Example We demonstrate a false example to show that there is no top-down, left-right dp solution to this problem. We need to test all possible paths from $(0,0)$ to $(i,j)$, find the lowest bound of the path sum along each path, from start to end. (Actually the value is lowest bound + 1, which keeps the knight alive) Keep two tables, $dpSum[i][j]$ records the path sum to have $dpMin[i][j]$, which records the lowest bound upon current position along this path. If we have both $(i-1, j)$ and $(i, j-1)$ for above two tables, do we have $dp[i][j]$ ? The answer is no. Think about it. ","date":"2020-03-01","objectID":"/playground/dungeon-game/:1:0","tags":["dynamic programming"],"title":"Dungeon Game","uri":"/playground/dungeon-game/"},{"categories":null,"content":"Solution Actually we need to calculate at a bottom-up, right-left order. Let $dp[i][j]$ to be the lowest HP to start from $(i,j)$ to the end, we have: $$dp[i][j] = \\begin{cases} min(dp[i][j-1], dp[i-1][j]) - grid[i][j], \u0026 \\text{$min(dp[i][j-1], dp[i-1][j]) - grid[i][j] \u003e 0$} \\\\ 1, \u0026 \\text{$min(dp[i][j-1], dp[i-1][j]) - grid[i][j] \\leq 0$} \\end{cases}$$ class Solution: def calculateMinimumHP(self, grid: List[List[int]]) -\u003e int: n_row, n_col = len(grid), len(grid[0]) dp = [[float('inf')]*n_col for _ in range(n_row)] dp[n_row-1][n_col-1] = 1 - min(0, grid[n_row-1][n_col-1]) # init last row and col for i in range(n_row-2, -1, -1): need = dp[i+1][n_col-1] - grid[i][n_col-1] dp[i][n_col-1] = need if need \u003e 0 else 1 for j in range(n_col-2, -1, -1): need = dp[n_row-1][j+1] - grid[n_row-1][j] dp[n_row-1][j] = need if need \u003e 0 else 1 # init dp table for i in range(n_row-2, -1, -1): for j in range(n_col-2, -1, -1): need = min(dp[i+1][j], dp[i][j+1]) - grid[i][j] dp[i][j] = need if need \u003e 0 else 1 return dp[0][0] ","date":"2020-03-01","objectID":"/playground/dungeon-game/:2:0","tags":["dynamic programming"],"title":"Dungeon Game","uri":"/playground/dungeon-game/"},{"categories":null,"content":"LeetCode 730 The problem seems to be similar to Longest Palindromic Subsequence, but tt’s hard to enumerate all the subsequences since it goes to $O(2^n)$ complexity. Actually, the distinct subsequence is similar to Distinct Subsequences II. We could use the method called Sequence Automata. Check: 序列自动机总结与例题, 序列自动机 class Solution: def init_next(self, s: str, a: int) -\u003e List[List[int]]: n = len(s) - 1 get_next = [[0]*a for _ in range(n+1)] for i in range(n, 0, -1): for c in range(a): get_next[i-1][c] = get_next[i][c] get_next[i-1][ord(s[i])-ord('a')] = i return get_next def countPalindromicSubsequences(self, S: str) -\u003e int: s1, s2, n, a, mod = ' '+S, ' '+S[::-1], len(S), 4, 10**9+7 next1, next2 = self.init_next(s1, a), self.init_next(s2, a) def dfs(i, j) -\u003e int: ans = 0 for c in range(a): i_n, j_n = next1[i][c], next2[j][c] if i_n and j_n: if i_n + j_n \u003e n + 1: continue if i_n + j_n \u003c n + 1: ans += 1 ans = (ans + dfs(i_n, j_n)) % mod return ans + 1 return dfs(0, 0) - 1 ","date":"2020-02-22","objectID":"/playground/count-different-palindromic-subsequences/:0:0","tags":["dynamic programming","backtracking"],"title":"Count Different Palindromic Subsequences","uri":"/playground/count-different-palindromic-subsequences/"},{"categories":null,"content":"LeetCode 516 This is similar to Longest Palindromic Substring, but it is hard to use the expanding idea cuz the subsequence is not continuous. We can try to use the dp idea as an initial: Let $dp[i][j]$ to be the longest palindromic substring for $s[i, j]$, then: $$ dp[i][j] = \\begin{cases} dp[i+1][j-1] + 2, \u0026 \\text{$s[i]=s[j]$} \\\\ \\max\\lbrace dp[i][j-1], dp[i+1][j] \\rbrace, \u0026 \\text{$s[i] \\neq s[j]$} \\\\ 1, \u0026\\text{$i=j$} \\\\ 0, \u0026\\text{$i\u003ej$} \\end{cases} $$ dp[i][j-1]\u003c--dp[i][j] / | / | dp[i+1][j-1] dp[i+1][j] ","date":"2020-02-22","objectID":"/playground/longest-palindromic-subsequence/:0:0","tags":["dynamic programming","backtracking"],"title":"Longest Palindromic Subsequence","uri":"/playground/longest-palindromic-subsequence/"},{"categories":null,"content":"Corner Cases There are two corner cases as listed above, $i=j$ and $i\u003cj$, we need to initialize them first. i\\j 0 1 2 3 4 0 1 a c f j 1 0 1 b e i 2 0 1 d h 3 0 1 g 4 0 1 class Solution: def longestPalindromeSubseq(self, s: str) -\u003e int: n = len(s) if n == 0: return 0 dp = [[0]*n for _ in range(n)] # init two corner cases for i in range(n): dp[i][i] = 1 for i in range(1, n): dp[i][i-1] = 0 # filling the table for j in range(1, n): for i in range(j-1, -1, -1): if s[i] == s[j]: dp[i][j] = dp[i+1][j-1] + 2 else: dp[i][j] = max(dp[i+1][j], dp[i][j-1]) return dp[0][n-1] ","date":"2020-02-22","objectID":"/playground/longest-palindromic-subsequence/:0:1","tags":["dynamic programming","backtracking"],"title":"Longest Palindromic Subsequence","uri":"/playground/longest-palindromic-subsequence/"},{"categories":null,"content":"Backtracking Solution WIP ","date":"2020-02-22","objectID":"/playground/longest-palindromic-subsequence/:1:0","tags":["dynamic programming","backtracking"],"title":"Longest Palindromic Subsequence","uri":"/playground/longest-palindromic-subsequence/"},{"categories":null,"content":"leetcode 152 The idea is the same like Maximum Subarray, let $dp[i]$ to be the maximum product subarray value that ends at $nums[i]$, but we have some issues here: If $dp[i-1] = 3$, while the minimum product subarray value that ends at $i$ is $-4$ and $nums[i] = -1$, then we get $-4 \\cdot -1 \u003e 3 \\cdot -1$ Apparently we need to record the minimum product subarray as well, and we compare all the values. $$ dp[i] = \\max \\lbrace dp_{min}[i-1] \\cdot nums[i], dp_{max}[i-1] \\cdot nums[i], nums[i] \\rbrace $$ class Solution: def maxProduct(self, nums: List[int]) -\u003e int: n, max_prod = len(nums), nums[0] dp = [{'max':0, 'min':0} for _ in range(n)] dp[0]['max'], dp[0]['min'] = nums[0], nums[0] for i in range(1, n): dp[i]['max'] = max(dp[i-1]['max']*nums[i], dp[i-1]['min']*nums[i], nums[i]) dp[i]['min'] = min(dp[i-1]['max']*nums[i], dp[i-1]['min']*nums[i], nums[i]) max_prod = max(max_prod, dp[i]['max']) return max_prod ","date":"2020-02-22","objectID":"/playground/maximum-product-subarray/:0:0","tags":["dynamic programming"],"title":"Maximum Product Subarray","uri":"/playground/maximum-product-subarray/"},{"categories":null,"content":"leetcode 140 This is pretty much an easier version of Palindrome Partitioning, cuz finding a word is much easier than finding a valid palindrome substring. Using a backtracking method: When a substring is breakable, we iterate this substring and find all possible break positions. Dfs search each possible breaks. If search ends, we find a match. ","date":"2020-02-21","objectID":"/playground/word-break-2/:0:0","tags":["dynamic programming","backtracking","dfs"],"title":"Word Break 2","uri":"/playground/word-break-2/"},{"categories":null,"content":"Corner Cases: We can handle the situaion which search string is '', so we can put the terminal condition in the self.breakable() class Solution: def breakable(self, s: str, wordDict: List[str]) -\u003e bool: n = len(s) dp = [False]*(n+1) dp[0] = True for i in range(1, n+1): for n in range(i+1): if dp[n] and s[n:i] in wordDict: dp[i] = True break return dp[n] def wordBreak(self, s: str, wordDict: List[str]) -\u003e List[str]: n = len(s) results: List[str] = [] def search(i:int, path: List[str]): if i == n: results.append(' '.join(path)) if self.breakable(s[i:], wordDict): for j in range(i, n): sub_str = s[i:j+1] if sub_str in wordDict: search(j+1, path+[sub_str]) search(0, []) return results ","date":"2020-02-21","objectID":"/playground/word-break-2/:0:1","tags":["dynamic programming","backtracking","dfs"],"title":"Word Break 2","uri":"/playground/word-break-2/"},{"categories":null,"content":"leetcode 139 This is an easy version of Longest Palindromic Substring, cuz finding a word is much easier than finding a valid palindrome substring. Let $dp[i]$ to be if $s[0, i]$ is a valid break, we have: If $s[0, n]$ is a valid break and $s[n+1, i]$ is in the dict $S$, then $s[0, i]$ is a valid break. i.e. $$ dp[n] \\land \\lbrace s[n+1, i] \\in S \\rbrace \\to dp[i] $$ It looks like we need to firstly iterate the $s$, and iterate substring at each position $i$, which makes it $O(n^2)$ complexity. When we find a match in the substring, we can stop the iteration. ","date":"2020-02-21","objectID":"/playground/word-break/:0:0","tags":["dynamic programming"],"title":"Word Break","uri":"/playground/word-break/"},{"categories":null,"content":"Corner Cases If we check substring from $s[0, i]$, then we don’t have $dp[-1]$, we can insert a True to the $dp$ table, and be careful of the index changes: When indexing $dp$, $[0, n-1] \\to [1, n]$ when indexing $s$, we need to use $s[i-1]$ $$dp[i] = \\bigcap_{n=0}^{i} \\left\\lbrace dp[n] \\land \\lbrace s[n+1, i] \\in S \\rbrace \\right\\rbrace $$ class Solution: def wordBreak(self, s: str, wordDict: List[str]) -\u003e bool: n = len(s) dp = [False]*(n+1) dp[0] = True for i in range(1, n+1): for n in range(i+1): if dp[n] and s[n:(i-1)+1] in wordDict: dp[i] = True break return dp[n] ","date":"2020-02-21","objectID":"/playground/word-break/:0:1","tags":["dynamic programming"],"title":"Word Break","uri":"/playground/word-break/"},{"categories":null,"content":"LeetCode 132 We need to borrow the idea from Longest Palindromic Substring, using the ad-hoc method, try to expand at each index as the center. Let $dp[i]$ to be the minimum cuts for $s[0, i]$, initialize it to the maximum cuts, which is $i-1$ (in 0-based indexing, it is $i$) By using the expanding idea, when we get a palindrome substring $s[l, r]$, the minimum cuts for $s[0, r]$ shrinks into $dp[l-1]+1$. Iterate through all the possible palindrome substrings. a b a c d d c a a a 0 1 0 1 2 2 1 2 2 2 ","date":"2020-02-20","objectID":"/playground/palindrome-partitioning-2/:0:0","tags":["dynamic programming"],"title":"Palindrome Partitioning 2","uri":"/playground/palindrome-partitioning-2/"},{"categories":null,"content":"Corner Case When i-1\u003c0, i.e. we have a palindrome substring which starts from 0, e.g. s=abc, r=2, then the dp[0-1]=0, means we don’t need any cut for string abc cuz it itself is a palindrome substring. class Solution: def minCut(self, s: str) -\u003e str: n = len(s) dp = [i for i in range(n)] for i in range(n): for j in range(i+1): l, r = i-j, i+j if l\u003e=0 and r\u003cn and s[l]==s[r]: dp[r] = min(dp[r], dp[l-1]+1 if l-1\u003e=0 else 0) else: break for j in range(i+1): l, r = i-1-j, i+j if l\u003e=0 and r\u003cn and s[l]==s[r]: dp[r] = min(dp[r], dp[l-1]+1 if l-1\u003e=0 else 0) else: break return dp[-1] ","date":"2020-02-20","objectID":"/playground/palindrome-partitioning-2/:0:1","tags":["dynamic programming"],"title":"Palindrome Partitioning 2","uri":"/playground/palindrome-partitioning-2/"},{"categories":null,"content":"LeetCode 131 Although this is similar to Longest Palindromic Substring, yet the idea is similar to Word Break 2. We use backtracking at each place it could be partitioned. class Solution: def partition(self, s: str) -\u003e List[List[str]]: n = len(s) results: List[List[str]] = [] def search(i, path: List[str]): if i == n: results.append(path) for j in range(i, n): sub = s[i:j+1] if sub == sub[::-1]: search(j+1, path+[sub]) search(0, []) return results ","date":"2020-02-20","objectID":"/playground/palindrome-partitioning/:0:0","tags":["backtracking","dfs"],"title":"Palindrome Partitioning","uri":"/playground/palindrome-partitioning/"},{"categories":null,"content":"LeetCode 188 Say you have an array for which the i-th element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete at most k transactions. ","date":"2020-02-19","objectID":"/playground/best-time-to-buy-and-sell-stock-iv/:0:0","tags":["dynamic programming"],"title":"Best Time to Buy and Sell Stock Iv","uri":"/playground/best-time-to-buy-and-sell-stock-iv/"},{"categories":null,"content":"DP Solution Let dp[i][j] to be the maximum profit to buy and sell the stock with at most i operations at t=j. For any given time t, we have two options: Do nothing, or buy the stock, the profit remains unchanged: dp[i][j] = dp[i][j-1] Sell the stock. We must buy the stock first. Assume that we but the stock at $t \\in [0, j-1]$ We have already gained profit dp[i-1][t-1] Buy the stock: -prices[t] Sell the stock: prices[j] Total profit: dp[i-1][t-1] - prices[t] + prices[j] We need to iterate through all possible buying time: $$ dp[i][j] = \\max \\left\\lbrace dp[i][j-1], \\max\\limits_{t \\in [0, j-1]} \\left\\lbrace dp[i-1][t-1] - p[t] \\right\\rbrace + p[j] \\right\\rbrace $$ Notice the 2nd term, we let $M_j=\\max\\limits_{t \\in [0, j-1]} \\left\\lbrace dp[i-1][t-1] - p[t] \\right\\rbrace$ and $ m_{t} = dp[i-1][t-1] - p[t] $: $$ M_{j} = \\max\\limits_{t \\in [0, j-1]} \\lbrace m_{t} \\rbrace = \\max \\left\\lbrace M_{j-1}, m_{j-1} \\right\\rbrace $$ At each iteration $i, j$, we could calculate $m_{j}=dp[i-1][j-1]-p[j]$ for the next iteration in advance. Because in current iteration we already have the data of dp[i-1][x]. Thus we can get rid of the inner loop and calculate $M_j$ in an iterative way. ","date":"2020-02-19","objectID":"/playground/best-time-to-buy-and-sell-stock-iv/:0:1","tags":["dynamic programming"],"title":"Best Time to Buy and Sell Stock Iv","uri":"/playground/best-time-to-buy-and-sell-stock-iv/"},{"categories":null,"content":"Corner Cases We start from $i=1, j=1$ to avoid corner cases. For $i=0$ or $j=0$, $dp[i][j]=0$, so we don’t need additional init steps. class Solution: def quickSolve(self, prices: List[int]) -\u003e int: profit = 0 for i in range(1, len(prices)): if prices[i] \u003e prices[i - 1]: profit += prices[i] - prices[i-1] return profit def maxProfit(self, k: int, prices: List[int]) -\u003e int: if k \u003e= len(prices) // 2: return self.quickSolve(prices) dp = [[0]*len(prices) for _ in range(k+1)] for i in range(1, k+1): tmp_max = -prices[0] for j in range(1, len(prices)): dp[i][j] = max(dp[i][j-1], tmp_max + prices[j]) tmp_max = max(tmp_max, dp[i-1][j-1] - prices[j]) return dp[k][-1] ","date":"2020-02-19","objectID":"/playground/best-time-to-buy-and-sell-stock-iv/:0:2","tags":["dynamic programming"],"title":"Best Time to Buy and Sell Stock Iv","uri":"/playground/best-time-to-buy-and-sell-stock-iv/"},{"categories":null,"content":"LeetCode 120 The method is identical to [Minimal Path Sum](/playground/minimal-path-sum/, only that the shape changes from rectangle to triangle. class Solution: def minimumTotal(self, triangle: List[List[int]]) -\u003e int: n_row = len(triangle) dp = [[] for _ in range(n_row)] dp[0].append(triangle[0][0]) for i in range(1, n_row): for j in range(i+1): top_left = dp[i-1][j-1] if j-1\u003e=0 else float(\"inf\") top_right = dp[i-1][j] if j \u003c len(triangle[i-1]) else float(\"inf\") minimal = min(top_left, top_right) + triangle[i][j] dp[i].append(minimal) return min(dp[-1]) Notes Init the first element manually Init the dp table by appending instead of indexing For each element to iterate, check left bound for top left, check right bound for top right ","date":"2020-02-18","objectID":"/playground/triangle/:0:0","tags":["dynamic programming"],"title":"Triangle","uri":"/playground/triangle/"},{"categories":null,"content":"Leetcode 115 This problem is quite similar to Interleaving String, but the solution is almost the same as Regular Expression Matching: we build top-down solution by backtracking, then build bottom-up solution of dp. ","date":"2020-02-18","objectID":"/playground/distinct-subsequences/:0:0","tags":["dynamic programming","backtracking"],"title":"Distinct Subsequences","uri":"/playground/distinct-subsequences/"},{"categories":null,"content":"Intro problem Let’s discuss a simplified problem: How to check if t is a subsequence of s? Using a greddy strategy, this would be quite straight forward: Iterate through s, match t in a greddy manner. If t has been iterated over, we find a match. If s has been iterated over, we need to iterate over t as well, or there’s no match, search over. If not 1 or 2, we need to continue searching, skip the characters in s[0] which don’t match t[0] def is_subsequence(s: str, t: str) -\u003e bool: def search(i: int, j: int) -\u003e bool: while i\u003clen(s) and j\u003clen(t) and s[i] != t[j]: i += 1 if j == len(t): return True elif i == len(s): return False else: return search(i+1, j+1) return search(0, 0) ","date":"2020-02-18","objectID":"/playground/distinct-subsequences/:1:0","tags":["dynamic programming","backtracking"],"title":"Distinct Subsequences","uri":"/playground/distinct-subsequences/"},{"categories":null,"content":"Backtracking Borrowing the idea from last section, we need to count all of the possible match, so we can’t stop when we find a character matches, we need to skip this match and explore if there’re other possibilities to achieve a match. We define search(i, j) to be the distinct subsequences for s[i:] and t[j:], despite that we search for the current match, we still goes for further possibilities in s def numDistinct(self, s: str, t: str) -\u003e int: def search(i, j): while i\u003clen(s) and j\u003clen(t) and s[i] != t[j]: i += 1 if j == len(t): return 1 elif i == len(s): return 0 else: return search(i+1, j+1) + search(i+1, j) return search(0, 0) Note: We can easily optimize time complexity by using a cache to avoid duplicate calculation. See Optimize Recursion. ","date":"2020-02-18","objectID":"/playground/distinct-subsequences/:2:0","tags":["dynamic programming","backtracking"],"title":"Distinct Subsequences","uri":"/playground/distinct-subsequences/"},{"categories":null,"content":"Dynamic Programming WIP ","date":"2020-02-18","objectID":"/playground/distinct-subsequences/:3:0","tags":["dynamic programming","backtracking"],"title":"Distinct Subsequences","uri":"/playground/distinct-subsequences/"},{"categories":null,"content":"Leetcode 97 ","date":"2020-02-17","objectID":"/playground/interleaving-string/:0:0","tags":["dynamic programming"],"title":"Interleaving String","uri":"/playground/interleaving-string/"},{"categories":null,"content":"Description Given s1, s2, s3, find whether s3 is formed by the interleaving of s1 and s2. Example: Input: s1 = \"aabcc\", s2 = \"dbbca\", s3 = \"aadbbcbcac\" Output: true Explanation: aadbbcbcac, aadbbcbcac are all valid interpretations for the interleaving. Notice: The order of the strings cannot be changed, which indicates that we can construct s3 using s1 and s2 from their substrings. Different ways of interleaving can led to same results. ","date":"2020-02-17","objectID":"/playground/interleaving-string/:1:0","tags":["dynamic programming"],"title":"Interleaving String","uri":"/playground/interleaving-string/"},{"categories":null,"content":"Example of constructing a string We define $dp[i][j]$ as: If $a3[0, i+j]$ can be constructed by $a1[0, i]$ and $a2[0, j]$ We have 2 choices to construct $a3[0, i+j+1]$: using $a1[i+1]$ or $a2[j+1]$ if dp[i][j] and s1[i+1] == s3[i+j+1]: dp[i+1][j] = True # Use s1[i+1] to construct s3[i+j+1] if dp[i][j] and s2[j+1] == s3[i+j+1]: dp[i][j+1] = True # Use s2[j+1] to construct s3[i+j+1] So we have 2 ways to get the needed dp[i][j] use_s1 = dp[i-1][j] and s1[i] == s3[i+j] use_s2 = dp[i][j-1] and s2[j] == s3[i+j] i.e. dp[i][j] = use_s1 or use_s2 ","date":"2020-02-17","objectID":"/playground/interleaving-string/:1:1","tags":["dynamic programming"],"title":"Interleaving String","uri":"/playground/interleaving-string/"},{"categories":null,"content":"Corner Cases If $i=0, j=5$, it represents that if $a3[0, 5]$ can be constructed by $s1=\\varnothing$ and $s2[0,5]$, we only need to check that if $s2[0,5] = s3[0,5]$ If we use 0-based index, this situation corresponding to dp[-1][4], which is not quite convenience. Using 1-based indexing we could initialize the first row and col and then calculating the dp table from dp[1][1] 0 d b b c a 0 T F F F F F a T F F F F F a T T T T F F b F T T F F F c F F T T T T c F F F T F T class Solution: def isInterleave(self, s1: str, s2: str, s3: str) -\u003e bool: if len(s1) + len(s2) != len(s3): return False m, n = len(s1), len(s2) dp = [[False]*(n+1) for _ in range(m+1)] for i in range(n+1): # s1 is empty dp[0][i] = s2[:i] == s3[:i] for i in range(m+1): # s2 is empty dp[i][0] = s1[:i] == s3[:i] for i in range(1, m+1): for j in range(1, n+1): use_s1 = dp[i-1][j] and s1[i-1] == s3[i+j-1] use_s2 = dp[i][j-1] and s2[j-1] == s3[i+j-1] dp[i][j] = use_s1 or use_s2 return dp[-1][-1] Note: Under 1-based indexing, take care of the indexing on s1, s2 and s3. ","date":"2020-02-17","objectID":"/playground/interleaving-string/:1:2","tags":["dynamic programming"],"title":"Interleaving String","uri":"/playground/interleaving-string/"},{"categories":null,"content":"Leetcode 639 Same as Decode Ways, we only need to add some cases on wildcard matching, in which we could match multiple patterns for S[i] class Solution: def numDecodings(self, s: str) -\u003e int: one = {str(k):1 for k in range(1, 10)} one.update({'*':9}) two = {str(k):1 for k in range(10, 27)} two.update({ '*0': 2, '*1': 2, '*2': 2, '*3': 2, '*4': 2, '*5': 2, '*6': 2, '*7': 1, '*8': 1, '*9': 1, '1*': 9, '2*': 6, '**': 15 }) dp = [0]*len(s) for i in range(len(s)): match_one = one.get(s[i], 0) match_two = two.get(s[i-1:i+1], 0) if i-1\u003e=0 else 0 one_bit = dp[i-1]*match_one if i-1\u003e=0 else match_one two_bit = dp[i-2]*match_two if i-2\u003e=0 else match_two dp[i] = one_bit + two_bit return dp[-1] % (10**9+7) ","date":"2020-02-17","objectID":"/playground/decode-ways-2/:0:0","tags":["dynamic programming"],"title":"Decode Ways 2","uri":"/playground/decode-ways-2/"},{"categories":null,"content":"Leetcode 91 This is a 1-d DP. Set $dp[i]$ to be the decode ways for s[:i+1], we have: $$ dp[i] = dp[i-2] \\cdot M^{1}_{i} + dp[i-1] \\cdot M^{2}_{i} $$ In which, $$ \\begin{aligned} M^{1}_{i} \u0026= | \\lbrace S[i] \\in A^{1} \\rbrace | \\\\ M^{2}_{i} \u0026= | \\lbrace S[i-1:i+1] \\in A^{2} \\rbrace \\end{aligned} $$ $M^{1}_{i}$ means how many ways to match the 1-character decoders $A^1$ with $S[i]$. In this problem we have either 1($S[i] \\in A^{1}$) or 0(not match, or not exists) $M^{2}_{i}$ means how many ways to match the 2-character decoders $A^2$ with $S[i-1:i+1]$. In this problem we have either 1($S[i] \\in A^{1}$) or 0(not match, or not exists) class Solution: def numDecodings(self, s: str) -\u003e int: one = {str(k):1 for k in range(1, 10)} two = {str(k):1 for k in range(10, 27)} dp = [0]*len(s) for i in range(len(s)): match_one = one.get(s[i], 0) match_two = two.get(s[i-1:i+1], 0) if i-1\u003e=0 else 0 one_bit = dp[i-1]*match_one if i-1\u003e=0 else match_one two_bit = dp[i-2]*match_two if i-2\u003e=0 else match_two dp[i] = one_bit + two_bit return dp[-1] Notes Use get() to take care of cases when s[i] is not in one Take care of the corner cases when $i \u003c= 2$, in which we can’t get dp values. Follow Up: Decode Ways 2 ","date":"2020-02-17","objectID":"/playground/decode-ways/:0:0","tags":["dynamic programming"],"title":"Decode Ways","uri":"/playground/decode-ways/"},{"categories":null,"content":"LeetCode 84 The idea is somewhat similar to Longest Valid Parentheses, in which: Maintain a stack which stores the indices Calculate the target function each time an element pops out from the stack Take care of the corner case with -1 when we don’t have the leftmost element Maintain a stack of indices, in which: All the elements in the stack follows ascending order. The elements popped out between adjacent elements remains in the stack are all taller, i.e. $$height[stack[j]] \u003c= height[k], \\ \\forall k \\in [stack[j-1], stack[j]]$$ So for any element $j$ popped out leftmost element is $stack[-1] + 1$ rightmost element is current index $i-1$ the width is $(i-1)-(stack[-1]+1)+1=i-stack[-1]-1$ Before height[3] enterAfter height[3] enter 0 1 2 3 4 x x x x x x x x x x x x 0 1 4 o o x x x x x when 3 pops, the leftmost is 1+1=2, rightmost is 4-1=3 class Solution: def largestRectangleArea(self, heights): stack, max_area = [], 0 heights.append(0) for i in range(len(heights)): while stack and heights[i] \u003c heights[stack[-1]]: popped_idx = stack.pop() height = heights[popped_idx] left = stack[-1] + 1 if stack else 0 right = max(i-1, 0) width = right - left + 1 max_area = max(max_area, height*width) stack.append(i) return max_area Notes: To ensure that left and right is valid, watch out for the corner cases when calculating. Append 0 to heights to ensure it pop out all the elements in the stack. ","date":"2020-02-16","objectID":"/playground/largest-rectangle-in-histogram/:0:0","tags":["stack"],"title":"Largest Rectangle in Histogram","uri":"/playground/largest-rectangle-in-histogram/"},{"categories":null,"content":"LeetCode 85 Borrow the idea from Largest Rectangle in Histogram, we can take the matrix as many histograms for each row level, in which we scan from top to down cumulatively to construct it: MatrixHistograms 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 2 0 1 0 0 1 2 0 0 2 3 1 1 3 4 2 class Solution: def largestRectangleArea(self, heights): stack, max_area = [], 0 heights.append(0) for i in range(len(heights)): while stack and heights[i] \u003c heights[stack[-1]]: popped_idx = stack.pop() height = heights[popped_idx] left = stack[-1] + 1 if stack else 0 right = max(i-1, 0) width = right - left + 1 max_area = max(max_area, height*width) stack.append(i) return max_area def maximalRectangle(self, matrix: List[List[str]]) -\u003e int: n_row, n_col = len(matrix), len(matrix[0]) max_area = 0 heights = [0]*n_col for i in range(n_row): for j in range(n_col): if matrix[i][j] == \"1\": heights[j] += 1 else: heights[j] = 0 max_area = max(max_area, self.largestRectangleArea(heights)) return max_area Notes Actually this is not a dp solution ","date":"2020-02-16","objectID":"/playground/maximal-rectangle/:0:0","tags":["dynamic programming"],"title":"Maximal Rectangle","uri":"/playground/maximal-rectangle/"},{"categories":null,"content":"This is about calculation Levenshtein Distance LeetCode 72 ","date":"2020-02-16","objectID":"/playground/edit-distance/:0:0","tags":["dynamic programming"],"title":"Edit Distance","uri":"/playground/edit-distance/"},{"categories":null,"content":"Levenshtein Distance Check Levenshtein Distance for information. Let $\\operatorname{lev}_{a,b}(i,j)$ be the distance between the first $i$ characters of $a$ and the first $j$ characters of $b$. $i$ and $j$ are 1-based indices. We have: $$ \\operatorname {lev}_{a,b}(i,j) = \\begin{cases} \\max(i,j) \u0026 \\text{ if $min(i,j)=0$}, \\\\ \\min {\\begin{cases} \\operatorname {lev}_{a,b}(i-1,j)+1 \\\\ \\operatorname {lev}_{a,b}(i,j-1)+1 \\\\ \\operatorname {lev}_{a,b}(i-1,j-1)+1_{(a_{i} \\neq b_{j})} \\end{cases}} \u0026{\\text{otherwise.}} \\end{cases} $$ ","date":"2020-02-16","objectID":"/playground/edit-distance/:1:0","tags":["dynamic programming"],"title":"Edit Distance","uri":"/playground/edit-distance/"},{"categories":null,"content":"Explanation For simplification, Let dp[i][j] represent $\\operatorname{lev}_{a,b}(i,j)$. We have 3 ways to reach dp[i][j]. Take a='abxy', b='abc' as an example: From dp[i-1][j]: Having abx -\u003e abc, we need one extra del step: abxy -\u003e abcy (del y)-\u003e abc From dp[i][j-1]: Having abxy -\u003e ab, we need one extra add step: abxy -\u003e ab (add c)-\u003e abc From dp[i-1][j-1]: Having abx -\u003e ab, we need one extra replace step: abxy -\u003e aby (replace y to c)-\u003e abc If a[i] == b[j], We could skip the replace step. ","date":"2020-02-16","objectID":"/playground/edit-distance/:1:1","tags":["dynamic programming"],"title":"Edit Distance","uri":"/playground/edit-distance/"},{"categories":null,"content":"DP Solution Then we consider the corner cases. We have 0-indexing system, in which situation we can achive -1 indexing. If i = -1 or j = -1, then we need to convert an empty string to another non-empty string, by inserting, the cost is the length of the non-empty string, in the above example, when i=-1 and j=1, we convert '' to ab, the cost is 2, dp[i][j] = j+1 = 2 If i = -1 and j = -1, we convert an empty string to another empty string, dp[-1][-1] = 0 There are two strategies to take care of these situations. Using a custom indexing system to take care of index overflow. We could use the dp as it was. Taking care of the indexing on the fly. This is less readable than method 1, personally not recommended. Add an extra row and column, start from (1,1) instead of (0,0), which has pros and cons compared to method 1. We use method 1: class Solution: def minDistance(self, word1: str, word2: str) -\u003e int: m, n = len(word1), len(word2) if m == 0 and n == 0: return 0 elif m == 0: return n elif n == 0: return m dp = [[float(\"inf\")]*n for _ in range(m)] def index(i, j): if i == -1 and j == -1: return 0 elif i == -1: return j+1 elif j == -1: return i+1 else: return dp[i][j] for i in range(m): for j in range(n): cost = int(word1[i]!=word2[j]) dp[i][j] = min(index(i-1,j)+1, index(i,j-1)+1, index(i-1,j-1)+cost) return dp[-1][-1] This method has problem to deal with empty string, in which we generate the result in index() on the fly, which is out of the dp table itself. Try method 3: class Solution: def minDistance(self, word1: str, word2: str) -\u003e int: m, n = len(word1), len(word2) dp = [[float(\"inf\")]*(n+1) for _ in range(m+1)] dp[0][0] = 0 for i in range(1, n+1): dp[0][i] = i for j in range(1, m+1): dp[j][0] = j for i in range(1, m+1): for j in range(1, n+1): cost = int(word1[i-1]!=word2[j-1]) dp[i][j] = min(dp[i-1][j]+1,dp[i][j-1]+1,dp[i-1][j-1]+cost) return dp[-1][-1] Notes: word index changes from i, j to i-1, j-1 in method 3. We may optimize the space of the code to use only two vectors, or even one. ","date":"2020-02-16","objectID":"/playground/edit-distance/:1:2","tags":["dynamic programming"],"title":"Edit Distance","uri":"/playground/edit-distance/"},{"categories":null,"content":"LeetCode 64 This is pretty much like unique paths. Let $dp[i][j]$ to be the total unique paths to $board[i][j]$, the state transision is easy to find: $$ dp[i][j] = \\min \\lbrace dp[i-1][j], dp[i][j-1] \\rbrace + grid[i][j] $$ As in the unique paths, we define a custom indexing function to take care of corner cases. class Solution: def minPathSum(self, grid: List[List[int]]) -\u003e int: n_row, n_col = len(grid), len(grid[0]) min_sum = float(\"inf\") def index(i, j): if i in range(n_row) and j in range(n_col): return dp[i][j] else: return float(\"inf\") dp = [[float(\"inf\")]*n_col for _ in range(n_row)] dp[0][0] = grid[0][0] for i in range(n_row): for j in range(n_col): if i == 0 and j == 0: continue dp[i][j] = min(index(i-1, j), index(i, j-1)) + grid[i][j] return dp[n_row-1][n_col-1] ","date":"2020-02-16","objectID":"/playground/minimal-path-sum/:0:0","tags":["dynamic programming"],"title":"Minimal Path Sum","uri":"/playground/minimal-path-sum/"},{"categories":null,"content":"Walk over all grids without duplicate, with obstacles ","date":"2020-02-15","objectID":"/playground/unique-paths-3/:0:0","tags":["backtracking","dfs"],"title":"Unique Paths 3","uri":"/playground/unique-paths-3/"},{"categories":null,"content":"Description See LeetCode 980 ","date":"2020-02-15","objectID":"/playground/unique-paths-3/:1:0","tags":["backtracking","dfs"],"title":"Unique Paths 3","uri":"/playground/unique-paths-3/"},{"categories":null,"content":"DFS Backtracking Solution Unlike the DP solutions to Unique path 1 and 2, we cannot reuse calculated paths cuz we can walk forth and back into the same arange (i, j). And the start and end position varies. Using a DFS. class Solution: def uniquePathsIII(self, grid: List[List[int]]) -\u003e int: n_row, n_col = len(grid), len(grid[0]) self.count, empty = 0, 1 # find start and end, count empty grids for i in range(n_row): for j in range(n_col): if grid[i][j] == 0: empty += 1 if grid[i][j] == 1: start = [i, j] if grid[i][j] == 2: end = [i, j] def search(i, j, empty): if i in range(n_row) and j in range(n_col) and grid[i][j] \u003e= 0: if ([i, j] == end) and (empty == 0): self.count += 1 return grid[i][j] = -2 search(i-1, j, empty-1) search(i+1, j, empty-1) search(i, j-1, empty-1) search(i, j+1, empty-1) grid[i][j] = 0 search(start[0], start[1], empty) return self.count Note We initialize empty with 1, cuz the start of the search counts. Search in a grid is frequently used, we need to restore the element after search. The search function could be memorized to accelerate. ","date":"2020-02-15","objectID":"/playground/unique-paths-3/:2:0","tags":["backtracking","dfs"],"title":"Unique Paths 3","uri":"/playground/unique-paths-3/"},{"categories":null,"content":"Count all paths from a to b, with obstacles. ","date":"2020-02-15","objectID":"/playground/unique-paths-2/:0:0","tags":["dynamic programming"],"title":"Unique Paths 2","uri":"/playground/unique-paths-2/"},{"categories":null,"content":"Description See LeetCode 63 ","date":"2020-02-15","objectID":"/playground/unique-paths-2/:1:0","tags":["dynamic programming"],"title":"Unique Paths 2","uri":"/playground/unique-paths-2/"},{"categories":null,"content":"Dynamic Programming Solution Let $dp[i][j]$ to be the total unique paths to $board[i][j]$, the state transision is easy to find: dp[i-1][j] ^ | dp[i][j-1]\u003c--dp[i][j] $$ dp[i][j] = \\begin{cases} dp[i-1][j] + dp[i][j-1], \u0026 \\text{$board[i][j] = 0$} \\\\ 0, \u0026 \\text{$board[i][j] = 1$} \\end{cases} $$ Watch out for the initial condition: we con’t initialize all values to 1 on the first row and first col. class Solution: def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -\u003e int: n_row, n_col = len(obstacleGrid), len(obstacleGrid[0]) dp = [[0]*n_col for _ in range(n_row)] if obstacleGrid[0][0] != 1: dp[0][0] = 1 def index(i, j): if i in range(n_row) and j in range(n_col): return dp[i][j] else: return 0 for i in range(n_row): for j in range(n_col): if i == 0 and j == 0: continue if obstacleGrid[i][j] == 0: dp[i][j] = index(i-1, j) + index(i, j-1) return dp[n_row-1][n_col-1] ","date":"2020-02-15","objectID":"/playground/unique-paths-2/:2:0","tags":["dynamic programming"],"title":"Unique Paths 2","uri":"/playground/unique-paths-2/"},{"categories":null,"content":"Count all paths from a to b ","date":"2020-02-15","objectID":"/playground/unique-paths/:0:0","tags":["dynamic programming"],"title":"Unique Paths","uri":"/playground/unique-paths/"},{"categories":null,"content":"Description See LeetCode 62 ","date":"2020-02-15","objectID":"/playground/unique-paths/:1:0","tags":["dynamic programming"],"title":"Unique Paths","uri":"/playground/unique-paths/"},{"categories":null,"content":"Dynamic Programming Solution Let $dp[i][j]$ to be the total unique paths to $board[i][j]$, the state transision is easy to find: dp[i-1][j] ^ | dp[i][j-1]\u003c--dp[i][j] $$ dp[i][j] = dp[i-1][j] + dp[i][j-1] $$ Define a custom indexing function to take care of corner cases. The filling order is described as follow for $row=3, col=4$: 0 1 2 3 4 5 6 7 8 9 10 11 class Solution: def uniquePaths(self, m: int, n: int) -\u003e int: dp = [[0]*n for _ in range(m)] dp[0][0] = 1 def index(i, j): if i in range(m) and j in range(n): return dp[i][j] else: return 0 for i in range(m): for j in range(n): if i == 0 and j == 0: continue dp[i][j] = index(i-1, j) + index(i, j-1) return dp[m-1][n-1] ","date":"2020-02-15","objectID":"/playground/unique-paths/:2:0","tags":["dynamic programming"],"title":"Unique Paths","uri":"/playground/unique-paths/"},{"categories":null,"content":"Description Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Example: Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Follow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle. ","date":"2020-02-15","objectID":"/playground/maximum-subarray/:1:0","tags":["dynamic programming"],"title":"Maximum Subarray","uri":"/playground/maximum-subarray/"},{"categories":null,"content":"Dynamic Programming Solution Let $dp[i]$ to be the maximum subarray value that ends at $nums[i]$, which is all the possible subarray sum which ends at $i$, i.e. $$ \\begin{aligned} dp[i] \u0026= \\max_{j \\in [0, i] } \\left\\lbrace \\sum\\limits_{k=j}^{i} nums[k] \\right\\rbrace \\\\ \u0026= \\max_{j \\in [0, i] } \\left\\lbrace \\sum\\limits_{k=j}^{i-1} nums[k],0 \\right\\rbrace + nums[i] \\\\ \u0026= \\max \\left\\lbrace dp[i-1], 0 \\right\\rbrace + nums[i] \\end{aligned} $$ class Solution: def maxSubArray(self, nums: List[int]) -\u003e int: dp, max_sum = [0]*len(nums), -float(\"inf\") for i in range(len(nums)): dp[i] = (max(dp[i-1], 0) if i-1\u003e=0 else 0) + nums[i] max_sum = max(max_sum, dp[i]) return max_sum Notes When calculating dp[i-1], watch out for the corner case. Generate max_sum in each iter on the fly. ","date":"2020-02-15","objectID":"/playground/maximum-subarray/:2:0","tags":["dynamic programming"],"title":"Maximum Subarray","uri":"/playground/maximum-subarray/"},{"categories":null,"content":"Description Given a string containing just the characters ‘(’, ‘)’, ‘{’, ‘}’, ‘[’ and ‘]’, determine if the input string is valid. An input string is valid if: Open brackets must be closed by the same type of brackets. Open brackets must be closed in the correct order. Note that an empty string is also considered valid. ","date":"2020-02-14","objectID":"/playground/valid-parentheses/:1:0","tags":["stack"],"title":"Valid Parentheses","uri":"/playground/valid-parentheses/"},{"categories":null,"content":"Stack Solution Keep a stack When a ( arrives, push it into the stack When a ) arrives, compare it with the top of stack, if it forms a pair, pop it, else push it. class Solution: def isValid(self, s: str) -\u003e bool: left_set = {'(', '{', '['} right_set = {')', '}', ']'} get_pair = { ')': '(', '}': '{', ']': '[' } stack = [] for p in s: if p in left_set: stack.append(p) else: if stack and stack[-1] == get_pair[p]: stack.pop() else: stack.append(p) return len(stack) == 0 Notes We use a dict to check if the pair is valid. Watch out for corner case before indexig. Follow up ","date":"2020-02-14","objectID":"/playground/valid-parentheses/:2:0","tags":["stack"],"title":"Valid Parentheses","uri":"/playground/valid-parentheses/"},{"categories":null,"content":"Description Given a string containing just the characters ‘(’ and ‘)’, find the length of the longest valid (well-formed) parentheses substring. Example 1: Input: \"(()\" Output: 2 Explanation: The longest valid parentheses substring is \"()\" Example 2: Input: \")()())\" Output: 4 Explanation: The longest valid parentheses substring is \"()()\" ","date":"2020-02-14","objectID":"/playground/longest-valid-parentheses/:1:0","tags":["dynamic programming","stack"],"title":"Longest Valid Parentheses","uri":"/playground/longest-valid-parentheses/"},{"categories":null,"content":"Dynamic Programming Solution Let $dp[i]$ to be the length of the longest valid parentheses which ends at $i$‘th position. $s[i]$ must be ) to have a valid parentheses, else $dp[i] = 0$ Depends on s[i-1], we have 2 situations: $s[i-1] = ($ … i-2 i-1 i … . ( ) dp[i-2] 0 dp[i] dp[i] = dp[i-2] + 2 $s[i-1] = )$ we have 2 situations, only needs to consider $s[i-dp[i-1]-1] = ($ i-dp[i-1]-2 i-dp[i-1]-1 i-dp[i-1] … i-1 i ? ( ( … ) ) dp[i-dp[i-1]-2] 0 0 … dp[i-1] dp[i] dp[i] = dp[i-dp[i-1]-2] + dp[i-1] + 2 Combining all the situations above, we have: class Solution: def longestValidParentheses(self, s: str) -\u003e int: n = len(s) dp = [0]*n _max = 0 for i in range(n): if s[i] == ')' and (i-1\u003e=0): if s[i-1] == '(': dp[i] = (dp[i-2] if (i-2\u003e=0) else 0) + 2 _max = max(_max, dp[i]) else: if (i-dp[i-1]-1\u003e=0) and s[i-dp[i-1]-1] == '(': dp[i] = dp[i-1] + 2 + (dp[i-dp[i-1]-2] if (i-dp[i-1]-2\u003e=0) else 0) _max = max(_max, dp[i]) return _max Though the DP solution is not quite intuitive for me. ","date":"2020-02-14","objectID":"/playground/longest-valid-parentheses/:2:0","tags":["dynamic programming","stack"],"title":"Longest Valid Parentheses","uri":"/playground/longest-valid-parentheses/"},{"categories":null,"content":"Stack Solution Maintain a stack, which borrows the idea from Valid Parentheses When a ( arrives, push it into the stack When a ) arrives, compare it with the top of stack, if it forms a pair, pop it, else push it. After the whole process, the items left in the stack are those not having valid pairs. On the other side, other elements are valid pairs. We just need to find longest space between those elements in the stack. So we push index of the elements, instead of the element itself. class Solution: def longestValidParentheses(self, s: str) -\u003e int: stack: List[int] = [] max_len: int = 0 for i in range(len(s)): if s[i] == '(': stack.append(i) elif s[i] == ')': if stack and s[stack[-1]] == '(': stack.pop() last_not_paired = stack[-1] if stack else -1 max_len = max(max_len, i-last_not_paired) else: stack.append(i) return max_len Notes We generate the max length on the fly. Each time a pair generated, we calculate space between current element and last element remains in the stack, which is the last index that has not been paired yet, which is the current length of the generated valid substring. Watch out for corner case: if the stack is empty, we cannot find last element that has not been pairs, just put -1 to it shoud be fine. ","date":"2020-02-14","objectID":"/playground/longest-valid-parentheses/:3:0","tags":["dynamic programming","stack"],"title":"Longest Valid Parentheses","uri":"/playground/longest-valid-parentheses/"},{"categories":null,"content":"Description Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: Input: \"babad\" Output: \"bab\" Note: \"aba\" is also a valid answer. Example 2: Input: \"cbbd\" Output: \"bb\" ","date":"2020-02-11","objectID":"/playground/longest-palindromic-substring/:1:0","tags":["dynamic programming"],"title":"Longest Palindromic Substring","uri":"/playground/longest-palindromic-substring/"},{"categories":null,"content":"Dynamic Programming Solution Let dp[i][j] indicates that s[i:j+1] is a palindromic sequence, we have $$ dp[i][j] = \\begin{cases} True, \u0026 \\text{$i$ = $j$} \\\\ s[i] = s[j], \u0026 \\text{$j$ - $i$ = 1} \\\\ (s[i] = s[j]) \\ \\text{and} \\ dp[i+1][j-1], \u0026 \\text{$j$ - $i$ \u003e 1} \\end{cases} $$ dp[i][j] / dp[i+1][j-1] Filling order is as described below: i\\j b a b a d b 0 1 3 6 10 a 2 4 7 11 b 5 8 12 a 9 13 d 14 class Solution: def longestPalindrome(self, s: str) -\u003e str: max_substr = \"\" dp = [[False]*len(s) for _ in range(len(s))] for j in range(len(s)): for i in range(j+1): if i == j: dp[i][j] = True elif j-i == 1: dp[i][j] = s[i] == s[j] else: dp[i][j] = (s[i] == s[j]) and dp[i+1][j-1] if dp[i][j] and j-i+1 \u003e len(max_substr): max_substr = s[i:j+1] return max_substr ","date":"2020-02-11","objectID":"/playground/longest-palindromic-substring/:2:0","tags":["dynamic programming"],"title":"Longest Palindromic Substring","uri":"/playground/longest-palindromic-substring/"},{"categories":null,"content":"Palindrome Ad-hoc Solution Set $i$ to be the center of palindrome sunstring, $j$ to be the radius, we can expand from center to side to valid the substring: Odd length substring: Expand from $(i, i)$, in which $j=0$, $L=i-j, R=i+j$, length of the substring is $2j+1$, $r_{max} = i$ Even length substring: Expand from $(i-1, i)$, in which $j=0$, $L=i-1-j, R=i+j$, length of the substring is $2j+2$, $r_{max} = i$ i-j i-1 i i+1 i+r 1 2 3 4 5 4 3 2 1 i-1 i 1 2 3 4 5 5 4 3 2 1 class Solution: def longestPalindrome(self, s: str) -\u003e str: n, max_len, max_str = len(s), 0, \"\" for i in range(n): for j in range(i+1): l, r = i-j, i+j if l\u003e=0 and r\u003cn and s[l]==s[r]: if max_len \u003c 2*j+1: max_len = 2*j+1 max_str = s[l:r+1] else: break for j in range(i+1): l, r = i-1-j, i+j if l\u003e=0 and r\u003cn and s[l]==s[r]: if max_len \u003c 2*j+2: max_len = 2*j+2 max_str = s[l:r+1] else: break return max_str ","date":"2020-02-11","objectID":"/playground/longest-palindromic-substring/:3:0","tags":["dynamic programming"],"title":"Longest Palindromic Substring","uri":"/playground/longest-palindromic-substring/"},{"categories":null,"content":"Manacher Algorithm Check wiki~ it is O(n) ","date":"2020-02-11","objectID":"/playground/longest-palindromic-substring/:4:0","tags":["dynamic programming"],"title":"Longest Palindromic Substring","uri":"/playground/longest-palindromic-substring/"},{"categories":null,"content":"Description Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for ‘?’ and ‘*’. ‘?’ Matches any single character. ‘*’ Matches any sequence of characters (including the empty sequence). The matching should cover the entire input string (not partial). Note: s could be empty and contains only lowercase letters a-z. p could be empty and contains only lowercase letters a-z, and characters like ? or *. Example 1: Input: s = \"aa\" p = \"a\" Output: false Explanation: \"a\" does not match the entire string \"aa\". Example 2: Input: s = \"aa\" p = \"*\" Output: true Explanation: '*' matches any sequence. ","date":"2020-02-11","objectID":"/playground/wildcard-matching/:1:0","tags":["dynamic programming","backtracking"],"title":"Wildcard Matching","uri":"/playground/wildcard-matching/"},{"categories":null,"content":"Backtracking Solution This is an easier version of Regular Expression Matching, since we don’t need to check character matching when we get a wildcard. The logic is: We define search(si, pi) as the search function, which means if s[si:] is matched by p[pi:]. First check if it is a wildcard: pi \u003c len(p) and p[pi] == \"*\" If it is, match this or not: search(si+1, pi) or search(si, pi+1) If not, just match current character: si \u003c len(s) and p[pi] in {s[si], \"?\"} Search next level: search(si+1, pi+1) class Solution: def isMatch(self, s: str, p: str) -\u003e bool: def search(si: int, pi: int) -\u003e bool: if pi == len(p): return si == len(s) wildcard: bool = pi \u003c len(p) and p[pi] == \"*\" if wildcard: return (si \u003c len(s) and search(si+1, pi)) or search(si, pi+1) else: match: bool = si \u003c len(s) and p[pi] in {s[si], \"?\"} return match and search(si+1, pi+1) return search(0, 0) Again, it could be optimized through cache. ","date":"2020-02-11","objectID":"/playground/wildcard-matching/:2:0","tags":["dynamic programming","backtracking"],"title":"Wildcard Matching","uri":"/playground/wildcard-matching/"},{"categories":null,"content":"Dynamic Programming First we can construct state transision from the backtracking method. Jut let search(si, pi) to be dp[si][pi] dp[si][pi] \u003c--dp[si][pi+1] ^ \\ | \\ dp[si+1][pi] dp[si+1][pi+1] We have a (len(s) + 1, len(p) +1) matrix, the iteration order is described below. Starting from dp[len(s)][len(p)-1], we need to get dp[0][0] Set dp[len(s)][len(p)] = True, which indicates our backtracking terminal condition(find a match). s\\p * a * b a end F b F c F e F b F start T class Solution: def isMatch(self, s: str, p: str) -\u003e bool: dp: List[List[bool]] = [[False]*(len(p)+1) for _ in range(len(s)+1)] dp[len(s)][len(p)] = True for si in range(len(s), -1, -1): for pi in range(len(p)-1, -1, -1): wildcard: bool = pi \u003c len(p) and p[pi] == \"*\" if wildcard: dp[si][pi] = (si \u003c len(s) and dp[si+1][pi]) or dp[si][pi+1] else: match: bool = si \u003c len(s) and p[pi] in {s[si], \"?\"} dp[si][pi] = match and dp[si+1][pi+1] return dp[0][0] ","date":"2020-02-11","objectID":"/playground/wildcard-matching/:3:0","tags":["dynamic programming","backtracking"],"title":"Wildcard Matching","uri":"/playground/wildcard-matching/"},{"categories":null,"content":"We can easily optimize a recursion by using a cache, which loads calculated function calls during the recursion process. from functools import wraps def memo(func): cache = dict() @wraps(func) def wrap(*args): if args not in cache: cache[args] = func(*args) return cache[args] return wrap Some conditional branches could be merged by and operator. @memo def fib(n: int) -\u003e int: if n in {0, 1}: return 1 else: return fib(n-1) + fib(n-1) ","date":"2020-02-11","objectID":"/playground/optimize-recursion/:0:0","tags":null,"title":"Optimize Recursion","uri":"/playground/optimize-recursion/"},{"categories":null,"content":"Description Given an input string (s) and a pattern (p), implement regular expression matching with support for ‘.’ and ‘*’. '.' Matches any single character. '*' Matches zero or more of the preceding element. The matching should cover the entire input string (not partial). Note: s could be empty and contains only lowercase letters a-z. p could be empty and contains only lowercase letters a-z, and characters like . or *. Example : Input: s = \"aab\" p = \"a*b\" Output: true Explanation: \"a*\" does not match the entire string \"aa\". ","date":"2020-02-10","objectID":"/playground/regular-expression-matching/:1:0","tags":["dynamic programming","backtracking"],"title":"Regular Expression Matching","uri":"/playground/regular-expression-matching/"},{"categories":null,"content":"Backtracking Solution We define search(si, pi) as the search function, which means if s[si:] is matched by p[pi:]. First we need to check if p[pi] is a wildcard: p[pi+1] == \"*\", and don’t forget the corner case: pi+1 \u003c len(p) Not wildcard: Just check if s[si] and p[pi] is same character: si \u003c len(s) and s[si] == p[pi] If it is, search next level: search(si+1, pi+1) If not, just return False, search ends here. Wildcard: Same as step 1, check if it’s the same character: If it is, that’s a valid wildcard match. But we could still choose to match or skip this wildcard: search(si+1, pi) or search(si, pi+2) If not, the only choice is to skip this wildcard: search(si, pi+2) class Solution: def isMatch(self, s: str, p: str) -\u003e bool: def search(si: int, pi: int) -\u003e bool: if pi == len(p): return si == len(s) wildcard: bool = pi+1 \u003c len(p) and p[pi+1] == \"*\" match: bool = si \u003c len(s) and p[pi] in {s[si], \".\"} if wildcard: if match: return search(si+1, pi) or search(si, pi+2) else: return search(si, pi+2) else: if march: search(si+1, pi+1) else: return False return search(0, 0) Some conditional branches could be merged by and operator. class Solution: def isMatch(self, s: str, p: str) -\u003e bool: def search(si: int, pi: int) -\u003e bool: if pi == len(p): return si == len(s) wildcard: bool = pi+1 \u003c len(p) and p[pi+1] == \"*\" match: bool = si \u003c len(s) and p[pi] in {s[si], \".\"} if wildcard: return (match and search(si+1, pi)) or search(si, pi+2) else: return match and search(si+1, pi+1) return search(0, 0) The code could be easily optimized using a cache to avoid duplicate calculation. See Optimize Recursion. Always pay attention to corner cases in recursion. Below is an example for matching aab and c*a*ab, with the dicision tree of the backtracking search path. graph TD; subgraph DecisionTree DROOT(Wildcard?)--\u003e|Y|DL(char?); DL--\u003e|Y|DLL(match?); DLL--\u003e|Y|DLLL(si+1, pi); DLL--\u003e|N|DLLR(si, pi+2); DL--\u003e|N|DLR(si, pi+2); DROOT--\u003e|N|DR(char?); DR--\u003e|Y|DRL(si+1, pi+1); DR--\u003e|N|DRR(False); end subgraph MatchExample ROOT(aab,c*a*ab)--\u003e|skip c*|L(aab, a*ab); L--\u003e|match a*|LL(ab,a*ab) L--\u003e|skip a*|LR(aab,ab); LL--\u003e|match a*|LLL(b,a*ab); LLL--\u003e|skip a*|LLLL(b,ab); LL--\u003e|skip a*|LLR(ab,ab); LR--\u003e|match a|LRL(ab,b); LLR--\u003e|match a|LLRL(b,b); LLRL--\u003e|match b|LLRLL([END]); end ","date":"2020-02-10","objectID":"/playground/regular-expression-matching/:2:0","tags":["dynamic programming","backtracking"],"title":"Regular Expression Matching","uri":"/playground/regular-expression-matching/"},{"categories":null,"content":"Dyamic Programming First we can construct state transision from the backtracking method. Jut let search(si, pi) to be dp[si][pi] dp[si][pi] \u003c--------------- dp[si][pi+2] ^ \\ | \\ dp[si+1][pi] dp[si+1][pi+1] We have a (len(s) + 1, len(p) +1) matrix, the iteration order is described below. Starting from dp[len(s)][len(p)-1], we need to get dp[0][0] Set dp[len(s)][len(p)] = True, which indicates our backtracking terminal condition(find a match). s\\p c * a * a b a end F a F b F start T The core logic is pretty straightforward like backtracking solution: class Solution: def isMatch(self, s: str, p: str) -\u003e bool: dp: List[List[bool]] = [[False]*(len(p)+1) for _ in range(len(s)+1)] dp[len(s)][len(p)] = True for si in range(len(s), -1, -1): for pi in range(len(p)-1, -1, -1): wildcard: bool = pi+1 \u003c len(p) and p[pi+1] == \"*\" match: bool = si \u003c len(s) and p[pi] in {s[si], \".\"} if wildcard: dp[si][pi] = (match and dp[si+1][pi]) or dp[si][pi+2] else: dp[si][pi] = match and dp[si+1][pi+1] return dp[0][0] Notes: This is similar to Distinct Subsequences ","date":"2020-02-10","objectID":"/playground/regular-expression-matching/:3:0","tags":["dynamic programming","backtracking"],"title":"Regular Expression Matching","uri":"/playground/regular-expression-matching/"},{"categories":null,"content":"如何将艺术风格量化? 基于深度学习的风格迁移是个很有趣也很浪漫的领域 ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:0:0","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"Texture Synthesis Using Convolutional Neural Networks 2015a, Gatys 将图片输入预训练好的神经网络(VGG-19)中， 生成每层的特征 将灰度图同样输入预训练好的神经网络中，生成每层的特征 计算每层特征的Gram矩阵，计算两张图片对应层的Gram矩阵的平方误差，再将每层的误差按权求和，获得损失函数 计算损失函数对灰度图每个像素的梯度，进行梯度下降，最终得到输出图。这里进行梯度下降的不是模型参数，而是输入像素。这也是Deep Dream的主要思路。 其中最重要的贡献是定义了纹理，即特征图的Gram矩阵。网络第$l$层的纹理误差为 $$ E_{l}(S, \\hat{X}) = \\frac{1}{4C_{in}^2C_{out}^2}\\sum\\limits_{i,j}(G_{ij} - \\hat{G}_{ij})^2 $$ 其中$S$是需要学习的纹理(Texture Style)的图片，$\\hat{X}$代表了需要生成的图片，总误差就是各层误差的加权和： $$ \\mathcal{L}_{style}(S, \\hat{X}) = \\sum\\limits_{l}w_{l}E_{l} $$ 计算Gram矩阵的方法如下。Gram矩阵可以视为有偏的协方差矩阵 ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:1:0","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"A Neural Algorithm of Artistic Style 2015b, Gatys 本文增加了计算内容相似性的部分。内容相似性就是特征图的平方误差。 $$ \\mathcal{L}_{content}(P, \\hat{X}) = \\frac{1}{2}\\sum\\limits_{i,j}(F_{ij} - \\hat{F}_{ij})^2 $$ 其中$P$代表了需要渲染的图片，$\\hat{X}$代表了需要生成的图片，$F_{ij}$和$\\hat{F}_{ij}$代表了他们在某一层的特征图。总的损失是内容和风格损失的加权和： $$ \\mathcal{L}(P, S, \\hat{X}) = \\alpha\\mathcal{L}_{content}(P, \\hat{X}) + \\beta\\mathcal{L}_{style}(S, \\hat{X}) $$ 其中alpha和beta可以使用L-BFGS训练 ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:2:0","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"Perceptual Losses for Real-Time Style Transfer and Super-Resolution Mar 2016, Justin Johnson 以上方法生成一张图是通过梯度下降的方法，也就是一次训练过程，耗时较长，计算资源消耗也大。本工作将生成图片的过程变成了一次inference，降低了计算量，提升了速度。 本文对风格和内容损失有更现代的定义： ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:3:0","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"Feature Reconstruction Loss 网络$j$层对应的content loss $$ \\mathcal{L^{j}_{content}}(y, \\hat{y}) = \\frac{1}{C_{j}H_{j}W_{j}} \\Vert \\phi_{j}(y) - \\phi_{j}(\\hat{y}) \\Vert^{2}_{2} $$ ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:3:1","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"Style Reconstruction Loss 网络$j$层对应的style loss $$ \\mathcal{L^{j}_{style}}(y, \\hat{y}) = \\frac{1}{C_{j}H_{j}W_{j}}\\Vert G^{\\phi}_{j}(y) - G^{\\phi}_{j}({\\hat{y}}) \\Vert^{2}_{F} $$ 本文使用一个resnet来学习生成图片的变换，而将内容和风格损失作为损失函数，其计算方式通过一个固定参数的VGG16来生成。去掉风格损失之后还可以作为超分辨率模型使用。此外3.3节还提到了两个额外的损失函数，Pixel Loss和Total Variation Regularization。后者在风格迁移任务中也被使用到。 ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:3:2","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"Fast Patch-based Style Transfer of Arbitrary Style Dec 2016, Tian Qi Chen 实现了任意风格任意图片的迁移， ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:4:0","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"Style Swap Content图片产生特征图$\\phi(C)$ Style图片产生特征图$\\phi(S)$ 用$\\phi(S)$做卷积核扫过$\\phi(C)$,寻找最相似的 参考blog ","date":"2018-10-08","objectID":"/2018-10-08-style-transfer/:4:1","tags":["style transfer"],"title":"风格迁移(Style Transfer)","uri":"/2018-10-08-style-transfer/"},{"categories":null,"content":"因为本域名没备案, 七牛云收回了临时链接, 本blog之前所有文章的图片都挂了, 需要一段时间抢修, 见谅 目前切换阿里云oss 七牛云太辣鸡了…没有备案域名, 连图片都取不回来了 ","date":"2018-10-08","objectID":"/2018-10-08-imgshare-dead/:0:0","tags":["chobits"],"title":"七牛云图床挂了","uri":"/2018-10-08-imgshare-dead/"},{"categories":null,"content":"Kaggle 实例分割比赛小结. 比赛花了我1800在阿里云上训练模型, 最后还因为没提交模型的checksum而取消了成绩. 包大人, 冤啊! ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:0:0","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"难点 训练数据少. 黑白600, 彩色100 细胞核形状多样, 有的十分细长 细胞核紧挨, 有重叠部分, 难以使用unet区分(unet先预测mask再用watershed(分水岭算法)分割) 黑白染色和彩色染色的图片细胞核区别显著. 彩色图片训练数据少, 颜色种类不同, 模型表现差 测试集具备训练集没有的细胞核类型. 甚至教科书中的手绘插图 ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:1:0","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"解决方案 实例分割可用选项: U-Net, 比赛初期的主流选择. 这是二分类网络, 且只能用于语义分割而不是instance level的实例分割(无法区分不同细胞核), 需要用后处理(watershed算法)分割拥挤细胞核. 直觉上比Mask RCNN这种自带目标检测的有劣势 FCIS(全卷积网络用来做实例分割), 比U-Net的优势是可以做实例分割, 实际上就是先detection之后, 开一个新分支, 把ROI进一步划分(成9块), 专门判断ROI里面那些内容是前景. 这么做还是比较粗糙, Mask RCNN论文里也提到 But FCIS exhibits systematic errors on overlapping instances. 对重叠部分表现很差. 敲定Mask RCNN之后, 选择合适的Backbone. ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:2:0","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"Mask-RCNN base model SE-ResNeXt50-FPN BackBone, 训练集不大, 所以需要一个尽量小巧并且高效的Backbone. 最初想到的是ResNet50. 做了一些research之后发现还可以用Xception里depthwise conv进一步减肥, 于是使用ResNeXt50. 这场比赛比较大的困难是不同图片里的细胞核大小天差地别, 因此很自然上FPN. 最后实验性地加入了SE 模块, 免费提分的工具何不试试. 图片尺寸只有256, 而coco数据集图片一般是1024*1024尺寸的. 因此取消ResNeXt的conv1的pooling层获得类似尺寸的FPN-Feature Map 更多细长比例Anchor Boxes捕捉细长并紧挨的细胞核, 并且根据mask绘制更紧密的gt(训练集只提供了mask). 但是考虑到太过细长的anchorbox超出了3x3卷积核的感受野, 是不是需要更丰富的卷积核呢?(提升点) RPN的cls head使用了focal loss, 这样也避免了OHEM的复杂性. reg head使用了weighted_smooth_l1, 加上了权重, 给foreground和scale较小的实例更高的权重 UnitBox用于bbox regression, UnitBox: An Advanced Object Detection Network: RPN的Reg Head原来使用的是weighted_smooth_l1, (未来得及使用iou loss替代, 提升点. 因为有大量紧密重叠的细胞核, 类似于人群中人脸识别的特点, iou loss对小的细胞核有帮助, 可以回归得更准确) 使用UnitBox的encoding方式主要是因为没有exp, 而训练伊始exp这部分很容易爆炸, 强制做clip收敛又慢, 因此做了这样的改进. 因为是细胞核(偏向于圆形)且为二分类, Mask Head使用Dice Loss, 比cross entropy效果更好一些. dice loss蕴含了强调了中心点的重要性. 因为最难处理的情况是proposal包含了拥挤的细胞核, mask head的任务是找出中心的细胞核.此时mask的每一个pixel都是相同的权重, 这不符合直觉, 需要dice loss来进行强调 ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:2:1","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"预处理 CLAHE(限制对比度自适应直方图均衡算法), 对医学图像增强效果好 Stain Normalize: A. Vahadane et al., ‘Structure-Preserving Color Normalization and Sparse Stain Separation for Histological Images 维持不同染色细胞颜色统一 ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:2:2","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"Train Time Augment 随机裁剪到256*256, 若不够则zero padding维持长宽比 0.5~1.5的随机缩放 水平/垂直翻转 90°旋转 随机高斯模糊 高斯噪音 随机对比度 random_hue_transform做数据增强: 有彩色和黑白图同时存在, 彩色图片染色风格不同 ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:2:3","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"Test Time Augment padding 到anchor的整数倍, 保证anchor可以覆盖到边缘 scale: 0.6, 0.8, 1.0, 1.2, 1.4, rotate: 90, 180, 270 把做了各种增强之后的rpn proposal ensemble, 之后再nms mask instance clustering. 以iou为依据进行聚类, 聚类核心为所有mask的交集(按交集/并集处理之后发现交集比较好) mask instance ensemble, 叠加之后取均值. 最佳阈值根据验证集计算出 根据mask 尺寸进行false positive suppression, 由于基本策略是先提升recall, 再抑制FP, 由于同一张图里的细胞大小差异不会过于悬殊, 因为是显微镜拍摄的图片, 显微镜中是切片, 细胞核距离镜头都是一样的. 直接使用一维异常值检测算法MAD(平均绝对偏差)过滤小的mask. 一开始使用DBSCAN, 后来发现MAD更快而且效果差不多. 因为这个任务比较简单. ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:2:4","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"训练策略 Cyclical Learning Rates for Training Neural Networks, SetpLR, DecayLR, etc, 最后还是用了StepLR 训练时的run-in: 初始阶段每个batch中掺入了小部分gt, 既保证了RCNN能有输出, 也保证了Mask head快速收敛. 否则需要很长时间才能收敛. 后期撤除gt 训练流程是依次训练三个head, 最后一起训练 ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:2:5","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"Winner Strategy 1st 4th 一文介绍3篇无需Proposal的_实例分割_论文 待续 ","date":"2018-07-13","objectID":"/2018-07-13-dsb2018/:2:6","tags":["Kaggle"],"title":"Data Science Bowl 2018 比赛小结","uri":"/2018-07-13-dsb2018/"},{"categories":null,"content":"存货, 介绍了Mask-RCNN和具体的实现细节 ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:0:0","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"RPN ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:1:0","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"网络结构 Mask RCNN的RPN是multi head的, 也就是每一个FPN的特征图对应一个RPN head, 最后把输出concatenate到一块. 对于单个RPN Head, 输入为四张FPN的特征图之一: Batch, Input_channel=4, Height=D, Width=D layer parameter output Conv1 C=512, K=3,S=1,P=1 B, 512, D, D Conv_cls C=$(2\\times s \\times r)$, K=1, S=1 B, 2k, D, D Conv_reg C=$(4\\times s \\times r)$, K=1, S=1 B, 4k, D, D 其中$s$代表anchor尺寸, 由于FPN输出4个通道, 恰好对应了4种scale, 因此这里天然用4是实现时的方案. $r$代表anchor长宽比, $s \\times r$个anchor是由$s$种尺寸, $r$种长宽比组合而成的. conv1是一个3x3的conv+relu+dropout层, 输出被cls和reg两个分支接收. 早期论文使用fc, 现在已经全卷积化, 更轻更快. 而且更准确(保留了空间信息) Conv_cls, 全卷积网络(没有激活函数), 输出在DxD个位置中, 每个位置上$s \\times r$个anchor的前景/背景的概率, 因此在每个位置上都有$(2\\times s \\times r)$个score.如果前景/背景概率和为1, 则只需要一维变量即可 Conv_reg, 全卷积网络(没有激活函数), 输出在DxD个位置中, 每个位置上$s \\times r$个anchor的坐标修正, 因此在每个位置上都有$(4\\times s \\times r)$个score ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:1:1","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"正负样本与损失函数 正负样本定义与batch内的重采样 IOU \u003e 0.7被标记为正样本, 或者是某一个instance中iou最高的. 后者是为了防止有些实例没有对应的proposal分配 由于负样本占绝大部分, 需要按照正负样本比例bootstrapping至1:1 损失函数 cls head只需要分类前景和背景, 因此就是log loss. (改进: focal loss) reg head是一个回归模型, 只计算正样本的损失. 使用smooth L1 loss (改进: weighted smooth L1, 或者IOU loss) ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:1:2","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"RoiAlign RoiAlign被用于根据RPN proposal或者RCNN proposal从特征图上切割固定尺寸的特征图给RCNN或者Mask Head 两张图解释一下ROI Align与ROI Pooling的区别. 使用ROI Pooling, proposal在feature map上的坐标大概率不是整数(尤其是有Bbox Regression), pooling的时候需要取整, 这个时候引入了不可逆的misalignment, 损失了精确的位置信息. 对要求精确到像素的实例分割是不可忍受的 使用ROI Align, 则pooling的时候不取整, 在每个block中取四个锚点(可以用更多锚点吗? 可以根据FPN的尺寸取不同数目的锚点吗?) 备注 代码中RCNN和Mask Head接收的尺寸都是14x14 与FPN合用的时候, ROI Align需要根据proposal的大小来判断从FPN的哪一个feature map上pool ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:2:0","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"RCNN ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:3:0","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"网络结构 layer parameter output fc1 (in_channel*14*14) -\u003e (1024) (1024) fc2 (1024) -\u003e (1024) (1024) cls (1024) -\u003e (num_classes) (num_classes) reg (1024) -\u003e (4*num_classes) (4*num_classes) 输入是RoiAlign提供的$(B, C, 14, 14)$ 先拉平, 通过两个FC+relu和一个dropout层, 输出一个特征向量. 这里不涉及空间属性, 因此不需要使用全卷积结构 两个FC分支分别接收同一个特征向量, cls分支输出属于某一类的score, reg分支输出delta ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:3:1","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"正负样本与损失函数 正样本IOU\u003e0.7 负样本IOU \u003c 0.3 需要将正负样本比例bootstrapping到1:1 损失函数 cls使用交叉熵 reg使用smooth L1 loss ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:3:2","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"Mask Head 4个CBR全卷积层, 其中有一个是步长2的反卷积层, 最后一层是1x1卷积, 用于转换输出维度到80层(对应了80类), 最终输出的mask是(80, 28, 28)的, 拉平之后用交叉熵损失函数(改进: dice loss for 人脸识别) 只取了正样本中iou \u003e 0.7的用于计算损失 与类别无关(只输出一个mask)或输出多类别的网络(使用softmax)相比, 这样做没有类内竞争, 对重叠的mask效果更好. 一个有趣的现象是类别无关的结构效果也几乎一样(nearly as effective), 说明网络将分类和分割解耦得很好 ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:4:0","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"NMS 未完待续 ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:5:0","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"训练细节 每张图都预处理到1024*800, 保持长宽比 120k iter之后学习率降低10(StepLR) 5 scale, 3 aspect的anchor box ","date":"2018-07-13","objectID":"/2018-07-13-mask-rcnn/:6:0","tags":["Mask-RCNN"],"title":"Mask-RCNN","uri":"/2018-07-13-mask-rcnn/"},{"categories":null,"content":"存货, 介绍了语义分割模型的发展历史以及基本理念 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:0:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"FCN Fully Convolutional Networks for Semantic Segmentation (2014) 全卷积架构, 不带全连接层 解决pooling过程中增大感受野的同时丢失位置信息的问题, 这对segment影响很大 Encoder-Decoder结构, 用可学习的transpose-conv来上采样, 同时还伴随前后跳跃连接(U-NET采用concat, FPN采用加法) 空洞卷积, 见下节 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:1:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"SegNet SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation (2015/11/2) 缓存了max pooling的位置(memorized max-pooling), 这样先upsampling之后获得稀疏特征图, 再卷积获得dense的特征图 最后一层输出$class_number$层, 再使用带权重的pixel level的softmax, 即class balancing, 按照类的中位数的频率来给权重(median frequency balancing), 稀少的类权重更高 The final decoder output is fed to a multi-class soft-max classifier to produce class probabilities for each pixel independently. ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:2:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"空洞卷积(dilated convolution) Multi-Scale Context Aggregation by Dilated Convolutions (2015/11/23) 空洞卷积(dilated convolution)替代pooling: 既能够增大感受野, 也避免了down-sampling保留了位置信息 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:3:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"DeepLab v1/v2 (2014v1, 2016v2) 空洞池化(atrous spatial pyramid pooling, ASPP), 多尺度的空洞卷积并行处理, 这个和inception v1的理念差不多哇 全连接的CRF进行后处理 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:4:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"PSPNet Pyramid Scene Parsing Network (2016/12/4) 还是多尺寸特征融合. deeplab中用不同尺寸的空洞卷积并行处理, 这里就用不同尺寸的pooling, 再上采样到相同尺寸后concat…之后的卷积层就能获得不同尺度的信息. 换汤不换药啊 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:5:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"DeepLab v3 加点BN, 又是篇文章 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:6:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"Hybrid Dilated Convolution, HDC Understanding Convolution for Semantic Segmentation (2017/2) 用更加奇形怪状的空洞卷积兼顾大物体和小物体的检查, 避免gridding effect ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:7:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"Mask RCNN Mask RCNN[ICCV17 Best Paper] 用了Faster-RCNN作为前端 用了encoder-decoder结构做mask head 解耦了classification和segmentation, classification只需要RCNN去做就行了, 而FCN等模型是需要同时解决两个问题的. 带来提升的原因是做segmentation的时候解决了class competition 详见blog中关于mask-rcnn的文章… ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:8:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"Learning to Segment Every Thing Learning to Segment Every Thing[CVPR18] 解决了一下有box却没有对应mask的情况下如何处理分类的问题: 学习一个模型, 把检测网络的权重转换成mask网络的权重. 因为detection和segmentation使用的都是同一份特征, 很自然的, 这篇文章说明两者是可以互相迁移的. 下文的PANet也考虑到了这一点, 不过是用额外的一条high-level的向量来指导mask的预测 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:9:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"PANet (SOTA) Path Aggregation Network for Instance Segmentation[CVPR 18] 在FPN的基础上又加了一个Bottom-Up的金字塔. 用conv3x3进行S=2的downsample之后与平行的P层相加, 再过一个3x3conv(这个FPN里没有, 倒是平行的C层需要先过conv1x1统一channel数). 作者认为: high response to edges or instance parts is a strong indicator to accurately localize instances 其中Detection分支是拉平成fc之后经过两个fc层 Mask分支是将四个特征图concat到一起. 然后利用conv3的输出辅助一个fc向量, 希望使用high level的特征指导mask. 因为fc层的好处就是可以学到交叉特征. ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:10:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"一些个人小结 可以发现, 在segmentation中大多数时候我们在和pooling作斗争. 那么能不能不用pooling? pooling能不能用conv代替中有讨论过可行性 Segmentation里面有很多deconv, deconv和upsample的区别中提到了其实deconv可以做成bilnear的upsample. 另外, 如何理解深度学习中的deconv中的高赞答案从计算角度给出了deconv的数学解释, 顺便还可以复习一下conv的实现 如果segmentation能一步到位, 那就不需要detection了! 有没有类似的工作? 一文介绍3篇无需Proposal的实例分割论文带来很多干货. 我认为这是未来趋势 下面介绍一些非主流方法 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:11:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"GAN Semantic Segmentation using Adversarial Networks(2016/11) FAIR的大作. 其中Soumith Chintala哥哥是pytorch的主力, 他的名字我老是读成吃蛋挞 大体思路是用一个segmentation网络生成mask, 然后再用判别网络区分真实mask和生成mask. 损失函数是生成网络的逐像素softmax损失和判别网络的二分类logloss Semi Supervised Semantic Segmentation Using Generative Adversarial Network(ICCV17) 将分割网络(FCN)变成了判别器, 判别的是逐像素是不是instance. 假设分割类别数为K，那么判别器则有K+1个类别的输出。多出来的分类类别为”该像素为假像素”。训练时，使用标记的分割图像训练前K个通道，使用（真实图片，生成图片）图片组按照adversarial loss的定义训练”该像素为假像素”的通道。真是图片既有分割数据库中的图片，也有大量未标注的图片。或者也可以理解为判断“真/假”的分类器，其“真”的这一类扩展成了K类具体的语义类别。 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:12:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"Pixel Embedding Semantic Instance Segmentation with a Discriminative Loss Function(CVPR2017) 假设全卷积网络最终输出为$(C, H, W)$传统方法将输出的每个像素位置变成一个$C$维向量, 维度等于类别数. 最终使用softmax损失会将每个向量embedding到一个one-hot向量上去 这里使用的Pixel Embedding是指将输出的向量在embedding空间中计算损失函数(如上图) 这么做并不需要 ","date":"2018-07-13","objectID":"/2018-07-13-semantic-segmentation/:13:0","tags":["DeepLab","UNet","FCN"],"title":"语义分割综述","uri":"/2018-07-13-semantic-segmentation/"},{"categories":null,"content":"存货, 介绍了RCNN系列的发展历史以及基本理念 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:0:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"RCNN ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:1:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"预测 Rich feature hierarchies for accurate object detection and semantic segmentation(2013) Selective Search 生成大约2000个 Region Proposals, 这是category-independent的, 也就是只有object/background之分 特征提取: 将Region Proposals切割的图片缩放(crop/warp) 到固定尺寸(227*227) 通过(Alexnet) 生成4096维特征向量 对每一类: 通过$k$个SVM(二分类), 输出属于各个类的score 给出score和proposal之后, 对每一类用non-maximum suppresion over a learned threshold来去重 通过训练好的LR模型Bounding Box Regression对proposal进行精修 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:1:1","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"训练 载入ILSVRC2012上预训练的Alexnet 训练(Fine-Tune)CNN: 数据: region proposals切割好的图片 模型: 将网络最后一层改为$R^{N+1}$层, 代表了N+1(背景)类 IoU \u003e 0.5是正样本, 否则是负样本(第N+1类) 优化: SGD, batch大小为128, 每个batch中正负样本比为1/3, 学习率为预训练的1/10 训练 SVM: 正样本: Ground Truth 通过CNN的特征向量 负样本: IoU \u003c 0.3 的proposal通过CNN的特征向量 训练LR进行Bounding Box Regression 输入: CNN输出的4096维向量 正样本: 计算出bbox delta ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:1:2","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"备注 Q: 为何不用Fine-tune的时候的CNN直接检测物体类别? 最后一层不就是softmax分类器吗? A: 因为准确率低, 为了获得充足的样本训练CNN, IoU阈值(\u003e0.5)取得比较低, 检测出来的物体往往不包含整个样本. 将提取出来的特征给SVM并用更严格的策略(Ground Truth为正样本, \u003c0.3为负样本, 其他丢弃, 这是验证集测试mAP出来的结果)来判断包括物体的bbox, 效果更好. 因为SVM适合小样本训练. Q: 什么是Bounding Box Regression? A: 有四个模型, 两个是回归坐标的, 两个是回归长宽的. 输入是CNN提取Regional Proposals的特征向量, 输出是四种model要回归的目标中的一个: $$ \\begin{aligned} \\hat G_x \u0026= P_w d_x(P) + P_x \\\\ \\hat G_y \u0026= P_h d_y(P) + P_y \\\\ \\hat G_w \u0026= P_w \\exp(d_w(P)) \\\\ \\hat G_h \u0026= P_h \\exp(d_h(P)) \\\\ \\end{aligned} $$ Q: 什么是Hard negative mining? A: rcnn中的Hard negative mining方法是如何实现的 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:1:3","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"SPPNet Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(2014/6) SPPNet主要做了两件事: 从特征图上裁剪ROI 在RCNN里, 通过Selective Search生成2000个Region Proposals, 然后crop/warp, 再喂进CNN, 太慢 在SPPNet里, 将整张图片喂进CNN, 然后从特征图上直接裁剪出ROI. 这样只需要一次前向传播, 比之前要2000次快得多 提出Spatial Pyramid Pooling Layer(SPP Layer)解决了把不同尺度的feature map转换为一个尺度的方案, 不再需要crop/warp了: 将feature map按不同维度划分并max pooling 拼接各个维度的输出, 产生固定维度的输出 训练上, 由于可以输入任何尺寸的图片, 所以是分几个阶段用不同分辨率的图训练的. 其他细节看这篇专栏吧…. ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:2:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"Fast RCNN Fast R-CNN(2015/4) ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:3:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"预测 Selective Search 生成大约2000个 Region Proposals, 和RCNN一样 特征提取: 将图片输入CNN, 得到特征图 用Region Proposals直接从特征图上得到切割过的特征 使用ROI pooling layer得到特征向量, 所以从Fast RCNN开始, 训练和预测对图片的尺寸没有要求了 特征向量 -\u003e FC1 -\u003e SoftMax Probability ($K+1$) 特征向量 -\u003e FC2 -\u003e bbox offsets ($4 \\times K$) 对每一类用nms得到最终结果 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:3:1","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"训练 预训练CNN (Imagenet): 将最后一层pooling layer换成ROI pooling layer 将最后一个全连接层换成两个并行的全连接层FC1, FC2 网络需要同时输入图片和ROI 同时训练bbox regressor和softmax分类器 bbox regressor用Smooth L1损失函数 Softmax for probability 将两个损失相加之后回传 一个Mini-Batch中重采样正负样本: 一个batch包含两张图 每张图贡献64个ROI, 一共128个 正样本: IoU\u003e0.5 负样本: IoU $\\in$ [0.1, 0.5) (hard negative mining) ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:3:2","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"Notes ROI pooling: 其实就是单层SPP layer, 详细机制参考region-of-interest-pooling-explained 原始图片中的ROI如何映射到到feature map?这坑挺深的 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:3:3","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"Faster RCNN Faster R-CNN: Towards Real-Time Object Detection(2015/6) 核心是把Selective Search交给了RPN做, 而RPN和fast rcnn是可以合并的 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:4:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"RPN https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work 输入: CNN产生的固定尺寸的特征图 第一层: $n \\times n$ 卷积层, 输出256维特征向量. ($3 \\times 3$ in the paper) 两个并行的分支: $cls$ 分支: $(2k, H, W)$ 每个像素位置产生$2k$个anchor boxes, 其中k为不同比例/尺寸的anchor数目 $reg$分支: $(4k, H, W)$每个像素位置产生$4k$个delta, bbox regression RPN’s loss function: $$ L(p_i, t_i) = \\frac{1}{N_{cls}} \\sum\\limits_{i} L_{cls}(p_i, p^{\\star}_{i}) + \\lambda \\frac{1}{N_{reg}} \\sum\\limits_{i} p_{i}^{\\star}L_{reg}(t_i, t^{\\star}_{i}) $$ $i$: index of anchor in a mini-batch $p_i$: predicted probability of foreground $p^*_i$: grund truth. 1 for postive, 0 for negative $t_i$: bbox regression offset $L_{cls}$: log loss $L_{reg}$: Smooth L1 loss ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:4:1","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"Notes 后续RPN产生的ROIs扔进fast rcnn即可, RPN和RCNN共享backbone, 训练过程和细节不表, 见Faster R-CNN 有关RPN的细节见faster rcnn中rpn的anchor，sliding windows，proposals ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:4:2","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"Feature Pyramid Network Feature Pyramid Networks for Object Detection(2016/11) 核心是Lateral Block. 解决了位置精度和语义信息的冲突问题: 越靠近输入的特征图语义信息越少, 但是位置信息越精确(特征图大) 越远离输入的特征图语义信息越强, 但是位置信息越弱(各种卷积, pooling) 左边是resnet的四层stage输出的特征图, 简称C2, C3, C4, C5 右边是P层, 每个P层通过上一层P层的upscale和相邻C层的smooth(统一通道数)之后相加得到. 每层通道数都是256, 其中最高层P4直接由C5 smooth而来. class LateralBlock(nn.Module): \"\"\" Feature Pyramid LateralBlock \"\"\" def __init__(self, c_planes): \"\"\" c_planes: channels of the C layer to smooth \"\"\" super(LateralBlock, self).__init__() self.lateral = nn.Conv2d(c_planes, 256, 1) self.smooth = nn.Conv2d(256, 256, 3, padding=1) def forward(self, c, p): \"\"\" :param c: c layer before conv 1x1 :param p: p layer before upsample :return: conv3x3( conv1x1(c) + upsample(p) ) \"\"\" _, _, H, W = c.size() c = self.lateral(c) p = F.upsample(p, scale_factor=2, mode='nearest') # p = F.upsample(p, size=(H, W), mode='bilinear') p = p[:, :, :H, :W] + c p = self.smooth(p) return p ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:5:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"RetinaNet Focal Loss for Dense Object Detection: 对正负样本不平衡的深入挖掘 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:6:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"YOLO 人家讲的很好了, 懒得复制粘贴了 简单说几点 直接在7x7的特征图上输出每个格子的检测/分类结果, 每个格子输出两个proposal$(C_x, C_y, w, h, score, \\vec c)$, 这个score相当于RPN的score, $\\vec c$是一个one-hot的类概率向量. 这个输出是先拉平之后经过fc层再还原成7x7的! 如此简单粗暴, 惊不惊喜, 意不意外? PANet有更优雅的做法, 分出一个branch来指导. 但总之有fc层会对速度影响挺大的. 这么多loss混在一起, 需要给予不同的权重防止某一种dominate. 有/没有object的权重是不同的(这个有没有需要用iou去判断) 小box的损失应该比大box大, 设计损失函数的时候直接用长宽的根号差来处理… 嗯, 这种把所有损失一锅端的方法非常粗暴 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:7:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"SSD 这玩意就是个class-specified RPN. SSD取了backbone不同阶段的特征图, 大概当时FPN还没横空出世吧? 比较一下这两兄弟 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:8:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"Mask RCNN 详见blog中关于mask-rcnn的文章…懒得再复制粘贴了 ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:9:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"DetNet(SOTA) DetNet: A Backbone network for Object Detection[ECCV18] 主要是对ResNet50的魔改 加入了空洞卷积 唔 为了防止越卷越小加入了一些1x1卷积(似乎? 没有看到特别有趣的insight… ","date":"2018-07-13","objectID":"/2018-07-13-object-detection/:10:0","tags":["RCNN","FPN"],"title":"目标检测综述-从RCNN到Mask-RCNN","uri":"/2018-07-13-object-detection/"},{"categories":null,"content":"存货, 介绍了从Inception系列到Resnet系列的发展历史以及基本理念和pytorch实现 ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:0:0","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"Inception系列 ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:1:0","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"1x1 卷积核的出处 出自颜水成组的Network in Network(ICLR 2014)中. 传统CNN使用线性滤波器, 为了捕捉高度非线性关系, 需要堆积卷积核, 计算量太大. 本文的贡献主要是 提出MLP conv层, 即用多层感知机建模卷积层之间的非线性关系, 可以 实现跨通道的交互和信息整合. 进行卷积核通道数的降维和升维，减少网络参数 具体到实现上, 可以用1x1 卷积核来近似实现. 普通卷积层+多个1＊1卷积（followed by 激活层）等价于类patch级别的MLP 提出用全局均值池化提到全连接层: 比如输出$k$类, 最后一层conv layer就有$k$个channel, 最后经过全局均值池化输出一个$k$维向量 原作把MLP conv和maxout进行比较, 并表示 maxout network imposes the prior that instances of a latent concept lie within a convex set in the input space, which does not necessarily hold ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:1:1","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"Inception v1 出自Google Brain. C Szegedy, CVPR 2015 整个网络由9个上图形成的block组成 第三个和第五个block有辅助分类器处理梯度消失问题 图中的1x1 conv降维不仅可以降低计算量, 还可以使得数据更加dense: Clustering sparse matrices into relatively dense submatrices tends to give state of the art practical performance for sparse matrix multiplication ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:1:2","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"Inception v2/v3 出自Google Brain. C Szegedy, CVPR 2016. 论文首先提出了一些在构建深度网络时值得借鉴的意见. 不要过早地引入Bottleneck. 并且Bottleneck之间过多地减少维度可能会造成信息的损失. 当卷积不会大幅度改变输入维度时，神经网络可能会执行地更好 Higher dimensional representations are easier to process locally within a network. 没太理解. 似乎是说特征维度越高, 越容易训练 在卷积之前提前降维信息损失会很低. 这可能是因为相邻层之间有较大的协方差(strong correlation between adjacent unit), 所以可以先降维来降低运算量, 训练速度也会提升, 这就是Bottleneck概念的来源 平衡网络的深度和宽度 提出了分解大卷积核为小卷积核的想法(Factorizing Convolutions with Large Filter Size). 感受野相同的情况下, 将 5×5 的卷积分解为两个 3×3 的卷积, 降低了2.78倍运算量 将 n*n 的卷积核尺寸分解为 1×n 和 n×1 两个卷积 提出Bottleneck概念, 先降维再升维. 这个概念取自于自编码器? 例如: 输入输出皆为256 channel的卷积层, 由3x3x256个卷积核组成 转换成一组1x1x64, 3x3x64, 1x1x256维的卷积层, 计算量降低了接近10倍 最后提出了Label Smoothing来对网络输出正则化, 也是率先使用了batch norm的网络之一 v3是同一篇论文提出的, 增加了RMSProp优化器, Factorized 7x7 卷积, 辅助分类器使用了 BatchNorm ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:1:3","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"Inception v4 使用了更多骨骼惊奇的building block, 并且结合了resnet的跳跃式结构. ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:1:4","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"Xception Xception: Deep Learning with Depthwise Separable Convolutions 某层卷积层输入channel为$16$, 输出channel为$32$ 使用32个$k \\times k \\times 16$的卷积核, 参数数量为: $$32 \\times k \\times k \\times 16$$ 使用$16$个$k \\times k \\times 1$的卷积核分别在$16$个channel上独自进行卷积, 输出为$16$个channel, 再用$32$个$1 \\times 1 \\times 16$的一维卷积核, 输出为$32$个channel, 参数数量为: $$16 \\times k \\times k \\times 1 + 32 \\times 1 \\times 1 \\times 16$$ 由此可见参数量被大大减小. 在实现上, Depthwise Conv等价于一个$k \\times k \\times 16$的卷积核在尚未sum各个channel的值之前就输出成16个channel, 然后再接$32$个Pointwise Conv, 即$1 \\times 1$卷积核 注: Depthwise Conv在pytorch中可以用groups参数实现: # A Depthwise Conv layer nn.Conv2d(128, 128, kernel_size=3, groups=128) ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:1:5","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"ResNet resnet由四个stage组成, 而每个stage又由数个bottleneck组成. bottleneck结构借鉴了Inception v2 一个Bottleneck由3层CBR(conv-batch norm-relu)层组成, 见下: 同时一个stage由若干重复bottleneck组成, 整个网络由4个stage组成 每个stage的开头第一个bottleneck的3x3卷积层的步长是2, 使得每个stage将feature map尺寸缩小一半 每个stage的开头第一个bottleneck需要处理上一个stage的输出通过shortcut与该bottleneck的输出相加时通道不匹配的问题, 故需要增加一层Conv-BN处理通道一致性 构造Bottleneck: class Bottleneck(nn.Module): \"\"\" ResNet 50/101 BottleNeck \"\"\" def __init__(self, in_planes, k_planes, stride=1): \"\"\" :param in_planes: input channels :param k_planes: conv kernel output channels in each stage :param stride: the 3x3 kernel in a stage may set to 2 only in each group of stages' 1st stage \"\"\" super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(in_planes, k_planes, 1, bias=False) self.bn1 = nn.BatchNorm2d(k_planes) self.conv2 = nn.Conv2d(k_planes, k_planes, 3, padding=1, stride=stride, bias=False) self.bn2 = nn.BatchNorm2d(k_planes) self.conv3 = nn.Conv2d(k_planes, k_planes * 4, 1, bias=False) self.bn3 = nn.BatchNorm2d(k_planes * 4) self.relu = nn.ReLU(inplace=True) self.shortcut = nn.Sequential() # empty Sequential module returns the original input if stride != 1 or in_planes != k_planes * 4: # tackle input/output size/channel mismatch during shortcut add self.shortcut = nn.Sequential( nn.Conv2d(in_planes, k_planes * 4, 1, stride=stride, bias=False), nn.BatchNorm2d(k_planes * 4) ) def forward(self, x): out = self.relu(self.bn1(self.conv1(x))) out = self.relu(self.bn2(self.conv2(out))) out = self.bn3(self.conv3(out)) out += self.shortcut(x) out = self.relu(out) return out 构造第一层conv1 def _make_conv1(): return nn.Sequential( nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False), # (224-7+2*3) // 2 +1 = 112 nn.BatchNorm2d(64), nn.ReLU(inplace=True), # nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # shrink to 1/4 of original size ) 构造四个stage def _make_stage(ch_in, ch, num_blocks, stride=1): layers = [] layers.append(Bottleneck(ch_in, ch, stride)) # only the first stage in the module need stride=2 for i in range(1, num_blocks): layers.append(Bottleneck(ch * 4, ch)) return nn.Sequential(*layers) ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:2:0","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"几个小问题 1大量使用BatchNorm. BN应该在激活函数之前还是之后呢? - https://zhuanlan.zhihu.com/p/28124810 - https://www.zhihu.com/question/64494691 - https://zhuanlan.zhihu.com/p/28749411 2. 为什么resnet? - https://www.zhihu.com/question/64494691 ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:2:1","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"ResNeXt 核心思路是在Bottleneck中: 利用1x1卷积核, 先降维再升维的方式降低计算开销(下图c) 在降维过程中, 利用Xception的Depthwise Conv进一步降低计算开销(下图b) 将输入的128通道分成32组, 每组4个通道 对每组使用4个核为3x3的卷积层, 输出4个通道 将32个卷积层的4x32=128个通道concat为128个输出层 与ResNet的比较: 构造Bottleneck, 和resnet相比改动很简单, 参考上图最右边的实现, 在每个stage的第二个卷积层启用group参数即可. ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:3:0","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"SENet 大体思路是对各个channel(或者说整张图片)的一个attention, 个人理解就是结合了文章开头NIN的理念和Attention机制 通过全局均值池化将$C$个channel变成$C$维向量. 第一个FC层称为squeeze层, 将维度压缩为$reduction$, 使用ReLU, 起到LSTM中门控的作用 第二个FC层称为excitation层, 将维度还原为$C$, 使用Sigmoid, 起到输出权重作用, 重新赋予每个channel权重. 核心思想是建模通道间的相关性 最后输出的向量作为各层的权重乘到输出上 class SEScale(nn.Module): def __init__(self, channel, reduction=16): super(SEScale, self).__init__() self.fc1 = nn.Conv2d(channel, reduction, kernel_size=1, padding=0) self.fc2 = nn.Conv2d(reduction, channel, kernel_size=1, padding=0) def forward(self, x): x = F.adaptive_avg_pool2d(x,1) x = self.fc1(x) x = F.relu(x, inplace=True) x = self.fc2(x) x = F.sigmoid(x) return x 带SEScale的ResNext: class SENextBottleneckBlock(nn.Module): def __init__(self, in_planes, planes, out_planes, groups, reduction=16, is_downsample=False, stride=1): super(SENextBottleneckBlock, self).__init__() self.is_downsample = is_downsample self.conv_bn1 = ConvBn2d(in_planes, planes, kernel_size=1, padding=0, stride=1) self.conv_bn2 = ConvBn2d( planes, planes, kernel_size=3, padding=1, stride=stride, groups=groups) self.conv_bn3 = ConvBn2d( planes, out_planes, kernel_size=1, padding=0, stride=1) self.scale = SEScale(out_planes, reduction) if is_downsample: self.downsample = ConvBn2d(in_planes, out_planes, kernel_size=1, padding=0, stride=stride) def forward(self, x): z = F.relu(self.conv_bn1(x),inplace=True) z = F.relu(self.conv_bn2(z),inplace=True) z = self.conv_bn3(z) if self.is_downsample: z = self.scale(z)*z + self.downsample(x) else: z = self.scale(z)*z + x z = F.relu(z,inplace=True) return z ","date":"2018-07-13","objectID":"/2018-07-13-image-classification/:4:0","tags":["ResNet","Inception","SE-Net"],"title":"图像识别综述-从Inception到ResNet","uri":"/2018-07-13-image-classification/"},{"categories":null,"content":"可能是有史以来公式最多的文章. 敲到吐血 你的PGM也会和Steins Gate的世界线一样收束吗? ","date":"2018-02-03","objectID":"/2018-02-03-crf/:0:0","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"序 设输入是$x$, 输出是$y$, HMM刻画的是联合概率分布$P(x, y | \\lambda)$, 因此是生成模型, 可以同时进行推断$P(y|x, \\lambda)$和解码$P(x|y, \\lambda)$. 而CRF刻画的是条件概率分布$P(y|x, \\lambda)$, 是判别模型 ","date":"2018-02-03","objectID":"/2018-02-03-crf/:1:0","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"Log-Linear Models ","date":"2018-02-03","objectID":"/2018-02-03-crf/:2:0","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"符号表 符号 含义 $x \\in \\mathcal X$ input/feature $y \\in \\mathcal Y$ output/label $\\vec{\\phi}(x, y): \\mathcal X \\times \\mathcal Y \\rightarrow R^d$ 特征函数, 将$(x, y)$映射为$R^d$维特征向量 $\\vec{w} \\in R^d$ weights for $\\vec \\phi$ ","date":"2018-02-03","objectID":"/2018-02-03-crf/:2:1","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"定义 Log Linear Models take the following form: $$ \\begin{align} P(y|x; \\vec w) \u0026= \\frac{1}{Z} \\exp(\\vec w \\cdot \\vec \\phi(x, y)) \\\\ Z \u0026= \\sum\\limits_{y_i \\in \\mathcal Y} \\exp(\\vec w \\cdot \\vec \\phi(x, y_i)) \\end{align} \\tag{1.1} $$ 其中$Z$是归一化常数, 使得输出构成概率分布. 这是条件概率, 因此是判别模型. $\\vec w \\times \\vec \\phi$代表了给定$x$之后$y$的’可能性’, 而模型的核心就是特征函数$\\vec \\phi$. $\\vec \\phi$实际上是不同的特征函数, 每个特征函数匹配不同pattern并score, 赋予模型分辨能力. 例如在logistic regression中: $$\\vec \\phi_k(\\vec x, y) = \\begin{cases} x_k, \u0026 \\text{y=1} \\\\ 0, \u0026 \\text{y=0} \\end{cases} $$ 特征函数赋予模型二分类能力(不论$\\vec x$如何, 只要出现了不同的$y$, 模型就要区分出来) $\\vec w$让模型对$\\vec \\phi$ 的每个特征给予不同的权重, 赋予模型判断特征权重的能力 使用指数控制其非负 使用$Z$归一化之后就转化成了概率分布. ","date":"2018-02-03","objectID":"/2018-02-03-crf/:2:2","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"预测 给定一组容量为$n$的样本$S = \\lbrace (x_i, y_i) \\rbrace^{n}_{i=1}$, 使用Log Linear Model计算其对数似然 $$ \\begin{align} \\log P(S | \\vec w) = \\log L(\\vec w) \u0026= \\log \\prod\\limits_{i=1}^n P(y_i|x_i; \\vec w) \\\\ \u0026= \\sum\\limits_{i=1}^n \\log P(y_i|x_i; \\vec w) \\end{align} \\tag{1.2} $$ ","date":"2018-02-03","objectID":"/2018-02-03-crf/:2:3","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"学习 使用最大似然估计参数$\\vec w$ $$ \\vec w^{*} = \\arg \\max\\limits_{\\vec w \\in R^d} \\sum\\limits_{i=1}^n \\log P(y_i|x_i; \\vec w) $$ 常用的优化手段有梯度下降(负对数似然), 其中梯度计算方式如下: $$ \\begin{align} \\frac{\\partial \\log L(\\vec w)}{\\partial \\vec w} \u0026= \\frac{\\partial}{\\partial \\vec w} \\sum\\limits_{i=1}^n \\log P(y_i|x_i; \\vec w) \\\\ \u0026= \\frac{\\partial}{\\partial \\vec w} \\sum\\limits_{i=1}^n \\log \\frac{\\exp(\\vec w \\cdot \\vec \\phi(x_i, y_i))}{\\sum\\limits_{y_j \\in \\mathcal Y} \\exp(\\vec w \\cdot \\vec \\phi(x, y_j))} \\\\ \u0026= \\sum\\limits_{i=1}^n \\vec \\phi(x_i, y_i) - \\sum\\limits_{i=1}^n \\sum\\limits_{y_j \\in \\mathcal Y} P(y_j|x_i; \\vec w) \\vec \\phi(x_i, y_j) \\end{align} \\tag{1.3} $$ 正则化是优化模型性能的重要方式, L2正则项为$\\frac{1}{2} ||\\vec w||^2$, 损失函数变为$\\log L(\\vec w) + \\frac{1}{2} ||\\vec w||^2$即可 ","date":"2018-02-03","objectID":"/2018-02-03-crf/:2:4","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"案例: Logistic Regression 定义: $$ \\begin{align} \\vec x \u0026\\in R^n \\\\ y \u0026\\in \\lbrace 0, 1 \\rbrace \\\\ \\vec \\phi_k(\\vec x, y) \u0026= \\begin{cases} x_k, \u0026 \\text{y=1} \\\\ 0, \u0026 \\text{y=0} \\end{cases} \\end{align} \\tag{e.0} $$ 由定义$(1.1)$求出条件概率 $$ P(y|\\vec x; \\vec w) = \\frac{\\exp(\\vec w \\cdot \\vec \\phi(\\vec x, y))}{\\exp(\\vec w \\cdot \\vec \\phi(\\vec x, 1)) + 1} \\tag{e.1} $$ 由$(1.2)$可知对数似然 $$ \\begin{align} \\log L(\\vec w) \u0026= \\sum\\limits_{i=1}^n \\log p(y_i|x_i; \\vec w) \\\\ \u0026= \\sum\\limits_{i=1}^n \\log \\frac{\\exp(\\vec w \\cdot \\vec \\phi(x_i, y_i))}{\\exp(\\vec w \\cdot \\vec \\phi(x_i, 1)) + \\exp(\\vec w \\cdot \\vec \\phi(x_i, 0))} \\\\ \u0026= \\sum\\limits_{i=1}^n \\log \\frac{\\exp(\\vec w \\cdot \\vec \\phi(x_i, y_i))}{\\exp(\\vec w \\cdot \\vec x_i) + 1} \\\\ \u0026= \\sum\\limits_{i=1}^n [\\vec w \\cdot \\vec \\phi(x_i, y_i) - \\log (\\exp(\\vec w \\cdot \\vec x_i) + 1)] \\end{align} \\tag{e.2} $$ 由$(1.3)$求出梯度 $$ \\begin{align} \\frac{\\partial \\log L(\\vec w)}{\\partial \\vec w} \u0026= \\frac{\\partial}{\\partial \\vec w} \\sum\\limits_{i=1}^n [\\vec w \\cdot \\vec \\phi(x_i, y_i) - \\log (\\exp(\\vec w \\cdot \\vec x_i) + 1)] \\\\ \u0026= \\sum\\limits_{i=1}^n [\\vec \\phi(x_i, y_i) - \\frac{\\exp(\\vec w \\cdot \\vec x_i) \\vec x_i}{\\exp(\\vec w \\cdot \\vec x_i) + 1}] \\\\ \u0026= \\sum\\limits_{i=1}^n [\\vec \\phi(x_i, y_i) - P(1|\\vec x_i; \\vec w) \\vec x_i] \\end{align} \\tag{e.3} $$ 事实上, 当$y \\in \\lbrace 0, 1 \\rbrace$时, $(e.0)$中的特征函数可以简化为$\\vec \\phi(\\vec x, y) = y \\vec x$, $(e.2)$, $(e.3)$可以进一步简化, 但当$y$为其他取值时不通用, 因此没有扔进去算 ","date":"2018-02-03","objectID":"/2018-02-03-crf/:2:5","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"MEMM 把log linear models推广到序列上, 就有了MEMM 符号 含义 $x \\in \\mathcal X$ input, for example the $j$’th word in a sentence $\\vec x = (x_1,x_2, \\cdots, x_T)$ input sequence $s^i \\in \\mathcal S, 1 \\leq i \\leq \\lvert \\mathcal S \\rvert$ state or output, like tag of a word $\\vec s = (s_1,s_2, \\cdots, s_T)$ state sequence $\\vec \\phi(x, y): \\mathcal X \\times \\mathcal Y \\rightarrow R^d$ 特征函数, 将$(x, y)$映射为$R^d$维特征向量 ${\\vec w} \\in R^d$ weights for $\\vec \\phi$ ","date":"2018-02-03","objectID":"/2018-02-03-crf/:3:0","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"定义 MEMM的结构中, 状态链是马尔科夫链, 符合局部Markov性, 而transition和emission矩阵由特征函数刻画. 整个模型刻画了条件概率分布: $$ \\begin{align} P((s_1,s_2, \\cdots, s_T)|\\vec x) \u0026= \\prod\\limits_{i=1}^{T} P(s_i | (s_1,s_2, \\cdots, s_{i-1}), \\vec x) \\\\ \u0026= \\prod\\limits_{i=1}^{T} P(s_i | s_{i-1}, \\vec x) \\end{align} \\tag{2.1} $$ 其中 第一步使用了chain rule of conditional probabilities, 可以把联合概率拆分为条件概率: $$ \\begin{align} P(A_n, \\cdots, A_1) \u0026= P(A_n | A_{n-1}, \\cdots, A_1) P(A_{n-1}, \\cdots, A_1) \\\\ P(A_4, A_3, A_2, A_1) \u0026= P(A_4|A_3, A_2, A_1) P(A_3|A_2, A_1) P(A_2|A_1) \\end{align} $$ 第2步使用了independence assumption: $$P(s_i|s_{i-1}, s_{i-2}, \\cdots, s_1) = P(s_i|s_{i-1})$$ 为了计算$(2.1)$, 我们使用Log Linear Model刻画输出与输入/上一个输出之间的条件概率: $$ P(s_i | s_{i-1}, \\vec x; \\vec w) = \\frac{\\exp \\Bigl ( \\vec w \\cdot \\vec \\phi \\bigl ( \\vec x, i, s_{i-1}, s_i \\bigr ) \\Bigr )}{\\sum\\limits_{s’ \\in \\mathcal S} \\exp \\Bigl ( \\vec w \\cdot \\vec \\phi \\bigl ( \\vec x, i, s_{i-1}, s’ \\bigr ) \\Bigr )} \\tag{2.2} $$ 对于$ \\vec \\phi \\bigl ( \\vec x, i, s_{i-1}, s_i \\bigr )$, 有: $\\vec x = (x_1, \\cdots, x_T)$是整个输入序列 $i$是输出序号 $s_{i-1}$是上一次输出 $s_i$是输出 ","date":"2018-02-03","objectID":"/2018-02-03-crf/:3:1","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"学习 获取$(x_1,x_2, \\cdots, x_T)$和$(s_1,s_2, \\cdots, s_T)$之后, 学习可以使用最大似然, 参考$(1.2)$和$(1.3)$ ","date":"2018-02-03","objectID":"/2018-02-03-crf/:3:2","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"预测(Decode) 和Log Linear Models中的预测不同, 前者只需给定输入预测输出的概率分布即可, 但MEMM给定输入之后可以刻画$|\\mathcal S|^T$种输出, 找到条件概率最大的输出序列就成为了一个任务: $$ \\arg \\max\\limits_{(s_1\\cdots s_T)} P\\bigl ((s_1,s_2, \\cdots, s_T)|\\vec x \\bigr ) $$ 这和HMM的Viterbi算法如出一辙. 使用动态规划解决: $$ \\begin{align} \\pi[i, s] \u0026= \\max\\limits_{s_1 \\cdots s_{i-1}} P \\bigl ( (s_1, s_2, \\cdots, s_{i}=s) | \\vec x \\bigr ) \\\\ \u0026= \\max\\limits_{s_1 \\cdots s_{i-1}} P\\bigl ( (s_i=s|s_{i-1}, \\vec x) \\bigr)P \\bigl ( (s_1, s_2, \\cdots, s_{i-1}) |\\vec x \\bigr ) \\\\ \u0026= \\max\\limits_{s_1 \\cdots s_{i-1}} P\\bigl ( (s_i=s|s_{i-1}, \\vec x) \\prod\\limits_{j=1}^{i-1} P\\bigl ( (s_j|s_{j-1}, \\vec x) \\end{align} \\tag{2.3} $$ 递推式为 $$ \\pi[i, s] = \\max\\limits_{s’ \\in \\mathcal S} \\pi[i-1, s’] \\cdot P(s_{i-1}=s’ | s_i=s, \\vec x) \\tag{2.4} $$ ","date":"2018-02-03","objectID":"/2018-02-03-crf/:3:3","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"MEMM与HMM 在计算transition \u0026 emission概率时, HMM和MEMM分别使用如下模型: $$ \\begin{align} P(s_i|s_{i-1})P(x_i|s_i) \u0026= A_{s_{i-1}s_i}B_{s_i}(o_i)\\\\ P(s_i | s_{i-1}, \\vec x) \u0026= \\frac{\\exp \\Bigl ( \\vec w \\cdot \\vec \\phi \\bigl ( \\vec x, i, s_{i-1}, s_i \\bigr ) \\Bigr )}{\\sum\\limits_{s’ \\in \\mathcal S} \\exp \\Bigl ( \\vec w \\cdot \\vec \\phi \\bigl ( \\vec x, i, s_{i-1}, s’ \\bigr ) \\Bigr )} \\end{align} $$ MEMM的关键优势在于使用特征函数$\\vec\\phi$可以捕捉更多信息. HMM中$A \\in R^{|\\mathcal S| \\times |\\mathcal S|}$, $B \\in R^{|\\mathcal S| \\times |\\mathcal O|}$均为二维矩阵, 用来刻画transition和emission MEMM中特征函数$\\vec\\phi \\in R^{|\\mathcal X| \\times T \\times |\\mathcal S| \\times |\\mathcal S|}$, 捕捉特征的能力远强于HMM. For example, the transition probability can be sensitive to any word in the input sequence $x_1 \\cdots x_T$. In addition, it is very easy to introduce features that are sensitive to spelling features (e.g., prefixes or suffixes) of the current word $x_i$, or of the surrounding words. These features are useful in many NLP applications, and are difficult to incorporate within HMMs in a clean way. 并且, HMM刻画的是联合概率分布(joint probability), MEMM刻画的是条件概率分布(conditional probability), 后者往往有更高的precision ","date":"2018-02-03","objectID":"/2018-02-03-crf/:3:4","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"CRF ","date":"2018-02-03","objectID":"/2018-02-03-crf/:4:0","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"定义 条件随机场的任务依然是刻画序列的条件概率$P(\\vec s | \\vec x)$: $$ P(\\vec s | \\vec x; \\vec w) = \\frac{\\exp \\Bigl (\\vec w \\cdot \\vec \\Phi(\\vec x, \\vec s) \\Bigr)}{\\sum\\limits_{\\vec s’ \\in \\mathcal S^T} \\exp \\Bigl (\\vec w \\cdot \\vec \\Phi(\\vec x, \\vec s’) \\Bigr)} \\tag{3.1} $$ 然而这里的特征函数是直接建立在整个输入序列对输出序列的. feature vector maps an entire input sequence $\\vec x$ paired with an entire state sequence $\\vec s$ to some $d$-dimensional feature vector. $$ \\vec \\Phi(\\vec x, \\vec s) = \\vec x \\times \\vec s \\rightarrow R^d $$ 但由于局部Markov性, 特征函数可以通过简单的方式进行分解 $$ \\begin{align} \\vec \\Phi(\\vec x, \\vec s) \u0026= \\sum\\limits_{i=1}^T \\vec \\phi \\bigl ( s_{i-1}, s_i, \\vec x, i \\bigr ) \\end{align} \\tag{3.2} $$ 这里的$\\phi \\bigl ( s_{i-1}, s_i, \\vec x, i \\bigr )$和MEMM中的一致. ","date":"2018-02-03","objectID":"/2018-02-03-crf/:4:1","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"概率计算 由于CRF是序列对序列的, 计算概率的过程比较复杂 非规范化概率的计算 非规范化概率的表示 $$ \\begin{align} \\hat P(\\vec s | \\vec x; \\vec w) \u0026= \\exp \\Bigl (\\vec w \\cdot \\vec \\Phi(\\vec x, \\vec s) \\Bigr) \\\\ \u0026= \\exp \\Bigl ( \\vec w \\cdot \\sum\\limits_{i=1}^T [ \\vec t(s_{i-1}, s_i, \\vec x, i) + \\vec e(s_i, \\vec x, i) ] \\Bigr ) \\\\ \u0026= \\exp \\Bigl ( \\sum\\limits_{k=1}^{|\\vec t|} \\sum\\limits_{i=1}^T \\lambda_k t_k(s_{i-1}, s_i, \\vec x, i) + \\sum\\limits_{k=1}^{|\\vec e|} \\sum\\limits_{i=1}^T \\mu_k e_k(s_i, \\vec x, i) \\Bigr ) \\end{align} \\tag{3.3} $$ 其中$\\vec t, \\vec e$分别是transition和emission特征, transition定义在边上, 依赖于当前和前一个位置, emission定义在节点上, 依赖于当前位置. $\\lambda, \\mu$分别是他们的权重, CRF完全由$\\lambda, t, \\mu, e$定义. 假设转移矩阵和位置及输入无关, 可以这样刻画transition和emission: $$ \\begin{align} t[i][j] \u0026= \\lambda t(s^i, s^j) \u0026 s^i, s^j \\in \\mathcal S \\\\ e[i][j] \u0026= \\mu e(x_i, s^j) \u0026 x_i \\in \\vec x, s^j \\in \\mathcal S \\end{align} $$ 为了处理边界条件, 定义起点为$s_0$, 终点为$s_{-1}$, 即 $$ \\vec s = (s_0, s_1, \\cdots, s_T, s_{-1}) $$ 规范化因子的计算: 前向算法 $(3.1)$分母是全局规范化因子, 涉及计算$\\mathcal S^T$条路径的非规范化概率之和, 这和HMM中计算$P(O|\\lambda)$时需要对所有状态求和是一致的, 都可以使用前向/后向算法解决. 举个例子, 假设$\\mathcal S \\in {a, b}$ $$ \\begin{align} \\alpha_1(a) = \\hat P \\bigl ( (s_0),s_1=a |(x_1) \\bigr ) \u0026= \\exp \\bigl ( t(0, a) + e(a, x_1) \\bigr ) \\\\ \\alpha_1(b) = \\hat P \\bigl ( (s_0),s_1=b |(x_1) \\bigr ) \u0026= \\exp \\bigl ( t(0, b) + e(b, x_1) \\bigr ) \\\\ \\end{align} \\tag{3.4} $$ 而 $$ \\begin{align} \\alpha_2(a) \u0026= \\hat P \\bigl ( (s_0, s_1), s_2=a |(x_1, x_2) \\bigr ) \\\\ \u0026= \\exp \\bigl ( t(0, a) + e(a, x_1) + t(a, a) + e(a, x_2) \\bigr ) \\\\ \u0026+ \\exp \\bigl ( t(0, b) + e(b, x_1) + t(b, a) + e(a, x_2) \\bigr ) \\\\ \u0026= \\alpha_1(a) \\cdot \\exp \\bigl ( t(a, a) + e(a, x_2) \\bigr ) \\\\ \u0026+ \\alpha_1(b) \\cdot \\exp \\bigl ( t(b, a) + e(a, x_2) \\bigr ) \\end{align} \\tag{3.5} $$ 由$(3.4)$和$(3.5)$可以归纳出递推式 $$ \\alpha_{t+1}(s^i) = \\sum\\limits_{j = 1}^{|\\mathcal S|} \\alpha_{t}(s^j) \\cdot \\exp\\bigl (t(s^j, s^i) + e(s^i, x_{t+1}) \\bigr ) \\tag{3.6} $$ 或 $$ \\alpha_{t+1}(s^i) = \\sum\\limits_{j = 1}^{|\\mathcal S|} \\exp \\bigl ( \\log \\alpha_{t}(s^j) + t(s^j, s^i) + e(s^i, x_{t+1}) \\bigr ) \\tag{3.7} $$ 或 $$ \\beta_{t+1}(s^i) = \\log \\sum\\limits_{j = 1}^{|\\mathcal S|} \\exp \\bigl ( \\beta_{t}(s^j) + t(s^j, s^i) + e(s^i, x_{t+1}) \\bigr ) \\tag{3.8} $$ 其中$\\beta_t(s^i) = \\log \\alpha_t(s^i) = \\log \\hat P \\bigl ( s_t=s^i |(x_1 \\cdots x_t) \\bigr )$ ","date":"2018-02-03","objectID":"/2018-02-03-crf/:4:2","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"Decode: Viterbi算法 给定$\\vec x$, 求$\\arg \\max\\limits_{\\vec s \\in \\mathcal S^T} P(\\vec s | \\vec x; \\vec w)$ $$ \\begin{align} \\arg \\max\\limits_{\\vec s \\in \\mathcal S^T} P(\\vec s | \\vec x; \\vec w) \u0026= \\arg \\max\\limits_{\\vec s \\in \\mathcal S^T} \\frac{\\exp \\Bigl (\\vec w \\cdot \\vec \\Phi(\\vec x, \\vec s) \\Bigr)}{\\sum\\limits_{\\vec s’ \\in \\mathcal S^T} \\exp \\Bigl (\\vec w \\cdot \\vec \\Phi(\\vec x, \\vec s’) \\Bigr)} \\\\ \u0026= \\arg \\max\\limits_{\\vec s \\in \\mathcal S^T} \\vec w \\cdot \\vec \\Phi(\\vec x, \\vec s) \\\\ \u0026= \\arg \\max\\limits_{\\vec s \\in \\mathcal S^T} \\vec w \\cdot \\sum\\limits_{i=1}^T \\vec \\phi \\bigl ( \\vec x, i, s_{i-1}, s_i \\bigr ) \\\\ \u0026= \\arg \\max\\limits_{\\vec s \\in \\mathcal S^T} \\sum\\limits_{k=1}^{|\\vec t|} \\sum\\limits_{i=1}^T \\lambda_k t_k(s_{i-1}, s_i, \\vec x, i) + \\sum\\limits_{k=1}^{|\\vec e|} \\sum\\limits_{i=1}^T \\mu_k e_k(s_i, \\vec x, i) \\end{align} $$ 依然是求$t + s$的最大带权路径. 使用Viterbi算法解决: 初始化 $$ \\delta_1(s^i) = t(s_0, s_1=s^i) \\\\ \\phi_1(s_1) = 0 \\tag{3.9.1} $$ 递推. 对$t=[2, T]$: $$ \\delta_t(s^i) = \\max\\limits_{1 \\leq j \\leq |\\mathcal S|} [\\delta_{t-1}(s^j) + t(s^j, s^i)] + e(s^i, x_t) \\\\ \\phi_t(q_i) = arg\\max\\limits_{1 \\leq j \\leq |\\mathcal S|} [\\delta_{t-1}(s^j) + t(s^j, s^i)] \\tag{3.9.2} $$ 终止 $$ P = \\max\\limits_{1 \\leq i \\leq |\\mathcal S|} \\delta_T(s^i) \\\\ I_T = arg\\max\\limits_{1 \\leq i \\leq |\\mathcal S|} \\delta_T(s^i) \\tag{3.9.3} $$ 逆推最优状态序列. 对$t=T-1, T-2, \\cdots, 1$ $$ I_t = \\phi_{t+1}(I_{t+1}) \\tag{3.9.4} $$ 其中 $$ \\delta_t(s^i)=\\max\\limits_{(s_1 \\cdots s_{t-1})} P \\left ((s_1 \\cdots s_{t-1}), s_t=s^i | \\vec x; \\vec w \\right ) \\tag{3.9.5} $$ ","date":"2018-02-03","objectID":"/2018-02-03-crf/:4:3","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"Appendix: CRF的计算 使用矩(跃)阵(迁)运(引)算(擎)加速, 并充分利用broadcast简化代码, 降低可读性( ","date":"2018-02-03","objectID":"/2018-02-03-crf/:5:0","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"概率计算 注意$\\vec s = (s_0, s_1, \\cdots, s_T, s_{-1})$, $|\\mathcal S|$包括起点和终点 数据结构 $$ \\begin{align} \\vec \\beta_t(s) \u0026= \\begin{bmatrix} \\beta_t(s^1) \\\\ \\vdots \\\\ \\beta_t(s^{|\\mathcal S|}) \\end{bmatrix} \\in R^{|\\mathcal S|}\\\\ T \u0026= \\begin{bmatrix} t(s^1, s^1) \u0026 \\cdots \u0026 e(s^1, s^{|\\mathcal S|}) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ t(s^{|\\mathcal S|}, s^1) \u0026 \\cdots \u0026 e(s^{|\\mathcal S|}, s^{|\\mathcal S|}) \\end{bmatrix} \\in R^{|\\mathcal S| \\times |\\mathcal S|} \\\\ E \u0026= \\begin{bmatrix} e(s^1, x_{1}) \u0026 \\cdots \u0026 e(s^1, x_{T}) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ e(s^{|\\mathcal S|}, x_{1}) \u0026 \\cdots \u0026 e(s^{|\\mathcal S|}, x_{T}) \\end{bmatrix} \\in R^{|\\mathcal S| \\times T} \\end{align} \\tag{a.1} $$ 那么根据$(3.8)$ $$ \\begin{align} \\beta_{t+1}(s^i) \u0026= \\log \\sum\\limits_{j = 1}^{|\\mathcal S|} \\exp \\left ( \\begin{bmatrix} \\beta_t(s^1) \\\\ \\vdots \\\\ \\beta_t(s^{|\\mathcal S|}) \\end{bmatrix} + \\begin{bmatrix} t(s^1, s^i) \\\\ \\vdots \\\\ t(s^{|\\mathcal S|}, s^i) \\end{bmatrix} + \\begin{bmatrix} e(s^i, x_{t+1}) \\\\ \\vdots \\\\ e(s^i, x_{t+1}) \\end{bmatrix} \\right ) \\\\ \u0026= \\log \\sum\\limits_{j = 1}^{|\\mathcal S|} \\exp \\left (\\vec \\beta_t(s) + T[:, i] + E[i, t+1]\\right ) \\\\ \\end{align} \\tag{e.1} $$ 再根据广播(broadcasting)运算, 不难得到 $$ \\begin{align} \\vec \\beta_{t+1}^T(s) \u0026= \\log \\sum\\limits_{j = 1}^{|\\mathcal S|} \\exp \\left (\\vec \\beta_t(s) + T + E^T[:, t+1] \\right ) \\end{align} \\tag{e.2} $$ 在$(e.1)$中, $E[i, t+1]$被广播到$\\vec \\beta_t(s) + T[:, i]$的每一行 在$(e.2)$中, $\\vec \\beta_t(s)$被广播到$T$的每一列, $E^T[:, t+1]$被广播到T的每一行, 最后逐列求log_sum_exp def neg_log_likelihood(self, x, s): \"\"\" 计算非规范化概率, 然后调用_forward_algo计算规范化因子, 返回负对数概率 :param x: input, words encoded in a sentence :param s: output, tag sequence to calculate probability :return: negative log probability of given tag sequence \"\"\" score = autograd.Variable(torch.FloatTensor([0])) # 添加起点和终点 s = torch.cat([torch.LongTensor([0]), s, torch.LongTensor([self.num_label-1])]) # emission直接用lstm求出来 emits = self._lstm_forward(x) for i in range(len(emits)): score = score + self.trans[s[i+1], s[i]] + emits[i][s[i+1]] score = score + self.trans[-1, s[-2]] # 计算规范化因子 forward_score = self._forward_algo(emits) return forward_score - score def _forward_algo(self, emits): \"\"\" 前向算法, 计算规范化因子, 涉及计算$\\mathcal S^T$条路径的非规范化概率之和 :param emits: emit scores for each word Tensor(n, m), n = words in the sentence; m = num_label :return: Tensor(1, 1), Z(s) used to normalize probability \"\"\" beta_init = torch.FloatTensor(1, self.num_label).fill_(-10000.) beta_init[0][0] = 0. # init start tag beta_t = autograd.Variable(init_alphas) # track grad # 利用e.2式 for emit_t in emits: sum_score = beta_t.view(1, -1) + self.trans + emit_t.view(-1, 1) beta_t = log_sum_exp(sum_score).view(1, - 1) terminal_var = beta_t + self.trans[-1].view(1, -1) Z = log_sum_exp(terminal_var) return Z ","date":"2018-02-03","objectID":"/2018-02-03-crf/:5:1","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"Decode 假设 $$ \\vec \\delta_t = \\begin{bmatrix} \\delta_t(s^1) \\\\ \\vdots \\\\ \\delta_t(s^{|\\mathcal S|}) \\end{bmatrix} \\tag{e.3} $$ 参考$(a.1)$ For $t \\in [0, T-1]$ $$\\vec \\delta_{t+1} = \\max(\\vec \\delta_t + T)^T + E[:, t+1] \\tag{e.4}$$ $t=T$, 这一步没有$E$ $$\\vec \\delta_{t+1} = \\max(\\vec \\delta_t + T)^T \\tag{e.5}$$ 其中$\\max$函数取函数每一列的最大值. def _viterbi_decode(self, emits): \"\"\" Decode the tag sequence with highest un-normalized log-probability(max_score) :param emits: emit scores for each word :return: max_score: un-normalized log-probability(max_score) tag_seq: tag sequence with highest max_score \"\"\" backpointers = [] # Initialize the viterbi variables in log space init_vvars = torch.FloatTensor(1, self.num_label).fill_(-10000.) init_vvars[0][0] = 0. # forward_var at step i holds the viterbi variables for step i-1 # actually no need to calculate grad for decoding forward_var = autograd.Variable(init_vvars) for emit_t in emits: # bptrs_t holds the backpointers for this step # viterbivars_t holds the viterbi variables for this step next_step_score = self.trans + forward_var viterbivars_t, bptrs_t = torch.max(next_step_score, dim=1) forward_var = viterbivars_t.view(1, -1) + emit_t.view(1, -1) backpointers.append(bptrs_t.data.numpy()) # Transition to end tag. We won't add terminal_var into packpointers terminal_var = forward_var + self.trans[-1].view(1, -1) max_score, best_tag = torch.max(terminal_var, dim=1) best_tag = best_tag.data[0] # Follow the back pointers to decode the best path. tag_seq = [best_tag] for bptrs_t in reversed(backpointers): best_tag = bptrs_t[best_tag] tag_seq.append(best_tag) # Pop off the start tag (we dont want to return that to the caller) start = tag_seq.pop() assert start == 0 # Sanity check tag_seq.reverse() return max_score, tag_seq ","date":"2018-02-03","objectID":"/2018-02-03-crf/:5:2","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":"Reference \u003c统计学习方法\u003e 李航 http://www.cs.columbia.edu/~mcollins/notes-spring2013.html http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html ","date":"2018-02-03","objectID":"/2018-02-03-crf/:6:0","tags":["MEMM","CRF","PGM","NLP"],"title":"对数线性模型, MEMM与CRF","uri":"/2018-02-03-crf/"},{"categories":null,"content":" 在 Cowboy Bebop 中, Faye 是个赌徒, 她使用HMM ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:0:0","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"0. 定义 如图所示, 线性HMM由状态$V$和观察$Q$的序列$I$和$O$组成, 其中$I$是马尔科夫链(Markov Chain). 状态之间通过转移矩阵$A$(transition)决定状态转移的概率分布, 每一时间步的状态通过观察矩阵$B$(emission)决定观察结果的概率分布.而状态一般是未知的. ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:1:0","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"0.0 符号表 符号 含义 备注 $Q = \\lbrace q_1, q_2, \\cdots, q_{N_Q} \\rbrace$ 状态(state) $I = (I_1, I_2, \\cdots, I_T)$ 状态序列 $I_t \\in Q$ $V = \\lbrace v_1, v_2, \\cdots, v_{N_V} \\rbrace$ 观察(observation) $O = (o_1, o_2, \\cdots, o_T )$ 观察序列 $o_t \\in V$ $A_{ij} \\in R^{N_Q \\times N_Q}$ 状态转移矩阵 $P(I_t = q_j \\lvert I_{t-1} = q_i)$ $B_{q_i}(v_j ) \\in R^{N_Q \\times N_V}$ 观测概率矩阵(emission) $P(o_t = v_j \\lvert I_t = q_i)$ $\\pi \\in R^{N_Q}$ 初始状态分布 $P(I_1 = q_i)$ $\\lambda=(\\pi, A, B)$ 模型参数 HMM的参数由$\\lambda$组成 状态转移矩阵代表了从状态$q_i$转移到$q_j$的概率 观测概率矩阵代表了从状态$q_i$观察到输出$v_j$的概率 ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:1:1","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"0.1 基本假设 一阶齐次Markov假设: 任意时刻的状态$I_t$只依赖于前一时刻的状态$I_{t-1}$ 任意时刻的观察$O_t$只依赖于该时刻的状态$I_t$ ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:1:2","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"0.2 基本问题 HMM作为一个生成模型, 也涉及概率计算, 学习问题和预测问题(也叫decoding) 概率计算: 已知$\\lambda$, 求$P(O|\\lambda)$ 学习问题: 已知$O$, 求$arg\\max\\limits_{\\lambda}P(O|\\lambda)$ 预测问题: 已知$O, \\lambda$, 求$arg\\max\\limits_{I}P(I|O, \\lambda)$ ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:1:3","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"1. 概率计算 已知$\\lambda=(A, B, \\pi)$, 计算观测序列$O$出现的概率$P(O|\\lambda)$. ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:2:0","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"1.1 直接计算 在已知$O$的情况下, 需要知道所有可能的状态序列$I$, 对所有状态序列与观察结果的联合概率分布求和. 状态序列$(I_1, I_2, \\cdots, I_T)$的概率为 $$ P(I) = \\pi_{I_1} \\prod_{t=2}^{T} A_{I_{t-1} I_t} \\tag{1.1.1} $$ 状态序列为$I$, 观测序列为$O$的概率为 $$ \\begin{align} P(O, I) \u0026= P(O | I)P(I) \\\\ \u0026= \\pi_{I_1}B_{I_1}(o_1) \\prod_{t=2}^{T} A_{I_{t-1} I_t}B_{I_t}(o_t) \\end{align} \\tag{1.1.2} $$ 对所有可能的$I$求和, 时间复杂度为$O(N_V^T)$, 每次计算时间复杂度为$O(T)$, 总复杂度为$O(TN_V^T)$ $$ P(O) = \\sum_I P(O, I) $$ ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:2:1","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"1.2 前向算法 对所有的$I$求和, 由于每一步$I$都可能属于$Q$中的任何一个, 相当于在以下$N \\times T$矩阵中寻找从左边到右边的所有路径的概率和. $$ \\begin{align} \u0026\\begin{bmatrix} o_1 \u0026 o_2 \u0026 \\cdots \u0026 o_T\\\\ \\end{bmatrix} \\\\ \u0026\\begin{bmatrix} q_1 \u0026 q_1 \u0026 \\cdots \u0026 q_1 \\\\ q_2 \u0026 q_2 \u0026 \\cdots \u0026 q_2 \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ q_N \u0026 q_N \u0026 \\cdots \u0026 q_N \\\\ \\end{bmatrix} \\end{align} \\tag{1.2.1} $$ (示意图, 侵删) 对所有的路径求和涉及到大量的重复运算. 假设已经计算出了$(I_1, I_2, \\cdots, I_t=q_t)$的所有可能的路径的概率之和, 那么在计算$(I_1, I_2, \\cdots, I_{t+1})$时, 所有经过节点$I_t=q_t$的路径即可复用之前的计算结果. 写成递推式即为: $$ \\alpha_{t+1}(q_i) = \\sum_{j=1}^N \\alpha_t(q_j) A_{ji} \\tag{1.2.2} $$ 输出是已知的, 可以求出给定输出的概率为: $$ \\alpha_{t+1}(q_i) = B_{q_i}(o_{t+1})\\sum_{j=1}^N \\alpha_t(q_j) A_{ji} \\tag{1.2.3} $$ 其中 $$\\alpha_{t}(q_i) = P((o_1, o_2, \\cdots, o_t), I_t=q_i)\\tag{1.2.4}$$ 使用此式递推, 时间复杂度为$O(TN^2)$ ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:2:2","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"1.3 后向算法 类似地定义后向算法. 假设已经计算出了$(I_t=q_i, I_{t+1}, \\cdots, I_T)$的所有可能的路径的概率之和, 那么在计算$(I_{t-1}, I_t, \\cdots, I_T)$时, 所有经过节点$I_t=q_t$的路径即可复用之前的计算结果. $$ \\beta_{t-1}(q_i) = \\sum_{j=1}^N A_{ij} \\beta_{t}(q_j) B_{q_j}(o_{t}) \\tag{1.3.1} $$ 其中 $$\\beta_{t}(q_i) = P((o_{t+1}, o_{t+2}, \\cdots, o_T), I_t=q_i)\\tag{1.3.2}$$ ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:2:3","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"1.4 小结 结合前向和后向算法的表达式, 可以发现 $$ P(O, I_t=q_i) = \\alpha_{t}(q_i) \\beta_{t}(q_i) \\tag{1.4.1} $$ 给定输出$O$, 状态$I_t=q_i$的概率可以利用全概率公式得到: $$ \\begin{align} \\gamma_t(q_i)=P(I_t=q_i|O) \u0026= \\frac{P(O, I_t=q_i)}{P(O)} \\\\ \u0026= \\frac{P(O, I_t=q_i)}{\\sum_j P(O, I_t=q_j)} \\\\ \u0026= \\frac{\\alpha_{t}(I_t=q_i) \\beta_{t}(I_t=q_i)}{\\sum_j \\alpha_{t}(I_t=q_j) \\beta_{t}(I_t=q_j)} \\end{align} \\tag{1.4.2} $$ 类似方法可以求出给定输出$O$, 状态$I_t=q_i$, $I_{t+1}=q_j$的概率: $$ \\begin{align} \\xi_t(q_i, q_j) \u0026= P(I_t=q_i, I_{t+1}=q_j|O) \\\\ \u0026=\\frac{P(I_t=q_i, I_{t+1}=q_j, O)}{P(O)} \\\\ \u0026=\\frac{P(I_t=q_i, I_{t+1}=q_j, O)}{\\sum_i \\sum_j P(I_t=q_i, I_{t+1}=q_j, O)} \\end{align} \\tag{1.4.3} $$ 其中$P(I_t=q_i, I_{t+1}=q_j, O)=\\alpha_t(q_i)A_{ij}B_{q_j}(o_{t+1})\\beta_{t+1}(q_j)$ $(1.4.1)$和$(1.4.3)$可以被用于化简学习问题的求解 ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:2:4","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"2. 学习问题 ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:3:0","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"2.1 Supervised: MLE 已知$I, O$, 求$A, B$的MLE, 直接用频数计算 $$ A_{ij} = \\frac{\\sum\\limits_{t=1}^{T-1} I(I_t = q_i, I_{t+1} = q_j)}{\\sum\\limits_{t=1}^{T-1} \\sum\\limits_{j=1}^{N_Q} I(I_t = q_i, I_{t+1} = q_j)} \\tag{2.1.1} $$ $$ B_{q_i}(v_j) = \\frac{\\sum\\limits_{t=1}^{T-1} I(I_t = q_i, o_t = v_j)}{\\sum\\limits_{t=1}^{T-1} \\sum\\limits_{j=1}^{N_V} I(I_t = q_i, o_t = v_j)} \\tag{2.1.2} $$ ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:3:1","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"2.2 Non-Supervised: Baum-Welch 只知道$O$, 不知道$I$, 求$A, B$的MLE, 采用EM算法 E步: 给定参数先验$\\overline \\lambda$, 计算Q函数 $$Q(\\lambda, \\overline \\lambda) = \\sum\\limits_I P(I| O, \\overline \\lambda) logP(I, O | \\lambda) \\tag{2.2.1}$$ M步: 求$Q(\\lambda, \\overline\\lambda)$对$\\lambda$的最大值 $$ \\max\\limits_{\\lambda} \\sum\\limits_I P(I| O, \\overline \\lambda) logP(I, O | \\lambda) \\tag{2.2.2}$$ $\\overline \\lambda = \\lambda$, 重复1和2. 由于 $$ P(I | O, \\overline \\lambda) = \\frac{P(I, O| \\overline \\lambda)}{P(O | \\overline \\lambda)} \\tag{2.2.3} $$ 而优化目标是最大化$P(I, O | \\lambda)$对$I$的期望,和$P(O | \\overline \\lambda)$无关 故可以用$P(I, O| \\overline \\lambda)$替代 其中 $$ P(I, O | \\lambda) = \\pi_{I_1} B_{I_1}(o_1)\\prod\\limits_{t=1}^{T-1} A_{I_{t}I_{t+1}}B_{I_{t+1}}(o_{t+1}) \\tag{2.2.4} $$ 将$(2.2.4)$带入$(2.2.2)$, 使用拉格朗日乘子法可以求出$\\lambda$ $$ \\pi_{q_i} = \\gamma_1(q_i) \\\\ A_{ij} = \\frac{\\sum\\limits_{t=1}^{T-1}\\xi_t(q_i, q_j)}{\\sum\\limits_{t=1}^{T-1} \\gamma_t(q_i)} \\\\ B_{q_i}(v_j) = \\frac{\\sum\\limits_{t=1}^{T-1} [\\gamma_t(q_i) I(o_t=v_j)]}{\\sum\\limits_{t=1}^{T-1} \\gamma_t(q_i)} \\tag{2.2.5} $$ ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:3:2","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"3. Decode 已知$\\lambda=(A, B, \\pi), O=(o_1, o_2, \\cdots, o_T)$, 求使得$P(I|O,\\lambda)$最大的状态序列$I$. 由于 $$P(I | O, \\lambda) = \\frac{P(I, O| \\lambda)}{P(O | \\lambda)}$$ 而$P(O | \\lambda)$与状态序列$I$无关, 因此该问题等价于 $$ \\begin{align} \\max\\limits_{I} P(I|O, \\lambda) \u0026 \\Leftrightarrow \\max\\limits_{I} P(I, O| \\lambda) \\\\ \u0026 \\Leftrightarrow \\max\\limits_{I} \\prod\\limits_{t=0}^{T-1} A_{I_{t}I_{t+1}}B_{I_{t+1}}(o_{t+1}) \\end{align} \\tag{3.1} $$ 其中为了简化计算, 令$A_{I_0I_1} = \\pi_{I_1}$. 由此可见, 问题转化为在$(1.2.1)$中找到一条路径使得$(4.1)$最大. 可以使用类似于Dijkstra算法的动态规划, 只不过这里加权路径使用的是累乘计算. 该算法被称为Viterbi算法. 由于图是层次的, 因此比Dijkstra算法简单 ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:4:0","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"3.1 Viterbi算法 初始化 $$ \\delta_1(q_i) = \\pi_{q_i} \\\\ \\phi_1(q_i) = 0 \\tag{3.1.1} $$ 递推. 对$t=[2, T]$: $$ \\delta_t(q_i) = \\max\\limits_{1 \\leq j \\leq N_Q} [\\delta_{t-1}(q_j)A_{ji}] B_{q_i}(o_t) \\\\ \\phi_t(q_i) = arg\\max\\limits_{1 \\leq j \\leq N_Q} [\\delta_{t-1}(q_j)A_{ji}] \\tag{3.1.2} $$ 终止 $$ P = \\max\\limits_{1 \\leq i \\leq N_Q} \\delta_T(q_i) \\\\ I_T = arg\\max\\limits_{1 \\leq i \\leq N_Q} \\delta_T(q_i) \\tag{3.1.3} $$ 逆推最优状态序列. 对$t=T-1, T-2, \\cdots, 1$ $$ I_t = \\phi_{t+1}(I_{t+1}) \\tag{3.1.4} $$ 其中 $$ \\delta_t(q_i)=\\max\\limits_{I_i \\cdots I_{t-1}} P((I_1, I_2, \\cdots, I_t=q_i), O | \\lambda) \\tag{3.1.5} $$ $\\delta_t(q_i)$代表输出为$O$且以$q_i$结尾长度为$t$的任意状态序列的最大联合概率 $\\phi_t(q_i)$代表$(3.1.5)$时的$I_{t-1}$, 顺着逆推即可得到概率最大路径 ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:4:1","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"4. Python 实现 to be continued…. ","date":"2018-02-03","objectID":"/2018-02-03-hmm/:5:0","tags":["HMM","PGM","NLP"],"title":"Hidden Markov Model","uri":"/2018-02-03-hmm/"},{"categories":null,"content":"新年第一篇, 和永无止境的八月一样循环的神经网络(其实是存货 ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:0:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Standard RNN Standard RNN结构如上图, 主要涉及: hidden state的更新 输出值y的计算 (本文忽略偏置项) $$ \\begin{aligned} h^{(t)} \u0026= tanh(Wx^{(t)} + Uh^{(t-1)}) \\\\ y^{(t)} \u0026= softmax(Vh^{(t)}) \\end{aligned} $$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:1:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"LSTM LSTM(Long Short-Term Memory)是RNN的变种, 他的诞生要追溯到1997年. Hochreiter S \u0026 Schmidhuber J (2000) 在Neural Computation上提出了该网络结构. 没错, 是Neural Computation XD LSTM意在解决RNN无法传递长程状态的缺陷, 其核心概念是Cell State ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:2:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Calculate Cell State 利用cell state来携带长程信息, 记为$c^{(t)}$. 在当前时间步$t$利用输入$x^{(t)}$和前一步的$h^{(t-1)}$产生该时刻的长程信息${\\hat c}^{(t)}$, 该层也可以称作cell state层: $$ \\hat c^{(t)} = tanh(W_c x^{(t)} + U_c h^{(t-1)}) \\tag{1.1} $$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:2:1","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Transmit Cell State 添加长程信息的频率应该是很低的, 因此设置输入门$g_{in}$挑选长程信息加入cell state: $g_{in} \\otimes {\\hat c}^{(t)}$ 如果长程信息设为$c^{(t)} = c^{(t-1)} + g_{in} \\otimes {\\hat c}^{(t)}$, 总是保留旧信息$c^{(t-1)}$会让$c^{(t)}$迅速饱和导致梯度爆炸, 无法传递长程信息. 因此需要设置遗忘门$g_{f}$控制旧cell state的去留: $g_{f} \\otimes c^{(t-1)}$ $$c^{(t)} = g_{f} \\otimes c^{(t-1)} + g_{in} \\otimes {\\hat c}^{(t)} \\tag{1.2}$$ 最后利用输出门$g_{out}$从cell state中选择合适的输出: $$ h^{(t)} = g_{out} \\otimes f(c^{(t)}) \\tag{1.3}$$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:2:2","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Long Term \u0026 Short Term 什么是LSTM中的Long Term \u0026 Short Term? 回溯cell state的计算图, 发现$c^{(t)}$的更新只涉及到element wise乘法和加法, 梯度可以稳定传播, 属于长程(Long Term)信息 回溯hidden state的计算图, 发现$h^{(t)}$的计算取决于$c^{(t)} $, 而$c^{(t)}$的计算取决于$Uh^{(t-1)}$, 故反向传播需要跨越$U$. 和RNN一样易导致梯度消失, $h^{(t)}$属于短程(Short Term)信息 ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:2:3","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Input, Forget, Output Gate \u0026 Cell State 输入门, 遗忘门, 输出门和Cell State都是根据当前输入$x^{(t)}$和前一步输出$h^{(t-1)}$决定, 这是可以并行计算的四层神经网络, 他们的权重用下标加以区分 $$ \\begin{aligned} \\hat c^{(t)} \u0026= tanh(W_c x^{(t)} + U_c h^{(t-1)}) \\\\ g_{in} \u0026= sigmoid(W_i x^{(t)} + U_i h^{(t-1)}) \\\\ g_f \u0026= sigmoid(W_f x^{(t)} + U_f h^{(t-1)}) \\\\ g_{out} \u0026= sigmoid(W_o x^{(t)} + U_o h^{(t-1)}) \\\\ \\end{aligned} \\tag{1.4} $$ 这四层网络可以直接用一个矩阵表示: $$ \\begin{aligned} \\begin{bmatrix} {\\hat c}_r^{(t)} \\\\ {\\hat g}_{in}^{(t)} \\\\ {\\hat g}_f ^{(t)} \\\\ {\\hat g}_{out}^{(t)} \\end{bmatrix} \u0026= \\begin{bmatrix} W_c \u0026 U_c \\\\ W_i \u0026 U_i \\\\ W_f \u0026 U_f \\\\ W_o \u0026 U_o \\end{bmatrix} \\cdot \\begin{bmatrix} x^{(t)} \\\\ h^{(t-1)} \\end{bmatrix} = W \\cdot I^{(t)} \\end{aligned} \\tag{1.5} $$ 其中${\\hat c_r}^{(t)}$, $\\hat g_{in}$, $\\hat g_f$, $\\hat g_{out}$代表被激活之前的仿射运算结果, 以下为简洁省略门的时间标记${(t)}$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:2:4","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Peephole Connection 四个门的开闭目前仅取决于当前输入$x^{(t)}$和前一步输出$h^{(t-1)}$, 而依上文所叙, $h^{(t-1)}$属于短程信息, 那么所有门控尤其是输出门并不能从长程信息中获益, 这不利于输出门保留长程信息. 因此将前一步的Cell State引入门单元的计算可以提高网络性能. 这项技术依然是LSTM的提出者3年后的工作, Gers \u0026 Schmidhuber (2000). 此时的计算方式为: $$ \\begin{aligned} \\begin{bmatrix} {\\hat c_r}^{(t)} \\\\ {\\hat g_{in}}^{(t)} \\\\ {\\hat g_f} ^{(t)} \\\\ {\\hat g_{out}}^{(t)} \\end{bmatrix} \u0026= \\begin{bmatrix} W_c \u0026 U_c \u0026 V_c \\\\ W_i \u0026 U_i \u0026 V_i \\\\ W_f \u0026 U_f \u0026 V_f \\\\ W_o \u0026 U_o \u0026 V_o \\end{bmatrix} \\cdot \\begin{bmatrix} x^{(t)} \\\\ h^{(t-1)} \\\\ c^{(t-1)} \\end{bmatrix} \\end{aligned} $$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:2:5","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"LSTM’s Calculation ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:3:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"FeedForward 假设输入向量为 $\\vec x \\in R^n$, 隐变量$\\vec h \\in R^d$, 则$(1.5)$中, $W_* \\in R^{d \\times n}$, $U_* \\in R^{d \\times d}$, $W \\in R^{4d \\times (n+d)}$ $$ \\begin{bmatrix} {\\hat c_r}^{(t)} \\\\ {\\hat g_i} \\\\ {\\hat g_f} \\\\ {\\hat g_o} \\end{bmatrix} = \\begin{bmatrix} W_c \u0026 U_c \\\\ W_i \u0026 U_i \\\\ W_f \u0026 U_f \\\\ W_o \u0026 U_o \\end{bmatrix} \\cdot \\begin{bmatrix} x^{(t)} \\\\ h^{(t-1)} \\end{bmatrix} = W \\cdot I^{(t)} \\tag{2.1} $$ $$ \\begin{bmatrix} {\\hat c}^{(t)} \\\\ g_i \\\\ g_f \\\\ g_o \\end{bmatrix} = \\begin{bmatrix} tanh({\\hat c}_r^{(t)}) \\\\ sigmoid(\\hat g_i) \\\\ sigmoid(\\hat g_f) \\\\ sigmoid(\\hat g_o) \\end{bmatrix} \\tag{2.2} $$ $$ c^{(t)} = g_{f} \\otimes c^{(t-1)} + g_i \\otimes {\\hat c}^{(t)} \\tag{2.3} $$ $$ h^{(t)} = g_o \\otimes tanh(c^{(t)}) \\tag{2.4} $$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:3:1","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Backward Propagation 以下$\\delta x$皆代表$\\frac{\\partial L}{\\partial x}$, 其中$L(y, \\hat y)$为损失函数 由$(2.4)$, 给定$\\delta h^{(t)}$, 求$\\delta g_o$ , $\\delta c^{(t)}$ $$ \\begin{aligned} \\delta g_o \u0026= \\frac{\\partial h^{(t)}}{\\partial g_o} \\delta h^{(t)} \\\\ \u0026= tanh(c^{(t)}) \\otimes \\delta h^{(t)} \\end{aligned} \\tag{2.5} $$ $$ \\begin{aligned} \\delta c^{(t)} \u0026= \\frac{\\partial h^{(t)}}{\\partial tanh(c^{(t)})} \\frac{\\partial tanh(c^{(t)})}{\\partial c^{(t)}} \\delta h^{(t)} \\\\ \u0026= g_o \\otimes (1 - tanh^2(c^{(t)})) \\otimes \\delta h^{(t)} \\end{aligned} \\tag{2.6} $$ 由$(2.3)$, 给定$\\delta c^{(t)}$, 求$\\delta g_{f}$, $\\delta c^{(t-1)}$, $\\delta g_i$, $\\delta {\\hat c}^{(t)}$ $$ \\begin{aligned} \\delta g_i \u0026= {\\hat c}^{(t)} \\otimes \\delta c^{(t)} \\\\ \\delta g_f \u0026= c^{(t-1)} \\otimes \\delta c^{(t)} \\\\ \\delta {\\hat c}^{(t)} \u0026= g_i \\otimes \\delta c^{(t)} \\\\ \\delta c^{(t-1)} \u0026= g_f \\otimes \\delta c^{(t)} \\\\ \\end{aligned} \\tag{2.7} $$ 由$(2.2)$, 给定$\\delta {\\hat c}^{(t)}$, $\\delta g_i$, $\\delta g_{f}$, $\\delta g_o$, 求$\\delta {\\hat c}_r^{(t)}$, $\\delta \\hat g_i$, $\\delta \\hat g_f$, $\\delta \\hat g_o$ $$ \\begin{aligned} \\delta \\hat g_i \u0026=g_i(1 - g_i) \\otimes \\delta g_i \\\\ \\delta \\hat g_f \u0026=g_f(1 - g_f) \\otimes \\delta g_f \\\\ \\delta \\hat g_o \u0026=g_o(1 - g_o) \\otimes \\delta g_o \\\\ \\delta {\\hat c}_r^{(t)} \u0026= (1 - tanh^2({\\hat c}^{(t)})) \\otimes \\delta {\\hat c}^{(t)} \\end{aligned} \\tag{2.8} $$ 由$(2.1)$, 给定$\\delta {\\hat c}_r^{(t)}$, $\\delta \\hat g_i$, $\\delta \\hat g_f$, $\\delta \\hat g_o$, 求$\\delta W^{(t)}$, $\\delta h^{(t-1)}$ $$ \\delta W^{(t)} = \\delta z^{(t)} \\cdot (I^{(t)})^T \\\\ \\delta I^{(t)} = W^T \\cdot \\delta z^{(t)} \\\\ $$ 假设一共有$t$个时间步, 前向传播完毕之后更新权重 $$ \\begin{aligned} \\delta W \u0026= \\sum_{t=1}^{T}\\delta W^{(t)} \\\\ W \u0026= W - \\epsilon \\cdot \\delta W \\end{aligned} \\tag{2.9} $$ 有了Auto Grad, 我想大部分情况下你都不需要手推LSTM的BP… ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:3:2","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"GRU 贫穷限制了我们的计算能力 简而言之LSTM需要传递长短两条状态, 而GRU(Gate Recurrent Unit)只需要一个状态, 但性能和LSTM相似, 且需要更少的参数, 只需要三组系数(两个门和一个状态). 该结构是Chung, Junyoung, et al. (2014)的工作 ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:4:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Reset \u0026 Update Gate GRU有两个门: Reset门$g_r$和Update门$g_z$, 由当前输入$x^{(t)}$和前一步输出$h^{(t-1)}$决定 $$ \\begin{aligned} \\begin{bmatrix} \\hat g_r \\\\ \\hat g_z \\end{bmatrix} \u0026= \\begin{bmatrix} W_r \u0026 U_r \\\\ W_z \u0026 U_z \\end{bmatrix} \\cdot \\begin{bmatrix} x^{(t)} \\\\ h^{(t-1)} \\end{bmatrix} \\\\ \\begin{bmatrix} g_r \\\\ g_z \\end{bmatrix} \u0026= \\begin{bmatrix} \\sigma(g_r) \\\\ \\sigma(g_z) \\end{bmatrix} \\end{aligned} \\tag{3.1} $$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:4:1","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Remember \u0026 Forget in One Shot GRU的信息传递也涉及两步 $$ h’ = tanh(\\begin{bmatrix} W_h \u0026 U_h\\end{bmatrix} \\cdot \\begin{bmatrix} x^{(t)} \\\\ g_r \\otimes h^{(t-1)} \\end{bmatrix}) \\tag{3.2} $$ $$ h^{(t)} = g_z \\otimes h^{(t-1)} + (1 - g_z) \\otimes h' \\tag{3.3} $$ 其中$(3.2)$步涉及到选择性地添加将上一步的隐状态$h^{(t-1)}$和输入$x^{(t)}$, $(3.3)$步涉及到同时进行遗忘$(g_z \\otimes h^{(t-1)})$和记忆$((1 - g_z) \\otimes h’)$, 而遗忘和记忆都通过$g_z$进行, 且归一化为1 ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:4:2","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Pytorch implementation ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:5:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Model 这里实现了一个LSTMTagger, 参考Sequence Models and Long-Short Term Memory Networks import torch from torch.autograd import Variable import torch.nn as nn class LSTMTagger(nn.Module): # 初始化隐藏层, 有两组状态, 即h和c def init_hidden(self): return (Variable(torch.zeros(1, 1, self.hidden_dim)), Variable(torch.zeros(1, 1, self.hidden_dim))) def __init__(self, num_embed, embed_dim, hidden_dim, num_label): super(LSTMTagger, self).__init__() self.hidden_dim = hidden_dim # 先将单词embed为embed_dim维向量 self.embed = nn.Embedding(num_embed, embed_dim) # 再通过LSTM self.lstm = nn.LSTM(embed_dim, hidden_dim) # 每个输出通过一个线性+激活层输出为num_label维概率分布 self.hidden2tag = nn.Linear(hidden_dim, num_label) self.log_softmax = nn.LogSoftmax() self.hidden = self.init_hidden() def forward(self, sentence): # 为每一句输入初始化h和c self.hidden = self.init_hidden() # word embedding embeds = self.embed(sentence) # 这里的out就是每一步输出的hidden state, 而self.hidden是最后一步的输出, 即out[-1] == self.hidden out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden) tag_scores = self.hidden2tag(out.view(len(sentence), -1)) log_prob = self.log_softmax(tag_scores) return log_prob ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:5:1","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Batch Mini Batch Training train.shape == (n_batch, batch_size, embedding_dim) 假设一篇文档长为10000个单词, 输出标记每个单词的词性, 即有10000个输出. 设batch_size=50, 则式$(2.9)$中$T=50$, 每传播50个单位计算一次梯度并更新, 一共迭代n_batch=10000/50=200次, 每个单词被embed为embedding\\_dim维, 计算细节如下(部分伪代码, 不是python): outputs = np.zeros((200, 50, 1)) for i\\_batch from 1 to 200: # feedforward for step from 1 to 50: hidden, outputs[i_batch][step] = lstm(hidden, train[i_batch][step]) loss = loss_function(outputs[i_batch], labels[i_batch]) loss.backward() # backprop # accumulate gradient grad = 0 for step from 1 to 50: dW = W[step].grad() # pytorch accumulate gradient automatically along timesteps grad += dW[i_ts] W = W - learning_rate * grad Variable Length Training: Using pack_padded_sequence() 假设训练集为3句话, 分别包括(3, 2, 4)个单词, 每个单词进行2维 embedding, 原则上需要一句话一个batch进行训练, 但每句话长度不一样, 无法固定batch_size: train = [ [[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10]], [[4, 3], [6, 5], [2, 1], [0, 0]] ] 此时可以使用padding对齐所有变长的句子, 参考pytorch论坛的讨论 ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:5:2","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Appendix ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:6:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Gradient of Activate Funtions $$ \\begin{aligned} y \u0026= sigmoid(x) = \\frac{1}{1 + e^x} \\\\ y’ \u0026= y (1 - y) \\end{aligned} \\tag{a.1} $$ $$ \\begin{aligned} y \u0026= tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\ y’ \u0026= 1 - y^2 \\end{aligned} \\tag{a.2} $$ ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:6:1","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"Reference LSTM Forward and Backward Pass Understanding LSTM Networks Step-by-step to LSTM: 解析LSTM神经网络设计原理 人人都能看懂的GRU 三次简化一张图: 一招理解LSTM/GRU门控机制 to be continued… ","date":"2018-01-29","objectID":"/2018-01-29-lstm/:7:0","tags":["LSTM","Neural Network","NLP"],"title":"循环神经网络的前世今生","uri":"/2018-01-29-lstm/"},{"categories":null,"content":"茴字有四种写法, 二叉树有三种遍历手段(递归, 栈, morris遍历) ","date":"2017-12-30","objectID":"/2017-12-30-binarytreetraverse/:0:0","tags":["Algorithm"],"title":"Binary Tree Traverse","uri":"/2017-12-30-binarytreetraverse/"},{"categories":null,"content":"先序遍历 root进栈 栈非空: 栈顶元素(父节点)出栈并处理 右子节点, 左子节点进栈, 这样下一轮会先处理左子节点, 符合父-\u003e左-\u003e右的顺序 vector\u003cint\u003e preorderTraversal(TreeNode *root) { vector\u003cint\u003e result; stack\u003cTreeNode*\u003e s; if(root == NULL) return result; s.push(root); while(!s.empty()) { TreeNode* top= s.top(); s.pop(); result.push_back(top-\u003eval); if(top-\u003eright) s.push(top-\u003eright); if(top-\u003eleft) s.push(top-\u003eleft); } return result; } ","date":"2017-12-30","objectID":"/2017-12-30-binarytreetraverse/:0:1","tags":["Algorithm"],"title":"Binary Tree Traverse","uri":"/2017-12-30-binarytreetraverse/"},{"categories":null,"content":"后序遍历 为了按后序遍历的顺序处理每个节点, 在考察每个节点时需要先判断前一轮循环处理的节点, 用变量last_pop维护. 是否处理完左右子树? 通过查看last_pop是否为当前节点的子节点判断. 因为如果已经处理了左/右子树, 他们都已出栈, 现在栈顶的元素会是他们的父节点. (左右皆可能, 因为不一定有右子节点, 故last_pop也可能是左子节点) 若是, 处理当前节点. 出栈并打印, 更新last_pop 若不是, 先处理左右子树, 即按右/左顺序压栈, 下一轮循环就会先处理左子节点(栈顶) 是否到达叶子节点? 若到达, 出栈并打印, 更新last_pop 变量解释与实现: cur: 当前遍历指针, 初始状态指向root last_pop: 上次处理的节点 finishedSubtrees: 是否处理完左右子树 isLeaf: 是否到达叶子节点 vector\u003cint\u003e postorderTraversal(TreeNode* root) { if (root == NULL) return result; stack\u003cTreeNode*\u003e s; vector\u003cint\u003e result; TreeNode* last_pop = root; s.push(root); while (!s.empty()) { TreeNode* cur = s.top(); bool finishedSubtrees = (cur-\u003eright == last_pop || cur-\u003eleft == last_pop); bool isLeaf = (cur-\u003eleft == NULL \u0026\u0026 cur-\u003eright == NULL); if (finishedSubtrees || isLeaf) { s.pop(); result.push_back(cur-\u003eval); last_pop = cur; } else { if (cur-\u003eright) s.push(cur-\u003eright); if (cur-\u003eleft) s.push(cur-\u003eleft); } } return result; } 例子 0 / \\ 1 2 / / \\ 3 4 5 last_pop cur finishedSubtrees isLeaf op stack 0 0 F F init 0 0 0 F F br2: push 2,1 021 0 1 F F br2: push 3 0213 0 3 F T br1: pop 3 021 3 1 T F br1: pop 1 02 1 2 F F br2: push 5,4 0254 1 4 F T br1: pop 4 025 4 5 F T br1: pop 5 02 5 2 T F br1: pop 2 0 2 0 T F br1: pop 0 br代表进入了哪个判断分支. Result: 3, 1, 4, 5, 2, 0 ","date":"2017-12-30","objectID":"/2017-12-30-binarytreetraverse/:0:2","tags":["Algorithm"],"title":"Binary Tree Traverse","uri":"/2017-12-30-binarytreetraverse/"},{"categories":null,"content":"中序遍历 同后序遍历, 只不过处理顺序需要稍微改一改, 遍历(压栈)顺序依然维持不变 是否处理完左子树? 通过查看last_pop是否为当前节点的左子节点判断. 若无左子节点也一并视为已处理完 若是, 处理当前节点. 出栈并打印, 更新last_pop, 并将右子节点压栈, 等待下一轮处理 若不是, 先处理左子树, 即左节点压栈, 等待下一轮处理 是否到达叶子节点? 若到达, 出栈并打印, 更新last_pop 变量解释与实现 cur: 当前遍历指针, 初始状态指向root last_pop: 上次处理的节点 finishedLeftTrees: 是否处理完左子树 isLeaf: 是否到达叶子节点 vector\u003cint\u003e inorderTraversal1(TreeNode* root) { vector\u003cint\u003e result; stack\u003cTreeNode*\u003e s; if(root == NULL) return result; TreeNode* last_pop = root; s.push(root); while (!s.empty()) { TreeNode* cur = s.top(); bool finishedLeftTrees = (cur-\u003eleft == NULL || cur-\u003eleft == last_pop); bool isLeaf = (cur-\u003eleft == NULL \u0026\u0026 cur-\u003eright == NULL); if (finishedLeftTrees || isLeaf) { s.pop(); result.push_back(cur-\u003eval); last_pop = cur; if (cur-\u003eright) s.push(cur-\u003eright); } else { if (cur-\u003eleft) s.push(cur-\u003eleft); } } return result; } ","date":"2017-12-30","objectID":"/2017-12-30-binarytreetraverse/:0:3","tags":["Algorithm"],"title":"Binary Tree Traverse","uri":"/2017-12-30-binarytreetraverse/"},{"categories":null,"content":"线索树的构造 利用已存在的空节点保存前驱/后继节点, 这方面的文章很多, 中文资料参考 https://www.cnblogs.com/AnnieKim/archive/2013/06/15/MorrisTraversal.html ","date":"2017-12-30","objectID":"/2017-12-30-binarytreetraverse/:0:4","tags":["Algorithm"],"title":"Binary Tree Traverse","uri":"/2017-12-30-binarytreetraverse/"},{"categories":null,"content":"生存模型(Survival Models)属于General Linear Model, 被广泛用于Censored Data的建模, 譬如用户流失预测. 这里介绍下最基本的生存模型以及在Censored Data上的MLE估计 ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:0:0","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Survival Function Assume $T$ is a continuous random variable indicates the death occurrence time, we have: $$ F(t) = P\\lbrace T \u003c t\\rbrace = \\int_0^t f(t) dt \\tag{1.1} $$ Then the Survival Function should be: $$ S(t) = P\\lbrace T \u003e t\\rbrace = 1 - F(t) = \\int_t^\\infty f(t) dt \\tag{1.2} $$ ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:1:0","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Harzard Function An alternative way to characterization the distribution is given by harzard function, or instantaneous rate of occurrence of the event: $$ \\begin{align} \\lambda(t) \u0026= \\lim_{dt \\to 0} \\frac{P\\lbrace t \\le T \u003c t + dt | T \\ge t\\rbrace}{dt} \\\\ \u0026= \\lim_{dt \\to 0} \\frac{P\\lbrace t \\le T \u003c t + dt \\rbrace}{P \\lbrace T \\ge t\\rbrace dt} \\\\ \u0026= \\lim_{dt \\to 0} \\frac{f(t)dt}{S(t) dt} \\\\ \u0026= \\frac{f(t)}{S(t)} \\end{align} \\tag{2.1} $$ Given $(1.2)$ we have $\\frac{d}{dt} S(t) = -f(t)$, so $(2.1)$ has another form $$ \\lambda(t) = -\\frac{d}{dt} log S(t) \\tag{2.2} $$ We could derive survival function from harzard function as well: $$ S(t) = exp\\lbrace - \\int_0^t \\lambda(x)dx \\rbrace = exp\\lbrace -\\Lambda(t) \\rbrace \\tag{2.3} $$ In which $\\Lambda(t) = \\int_0^t \\lambda(x)dx$, called cumulative hazard ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:2:0","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Example 2.1 Here we’re modeling a constant risk over time: $$ \\lambda(t) = \\lambda $$ From $(2.2)$, we could solve corresponding survival function and pdf $$ \\begin{align} S(t) \u0026= exp\\lbrace - \\int_0^t \\lambda(x)dx \\rbrace = e^{-\\lambda t} \\\\ f(t) \u0026= \\lambda e^{-\\lambda t} \\end{align} $$ That is exactly an exponential distribution ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:2:1","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Expectation of Life Given $S(t)$ or $\\lambda(t)$, it’s easy to denote expected value of $T$ $$ \\mu = \\int_0^\\infty tf(t)dt =\\int_0^\\infty S(t)dt $$ ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:2:2","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Censoring and the likelihood function ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:3:0","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Censoring Type Type I Typically 2 types of observatioin: A sample of $n$ units is followed for a fixed time $\\tau$ Generalization, fixed censoring: each unit has a fixed time $\\tau_i$ In cases above, number of deaths is a random variable. Type II A sample of $n$ units is followed as long as necessary until $d$ units have experienced the event Generalization, random censoring: Each unit has: Censoring time $C_i$ Potential lifetime $T_i$ Observe time $Y_i = min\\lbrace C_i, T_i\\rbrace$ Indicator $d_i, \\delta_i$ tells us whether the observation is terminated by death or censoring ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:3:1","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Likelihood of censoring model Unit died at $t_i$. Since we know it is dead while survives till $t_i$, we have: $$ L_i = f(t_i) = S(t_i)\\lambda(t_i) \\tag{3.1} $$ Unit still alive at $t_i$. We only know it survives till $t_i$ $$ L_i = f(t_i) = S(t_i) \\tag{3.2} $$ Given 2 conditions above, we have: $$ L = \\prod\\limits_{i=1}^{n}L_i = \\prod\\limits_{i} \\lambda(t_i)^{d_i}S(t_i) \\tag{3.3} $$ Taking logs, considering $(2.3)$, we have: $$ log L = \\sum\\limits_{i=1}^{n} \\lbrace d_ilog\\lambda(t_i) - \\Lambda(t_i) \\rbrace \\tag{3.4} $$ ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:3:2","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":"Example 3.1 Considering exponential distribution $\\lambda(t) = \\lambda$, from$(3.4)$, we have $$ log L = \\sum\\limits_{i=1}^{n} \\lbrace d_ilog\\lambda - \\lambda t_i \\rbrace $$ We could estimate $\\lambda$ using MLE: Let $D=\\sum d_i$ denotes the total number of deaths, $T = \\sum t_i$ denotes total number of observation time: $$ \\begin{align} log L \u0026= Dlog\\lambda - T\\lambda \\\\ \\frac{\\partial}{\\partial \\lambda} L \u0026= \\frac{D}{\\lambda} - T \\end{align} $$ Letting $\\frac{\\partial}{\\partial \\lambda} L = 0$ we get the estimation of $\\lambda$ $$ \\hat \\lambda = \\frac{D}{T} $$ Reference ","date":"2017-12-26","objectID":"/2017-12-26-survival-models/:3:3","tags":["stat","GLM"],"title":"Survival Models","uri":"/2017-12-26-survival-models/"},{"categories":null,"content":" 全面翻新的pandas介绍, 篇幅较长, 请善用右下角目录! import numpy as np import pandas as pd ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:0:0","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"数据结构 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:1:0","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"Series Series(data=None, index=None, dtype=None, name=None, copy=False, , fastpath=False) data: 可遍历序列数据 index: 索引, hsahable即可. 默认为从0开始的range dtype: numpy.dtype数据类型 copy: 是否为data的副本(默认为视图, 对它的改动会直接反映到data上) # 数组初始化 s = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c']) # 字典初始化 s = pd.Series({'d': 4, 'b':7, 'a':-5, 'c':3}) a -5 b 7 c 3 d 4 dtype: int64 # 转换为numpy数组 s.values array([-5, 7, 3, 4]) # 取出索引 s.index Index(['a', 'b', 'c', 'd'], dtype='object') # name属性: Series的name会自动继承于DataFrame的列名, Index的name会自动继承于Series的索引名 s.name = 'population' s.index.name = 'state' ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:1:1","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"DataFrame DataFrame(data=None, index=None, columns=None, dtype=None, copy=False) data: 初始化方式和Series相似, 可以传入二维数组或者字典(key作为列名, value是等长数组) columns: 列名. 字符串列表 其他参数和Series相似 # 初始化 df = pd.DataFrame(np.arange(16).reshape((4,4)), index=['ohio','colorado', 'utah', 'new york'], columns=['one', 'two', 'three', 'four']) df ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:1:2","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"索引对象 索引对象是不可改变的 索引的方法类似于集合操作: 包括diff, intersection, union, isin, drop, unique等 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:1:3","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"基本功能 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:2:0","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"reindex() DataFrame.reindex(index=None, columns=None, **kwargs) index: 重排的索引顺序 columns: 重排的列顺序(DataFrame独有) method: 插值选项 fill_value: 插值时用于填充的值 limit: 最大填充量 level: 层次索引的层级 copy: 拷贝副本 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:2:1","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"drop() DataFrame.drop(labels, axis=0, level=None, inplace=False, errors='raise') labels: index列表或者column列表, 取决于axis是’index’还是’columns' axis: 按行/列删除 level: 层次索引的层次 inplace: 就地删除 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:2:2","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"索引 Series的索引 # 取元素 s['a'] -5 # 切片, 注意切片是闭区间, 和数组不同 s['a':'c'] state a -5 b 7 c 3 Name: population, dtype: int64 # 花式索引 s[['c', 'a', 'b']] state c 3 a -5 b 7 Name: population, dtype: int64 DataFrame的索引 # 取列 df['two'] ohio 1 colorado 5 utah 9 new york 13 Name: two, dtype: int64 # 取多列 df[['three', 'one']] ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:2:3","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"算数计算和数据对齐: add(), sub(), etc. Series之间和DataFrame之间的运算 # 初始化 s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e']) s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g']) print(s1) print(s2) a 7.3 c -2.5 d 3.4 e 1.5 dtype: float64 a -2.1 c 3.6 e -1.5 f 4.0 g 3.1 dtype: float64 # 加法会自动对齐索引, 不重叠的索引会引入nan s1 + s2 a 5.2 c 1.1 d NaN e 0.0 f NaN g NaN dtype: float64 # 使用fill_value参数可以指定不重叠部分的默认填充值 s1.add(s2, fill_value=0) a 5.2 c 1.1 d 3.4 e 0.0 f 4.0 g 3.1 dtype: float64 s1.add(s2, fill_value=1) a 5.2 c 1.1 d 4.4 e 0.0 f 5.0 g 4.1 dtype: float64 DataFrame和Series的运算 DataFrame和Series做加减法时, 会将Series的索引匹配到DataFrame的列, 然后沿着行一直向下广播 df = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon']) s1 = df.loc['Utah'] # 一行 s2 = df.loc[:, 'd'] # 一列 print(df) print(s1) print(s2) b d e Utah 0.0 1.0 2.0 Ohio 3.0 4.0 5.0 Texas 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 b 0.0 d 1.0 e 2.0 Name: Utah, dtype: float64 Utah 1.0 Ohio 4.0 Texas 7.0 Oregon 10.0 Name: d, dtype: float64 # 匹配某一列, 然后沿着每一列广播 df.sub(s2, axis='index') ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:2:4","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"函数和映射: apply() apply可以作用到每个元素, 或者应用到各行各列形成的一维数组上 df = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon']) df ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:2:5","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"排序 按索引排序: sort_index() 默认是按升序排列, 参数ascending可以改为降序 # Series按索引排序 s = pd.Series(range(4), index=['d', 'a', 'b', 'c']) s.sort_index() a 1 b 2 c 3 d 0 dtype: int64 # DataFrame按索引和列排序. 通过axis参数选择按索引还是列排序 df = pd.DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=['d', 'a', 'b', 'c']) # 按索引排序(默认) df.sort_index(axis='index') ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:2:6","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"汇总和计算描述统计 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:3:0","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"求和: Sum() sum(axis=None, skipna=None, level=None, numeric_only=None, **kwargs) axis: 沿着索引还是列求和 skipna: 是否排除NaN. 若不排除, 只要有NaN, 和就为NaN level: 层次索引的层次 其他平均数, 中位数, 方差, 标准差等方法类似 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:3:1","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"相关系数和协方差 corr(method='pearson', min_periods=1) method : {‘pearson’, ‘kendall’, ‘spearman’}, 协方差计算公式 min_periods : 可选, 最小样本数 两个Series的相关系数和协方差的计算, 满足: 按索引对齐 索引重叠 非NaN DataFrame的协方差和相关系数会直接返回协方差矩阵: Compute pairwise correlation of columns, excluding NA/null values # 按列配对计算协方差 df.corr() ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:3:2","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"unique, value_counts和isin Series.unique() 返回Series的唯一值序列 Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True) normalize: 用频率替换频度 sort: 按频率排序 ascending: 升/降序排列, 默认是降序 bins: 自动划分阈值, 将数值类数据分为bins块 dropna: 去掉NaN s = pd.Series([7,3,2,2,5,6,6,5,9]) s.value_counts(bins=2) (1.992, 5.5] 5 (5.5, 9.0] 4 dtype: int64 DataFrame.isin(values) values: 数值列表 返回masked DataFrame, 在values中的为True, 否则为False 当values为序列时, 很显然 当values为字典时, DataFrame的列匹配key, 值匹配value 当values为DataFrame时, 相当于掩码, 见以下例子 df = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']}) other = pd.DataFrame({'A': [1, 3, 3, 2], 'B': ['e', 'f', 'f', 'e']}) print(df) print(other) A B 0 1 a 1 2 b 2 3 f A B 0 1 e 1 3 f 2 3 f 3 2 e df.isin(other) ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:3:3","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"处理缺失数据 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:4:0","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"dropna() DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False) axis: {‘index’, ‘columns’}, 丢掉含有NaN的行/列, 默认为行 how: {‘any’, ‘all’}, 只要有NaN就丢弃还是全NaN才丢弃 thresh: 大于等于多少个NaN就丢弃 subset: 如果丢弃行, 这就是一个列名的列表, 处于这个列表中的列才会计入NaN inplace: 就地丢弃 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:4:1","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"fillna() DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs) 参数不解释了, 只提一下value还可以是dict, Series, 乃至DataFrame, 填充方式与掩码相似 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:4:2","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"层次化索引 index是可以使用多组数组的, 代表了不同层次的索引 # 初始化层次索引 s = pd.Series(np.random.randn(10), index=[['a','a','a','b','b','b','c','c','d','d'], [1,2,3,1,2,3,1,2,2,3], [9,8,7,9,8,7,9,8,8,7]]) a 1 9 0.320638 2 8 -0.666312 3 7 -0.549029 b 1 9 -0.375222 2 8 0.942717 3 7 0.588951 c 1 9 2.538862 2 8 -1.635290 d 2 8 -1.602510 3 7 0.187961 dtype: float64 # 索引属于MultiIndex类 s.index MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3], [7, 8, 9]], labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2], [2, 1, 0, 2, 1, 0, 2, 1, 1, 0]]) ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:5:0","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"层次化索引方式 s['a'] 1 9 0.320638 2 8 -0.666312 3 7 -0.549029 dtype: float64 s['b':'c'] b 1 9 -0.375222 2 8 0.942717 3 7 0.588951 c 1 9 2.538862 2 8 -1.635290 dtype: float64 s[['b', 'd']] b 1 9 -0.375222 2 8 0.942717 3 7 0.588951 d 2 8 -1.602510 3 7 0.187961 dtype: float64 # 这里代表了第一层和第二层索引, 而不是行和列 s[:, 2] a 8 -0.666312 b 8 0.942717 c 8 -1.635290 d 8 -1.602510 dtype: float64 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:5:1","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"转化为DataFrame: unstack() 将最内层的索引变成新的列名, 形成新的DataFrame # 拆分 s.unstack() ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:5:2","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"重排层次顺序: swaplevel() # 交换两个index的层次顺序 swaplevel(indexName1, indexName2) ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:5:3","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"根据级别汇总统计 参考Sum()的level参数 ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:5:4","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"使用DataFrame的列 # 初始化并命名索引 df = s.unstack() df.index.names = ['index1', 'index2'] df ","date":"2017-09-29","objectID":"/code/2017-09-28-pandas/:5:5","tags":["pandas","python-data-analysis"],"title":"Python数据分析笔记2 pandas基础","uri":"/code/2017-09-28-pandas/"},{"categories":null,"content":"C++ 和 Python 混合编程可以兼顾性能与效率 ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:0:0","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"Python扩展 官方文档 python在设计之初就考虑到让模块的导入机制足够抽象, 抽象到让使用者无法了解到模块的具体实现细节, 甚至哪种编译语言写的都分辨不出来 为何要扩展python? 添加额外的功能: 有些功能python没有实现, 或者其他语言有现成的实现 性能瓶颈的效率提升: 现做一个简单的代码性能测试, 找到瓶颈之后在扩展中实现 保持专有代码的私密: 编译后的目标文件不容易被逆向工程 ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:1:0","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"创建Python扩展 ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:2:0","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"编写C++代码 这里实现了两个简单的阶乘和逆置字符串的函数 #include \u003ciostream\u003e using namespace std; /* 阶乘 */ int fac(int n) { if (n \u003c 2) {return(1);} return n*fac(n - 1); } /* 逆置字符串 */ char* rev(const char *c) { string s(c); reverse(s.begin(), s.end()); char *ret = strdup(s.c_str()); return ret; } /* 主函数用于测试 */ int test() { cout \u003c\u003c \"4! == \" \u003c\u003c fac(4) \u003c\u003c endl; cout \u003c\u003c \"reverse abc: \" \u003c\u003c rev(\"abc\") \u003c\u003c endl; return 0; } 此处主函数被改名成了test, 主要是为了导入Python之后可以调用, 如果直接用main是比较危险的, 因此改名为test ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:2:1","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"用样板包装C++代码 主要需要四个步骤 包含Python的头文件Python.h. 注意必须放在所有include声明的最前面 为模块的每个函数增加一个形如PyObject * Module_func()的包装函数 为每个模块增加一个形如PyMethodDef ModuleMethods[]的数组, 并构造PyModuleDef结构体 增加模块初始化函数PyInit_Module, 向PyModule_Create传入PyModuleDef结构体 #include \u003cPython.h\u003e /* 包装fac */ static PyObject * Extest_fac(PyObject *self, PyObject *args) { int num; if (PyArg_ParseTuple(args, \"i\", \u0026num)) { return Py_BuildValue(\"i\", fac(num)); } return NULL; } /* 包装rev */ static PyObject * Extest_rev(PyObject *self, PyObject *args) { char *orig_str; char *dupl_str; PyObject * retval; if (PyArg_ParseTuple(args, \"s\", \u0026orig_str)) { retval = Py_BuildValue(\"ss\", orig_str, dupl_str=rev(strdup(orig_str))); free(dupl_str); return retval; } return NULL; } /* 包装test */ static PyObject * Extest_test(PyObject *self, PyObject *args) { test(); return Py_BuildValue(\"\"); } 这段代码为每个被Python访问的函数包裹了一个静态函数, 函数名遵循模块名_函数名()的规则, 即 Module_func(), 主要功能如下: 接收PyObject *类型的数据 将其转换成C++数据类型, 涉及PyArg_Parse系列函数 调用相关函数处理 将输出值转换成PyObject *类型并返回, 涉及Py_BuildValue函数 简单介绍一下以上概念 Q: 什么是PyObject? 为什么要用PyObject *作为返回值? A: PyObject代表了Python中任一种对象. 得益于Python处理对象的机制几乎完全一致, 在C/C++中只用一种类型来表示似乎很合理. 几乎所有Python对象都生存在堆区, 因此不可以声明auto或者静态PyObject, 只能用指针作为返回值. 接下来介绍一下代码中用到的两个主要函数, PyArg_ParseTuple和Py_BuildValue ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:2:2","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"包装函数简介 介绍一下常用的包装函数. 有关parse的详细内容, 参考官方文档: Parsing arguments and building values PyArg_ParseTuple(PyObject *args, const char *format, …) args: 被解析的参数变量 format: 一个字符串告诉我们如何去解析元组中每一个元素。字符串的第n个字母正是代表着元组中第n个参数的类型。例如\"i\"代表整形\"s\"代表字符串类型, “O\"则代表一个Python对象. 有关含义参考文档 ...: 接下来的参数都是你想要通过PyArg_ParseTuple()函数解析并保存的元素 这样做之后, 参数的数量和模块中函数期待得到的参数数量保持一致并保证了位置的完整性。例如我们想传入一个字符串一个整数和一个Python列表可以这样写: int n; char *s; PyObject* list; PyArg_ParseTuple(args, \"siO\", \u0026n, \u0026s, \u0026list); 另外, PyArg_ParseTuple只能解析传入固定位置参数(positional parameters)的函数, 对于键值对参数的函数, 有PyArg_ParseTupleAndKeywords等其他一系列函数处理 PyObject* Py_BuildValue(const char *format, …) format: 和PyArg_ParseTuple一样, 控制参数类型 ...: 需要返回的对象 ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:2:3","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"添加函数与模块 写好包装函数之后, 我们需要把它们列在某个地方, 让python解释器能够导入并调用它们. 这就是我们的ExtestMethods要做的事情 /* 注册每个函数 */ static PyMethodDef ExtestMethods[] = { {\"fac\", Extest_fac, METH_VARARGS}, {\"rev\", Extest_rev, METH_VARARGS}, {\"test\", Extest_test, METH_VARARGS}, {NULL, NULL} }; /* 创建模块定义 */ static struct PyModuleDef extestDemo = { PyModuleDef_HEAD_INIT, \"Extest\", /* name of module */ \"cpp extent test\", /* module documentation, may be NULL */ -1, /* size of per-interpreter state of the module, or -1 if the module keeps state in global variables. */ ExtestMethods }; /* 初始化模块 */ PyMODINIT_FUNC PyInit_Extest(void) { return PyModule_Create(\u0026extestDemo); } 注意在python 3中, 和2.x不一样的地方: 参考这个问题. 这里使用的是python3 ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:2:4","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"安装模块 在同目录下创建一个setup.py, 使用distutils来将c++代码编译成系统可用的目标文件 from distutils.core import setup, Extension MOD = 'Extest' setup(name=MOD, ext_modules=[ Extension('Extest', sources=['Extest.cpp'])]) MOD是包名 Extension的第一个参数是模块名, 如果是包下的模块的话可以使用A.B. ext_modules是个列表意味着可以同时注册包下的多个模块. 在目录下运行 $ python setup.py build $ python setup.py install 然后就可以在python里调用这个包了 \u003e\u003e\u003e import Extest \u003e\u003e\u003e Extest.test() 4! == 24 reverse abc: cba ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:2:5","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"一些坑 在clion里编译需要在cMakeLists里加入以下几行 find_package(PythonLibs REQUIRED) include_directories(${PYTHON_INCLUDE_DIRS}) 参考问题 想要卸载已经安装的包, 要记下安装路径手动删除: $ python setup.py install --record files.txt $ cat files.txt | xargs rm -rf 此外, 还有计数引用的问题以及GIL的问题, 待续 ","date":"2017-09-27","objectID":"/2017-09-27-python-extend/:2:6","tags":["python"],"title":"Python核心编程笔记5 扩展Python","uri":"/2017-09-27-python-extend/"},{"categories":null,"content":"即使有GIL的存在使得python的多线程显得鸡肋, 但在重I/O应用中还是很实用, 并且multiprecessing是基于threading的, 有必要理解 ","date":"2017-09-26","objectID":"/2017-09-26-python-thread/:0:0","tags":["python"],"title":"Python核心编程笔记4 多线程","uri":"/2017-09-26-python-thread/"},{"categories":null,"content":"GIL 全局解释器锁(Global Interpreter Lock, GIL)是Python虚拟机的一个特性, 保证了任意时刻只有一个线程在解释器中运行. Python虚拟机的执行方式如下: 设置GIL 切换到一个线程运行 运行线程 执行指定数量的字节码 线程主动让出控制(如调用time.sleep(0)) 把线程设置为睡眠状态 解锁GIL 重复1~5 在调用外部代码(如c/c++扩展函数)时, GIL将锁定到这个函数返回为止. 编写扩展的程序员可以主动解锁GIL. 例如对面向I/O(调用内建C代码)的程序, GIL会在I/O调用之前被解锁, 以允许其他线程在等待I/O时运行. ","date":"2017-09-26","objectID":"/2017-09-26-python-thread/:1:0","tags":["python"],"title":"Python核心编程笔记4 多线程","uri":"/2017-09-26-python-thread/"},{"categories":null,"content":"thread模块 不建议使用thread模块. thread不支持守护线程, 只要主线程退出, 所有其他线程没有被清除就退出了, 这是不安全的. 有关守护进程, 见Threading模块的相应部分. thread模块的方法 函数 说明 start_new_thread(func,args,kw=None) 产生一个新线程调用func allocate_lock() 分配一个LockType类的锁对象 exit() 退出线程 LockType类型锁对象的方法 方法 说明 acquire(wait=None) 尝试获取锁对象 locked() 是否获取锁对象 release() 释放锁 运行两个io进程的例子. (在python3.x中thread已被抛弃, 改名为_thread) import _thread from time import sleep, ctime ioSec = [4, 2] # 一个I/O函数, 利用sleep模拟I/O操作 # ioName为函数序号, nsec:耗时, locks:锁序列, i:锁序号 def i_o(ioName, nsec, lock): print(\"I/O线程 %d 开始于 %s\" % (ioName, ctime())) sleep(nsec) print(\"I/O线程 %d 结束于 %s\" % (ioName, ctime())) lock.release() # 解锁 def main(): locks = [] nloops = range(len(ioSec)) # 先给所有子线程加锁 for i in nloops: lock = thread.allocate_lock() lock.acquire() locks.append(lock) # 执行子线程 for i, sec in enumerate(ioSec): _thread.start_new_thread(i_o, (i, ioSec[i], locks[i])) # 自旋锁 for lock in locks: while lock.locked(): pass if __name__ == '__main__': main() 程序输出 I/O线程 0 开始于 Tue Sep 26 16:04:54 2017 I/O线程 1 开始于 Tue Sep 26 16:04:54 2017 I/O线程 1 结束于 Tue Sep 26 16:04:56 2017 I/O线程 0 结束于 Tue Sep 26 16:04:58 2017 虽然两个I/O线程只运行了4秒, 是最长的线程的运行时间. 但是自旋锁会顺序从第一个锁开始检查, 所以如果后面的锁如果提前释放了, 可能会浪费等待时间. (可以跳过, 这段代码尝试维护一个locks列表来替代LockType对象) import _thread from time import sleep, ctime ioSec = [4, 2] # 一个I/O函数, 利用sleep模拟I/O操作 # ioName为函数序号, nsec:耗时, locks:锁序列, i:锁序号 def i_o(ioName, nsec, locks, i): print(\"I/O线程 %d 开始于 %s\" % (ioName, ctime())) sleep(nsec) print(\"I/O线程 %d 结束于 %s\" % (ioName, ctime())) locks[i] = False # 解锁 def main(): # 先给所有子线程加锁 locks = [True]*len(ioSec) # 执行子线程 for i, sec in enumerate(ioSec): _thread.start_new_thread(i_o, (i, ioSec[i], locks, i)) # 自旋锁 for i in range(len(locks)): while locks[i]: pass if __name__ == '__main__': main() (一个小bug) 如果自旋锁实现还是和上一节一样: for lock in locks: while lock: pass 则在debug模式下发现lock始终为True, 即使locks中所有元素已经变成了False. 目前还不知道为何会发生这种情况. 可能是lock为原数组元素的引用复制, 因此一直没有改变, 所以最好还是使用序号遍历来避免这种潜在的bug. 当然使用LockType对象最为安全. ","date":"2017-09-26","objectID":"/2017-09-26-python-thread/:2:0","tags":["python"],"title":"Python核心编程笔记4 多线程","uri":"/2017-09-26-python-thread/"},{"categories":null,"content":"threading模块 threading模块最重要的是Thread对象, 简单介绍一下: ","date":"2017-09-26","objectID":"/2017-09-26-python-thread/:3:0","tags":["python"],"title":"Python核心编程笔记4 多线程","uri":"/2017-09-26-python-thread/"},{"categories":null,"content":"Thread类 class threading.Thread(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None) group: 应为 None; 是为ThreadGroup类预留的参数 target: 被 run() 调用的函数(或者其他可调用对象), 默认为None name: 线程名, 默认为“Thread-N”, 其中N是个小自然数 args: trarget的可变长参数. 默认为空元组 kwargs: 关键字变量参数, 默认为空字典 daemon: 设置守护进程属性. 如果是None, 那么该线程是否为守护线程继承于当前线程 几个注意点: This constructor should always be called with keyword arguments. 该构造函数必须用关键字方式初始化 If the subclass overrides the constructor, it must make sure to invoke the base class constructor (Thread.init()) before doing anything else to the thread. 如果子类覆盖了构造函数, 必须首先调用基类的构造函数 守护线程的解释: 如果某个线程被设置为主线程的守护进程(通过Thread.setDaemon(True)), 那么主线程退出时不用等待这些子线程完成. ","date":"2017-09-26","objectID":"/2017-09-26-python-thread/:3:1","tags":["python"],"title":"Python核心编程笔记4 多线程","uri":"/2017-09-26-python-thread/"},{"categories":null,"content":"Thread类的其他方法: 方法 说明 start() 开始执行线程 run() 定义线程功能, 一般被子类覆盖 join(timeout=None) 程序挂起直到线程结束或者timeout getName(),setName(name) 获取/设置线程名 isAlive() 线程是否还在运行中 isDaemon() 是否为守护进程 setDaemon() 设置守护进程属性, 一定要在start()前调用 ","date":"2017-09-26","objectID":"/2017-09-26-python-thread/:3:2","tags":["python"],"title":"Python核心编程笔记4 多线程","uri":"/2017-09-26-python-thread/"},{"categories":null,"content":"使用Thread类 创建一个Thread实例, 传给它一个函数 创建一个Thread实例, 传给它一个可调用的类对象. 对象必须实现__init__和__call__方法 从Thread派生出一个子类, 创建一个这个子类的实例, 覆盖基类的run() 此处只提供一下最后一种方法的实现: import threading from time import ctime, sleep class MyThread(threading.Thread): def __init__(self, func, args, name=''): threading.Thread.__init__(self) self.func = func self.args = args self.name = name def run(self): print(\"线程 %s 开始于 %s\" % (self.name, ctime())) self.res = self.func(*self.args) print(\"线程 %s 结束于 %s\" % (self.name, ctime())) def getResult(self): return self.res def i_o(x): sleep(x) return x def main(): fib_num = [[3], [2], [4]] threads = [] for i, num in enumerate(fib_num): threads.append(MyThread(i_o, num, i)) for item in threads: item.start() for item in threads: item.join() print(\"%s: %s\" % (item.name, item.getResult())) if __name__ == '__main__': main() 输出结果: 线程 0 开始于 Tue Sep 26 21:41:57 2017 线程 1 开始于 Tue Sep 26 21:41:57 2017 线程 2 开始于 Tue Sep 26 21:41:57 2017 线程 1 结束于 Tue Sep 26 21:41:59 2017 线程 0 结束于 Tue Sep 26 21:42:00 2017 0: 3 1: 2 线程 2 结束于 Tue Sep 26 21:42:01 2017 2: 4 [Finished in 4.2s] 其中由于apply()已经被弃用, 使用func(*args)替代, 所以args需要是可迭代对象, 就用了数组. 这里有一个有趣的地方, 在最后一个循环中, print(\"%s: %s\" % (item.name, item.getResult()))是写在循环内部的. 这意味着主线程必须按顺序等待子线程结束之后才会join后续线程. 整个过程可以简单表示如下: t 0 2 3 4 m +***+***+***+ | | | | 0 +-------+ | | | | 1 +---+ | | | 2 +-----------+ + 开始/结束 * 等待 - 运行 主线程同时启动三个子线程 join子线程0, 等待0结束 子线程1结束 子线程0结束, 输出0:3 join子线程1, 发现已经结束了, 输出1:2 join子线程2, 等待2结束 子线程2结束, 输出2:4 假如有100个线程同时开始, 最长子线程时间为3秒, 按这样的循环, 会导致时间浪费吗? 答案是不会. 有关守护线程的机制, 参考这篇问答 ","date":"2017-09-26","objectID":"/2017-09-26-python-thread/:3:3","tags":["python"],"title":"Python核心编程笔记4 多线程","uri":"/2017-09-26-python-thread/"},{"categories":null,"content":"python有很好的OOP特性, 自由度也非常大 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:0:0","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"命名规则 小驼峰命名法(camel case), 适用于变量名: varFirst, verSecond 大驼峰命名法, 适用于类名: PersonFirst, PersonSecond 下划线命名法, 适用于函数/方法名: var_first, var_second ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:1:0","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"类属性(class attribute) ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:2:0","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"类的数据属性 相当于static修饰的变量 class C(object): foo = 100 \u003e\u003e\u003e print(C.foo) \u003e\u003e\u003e 100 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:2:1","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"方法与绑定: 静态方法和类方法 没有实例, 方法是不能被调用的. 但有两个特殊例子: 类方法(classmethod)和静态方法(staticmethod) 有关如何使用这两个方法, 参考stackoverflow上的这篇文章, 在这里总结一下: 类方法可以被子类继承, 静态方法不可以. 由于python不支持重载, 类方法的一大用处是作为工厂函数, 但静态方法就不可以, 因为如果是派生类, 静态方法会构造出基类, 而类方法可以构造出派生类. 类方法和实例方法一样, 需要将类作为第一个隐式参数传入方法: @classmethod def foo(cls, data): return cls(data) 此处类作为了第一个参数, 它由解释器传给方法, 类不需要特别地命名, 多数人使用cls作为变量名字 @classmethod means: when this method is called, we pass the class as the first argument instead of the instance of that class (as we normally do with methods). This means you can use the class and its properties inside that method rather than a particular instance. @staticmethod means: when this method is called, we don’t pass an instance of the class to it (as we normally do with methods). This means you can put a function inside a class but you can’t access the instance of that class (this is useful when your method does not use the instance). ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:2:2","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"特殊类属性 特殊属性 说明 __class__ (实例的)类 __name__ 类名 __doc__ 文档字符串 __bases__ 所有父类构成的元组 __dict__ 类的属性 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:2:3","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"实例(instance) ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:3:0","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"初始化 __init__方法用于初始化实例, 返回None __new__用于对内建类型进行派生 __del__是解构函数, 一般不要去手动实现 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:3:1","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"实例属性 vs 类属性 python能够在运行时直接创建实例属性, 但要小心会被类属性覆盖(Override) \u003e\u003e\u003e class Foo(object): ... x = 1.5 # 类属性 \u003e\u003e\u003e foo = Foo() # 创建实例 \u003e\u003e\u003e foo.x # 因为不存在实例属性, 此处调用类属性 1.5 \u003e\u003e\u003e foo.x = 1.7 # 在运行时直接创建实例属性 \u003e\u003e\u003e foo.x 1.7 \u003e\u003e\u003e Foo.x # 类属性没变 1.5 \u003e\u003e\u003e del foo.x # 删除实例属性 \u003e\u003e\u003e foo.x # 类属性重见天日 1.5 此外, 如果类属性是可变对象, 通过实例属性修改类属性是很危险的 \u003e\u003e\u003e class Foo(object): ... x = {2003: 'poe2'} # 类属性 \u003e\u003e\u003e foo = Foo() \u003e\u003e\u003e foo.x {2003: 'poe2'} \u003e\u003e\u003e foo.x[2004] = 'valid patch' # 添加一个key \u003e\u003e\u003e foo.x {2003: 'poe2', 2004: 'valid patch'} # 生效 \u003e\u003e\u003e Foo.x {2003: 'poe2', 2004: 'valid patch'} # 类属性也变了 \u003e\u003e\u003e del foo.x AttributeError: x # 没有覆盖所以无法删除 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:3:2","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"子类和派生 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:4:0","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"继承 派生类会继承基类的所有属性, 无论是数据属性还是方法属性. 如果出现了同名属性, 会发生覆盖: class P(object): def foo(self): print('foo from P') class C(P): def foo(self): print('foo from C') \u003e\u003e\u003e c = C() \u003e\u003e\u003e c.foo() # 调用派生类的foo foo from C 在类的继承中, 子类需要调用父类被覆盖的同名方法怎么办呢? 我们不希望复制父类中的代码, 可以用子类的实例启动父类的方法, 需要传入self参数: \u003e\u003e\u003e P.foo(c) # 通过传入派生类实例来调用基类的foo 对于__init__而言, 派生类方法会覆盖基类方法的构造函数, 因此需要在派生类中手动调用. class Inherited(Base): def __init__(self, inhData, bseData): Base.__init__(self, bseData) # 派生类调用基类方法 self. inhData = inhData 此时如果继承关系复杂, 还可以使用super()帮你处理好父类 class Inherited(Base): def __init__(self, inhData, bseData): super(Inherited, self).__init__(bseData) self. inhData = inhData ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:4:1","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"built-in类型的派生 待补充 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:4:2","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"多重继承的MRO问题 现在python的MRO(Method Resolution Order)搜索采用广度优先 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:4:3","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"类和实例的内建函数 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:5:0","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"基本方法 方法 说明 issubclass(sub, sup) sub是sup的子类:True isinstance(obj, cls) obj是cls或者其子类的一个实例 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:5:1","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"用魔法方法定制类 有关Magic Method的列表, 自行谷歌吧 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:5:2","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"私有化 python没有c++中的private关键字, 使用双下划线法’混淆’属性: class C(object): def __init__(self): self.__num = 0 # 解释器重命名为self._C__num 这么做的主要原因是保护变量不与父类的名空间冲突, 子类中的代码就可以安全使用 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:5:3","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"高级特性 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:6:0","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"__slot__类属性 __dict__跟踪了所有属性, inst.foo和inst.__dict__['foo']等价. 如果某一个类的实例太多, 那么__dict__会占据太多空间, 可以用__slot__代替. 其为一可迭代对象, 用户不再可以任意动态增加实例属性 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:6:1","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"描述符 待补充 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:6:2","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"元类和__metaclass__ 基于python中万物皆为类的概念, 元类也是一个类, 而它的实例是其他的类: class C(object): pass class CC: pass \u003e\u003e\u003e type(C) \u003ctype 'type'\u003e \u003e\u003e\u003e type(CC) \u003ctype 'classobj'\u003e \u003e\u003e\u003e import types \u003e\u003e\u003e type(CC) is types.ClassType # CC是元类ClassType True 在执行类定义的时候, 解释器必须要知道这个类的正确的元类, 首先寻找类属性的__metaclass__, 如果此属性存在, 就把这个属性赋值给此类作为它的元类. 如果没有定义, 会继续向上查找. 元类一般用于创建类. 元类初始化时通常传递三个参数: 类名, 从基类继承数据的元组和类的属性字典 以下是一个元类应用, 它在创建类时显示时间标签. from time import ctime class MetaC(type): def __init__(cls, name, bases, attrd): super(MetaC, cls).__init__(name, bases, attrd) print('%s created at %s' % (name, ctime())) class Foo(object): __metaclass__ = MetaC def __init__(self): print('instantiated class %s at %s' % (self.__class__.__name__, ctime())) \u003e\u003e\u003e f = Foo() 脚本输出 Foo created at Mon Sep 25 16:48:37 2017 instantiated class Foo at Mon Sep 25 16:48:37 2017 ","date":"2017-09-25","objectID":"/2017-09-25-python-oop/:6:3","tags":["python"],"title":"Python核心编程笔记3 面向对象编程","uri":"/2017-09-25-python-oop/"},{"categories":null,"content":"用了这么久numpy居然没有总结一下 import numpy as np ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:0:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"ndarray属性 # 初始化 a = np.array([[1, 2], [3, 4]]) # 第一维的维数 a.ndim 2 # 所有维的维数 a.shape (2, 2) # 数据类型 a.dtype dtype('int64') # 改变数据类型 a.astype(np.float64) array([[ 1., 2.], [ 3., 4.]]) ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:1:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"ndarray内置初始化方法 np.ones((2, 3)) array([[ 1., 1., 1.], [ 1., 1., 1.]]) np.zeros((2, 3)) array([[ 0., 0., 0.], [ 0., 0., 0.]]) np.arange(5) array([0, 1, 2, 3, 4]) ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:2:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"数组和标量的运算 # 相同尺寸的数组运算全部是element-wise的 a = np.ones((2, 3)) b = np.ones((2, 3)) / 2 a + b array([[ 1.5, 1.5, 1.5], [ 1.5, 1.5, 1.5]]) # 数组和标量的运算全是broadcasting a * 3 array([[ 3., 3., 3.], [ 3., 3., 3.]]) ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:3:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"索引和切片 a = np.array([[1, 2, 3], # 0 [4, 5, 6], # 1 [7, 8, 9], # 2 [10, 11, 12]]) # 3 #0 #1 #2 # 两种索引方式等价 a[1, 2] == a[1][2] == 6 True # 花式索引: 传入一个数组 a[[0, 2]] array([[1, 2, 3], [7, 8, 9]]) # 花式索引: 传入两个数组, 等价于取出了[1, 1], [3, 2], [2, 1] a[[1, 3, 2], [1, 2, 1]] array([ 5, 12, 8]) # 切片 a[1:2] array([[4, 5, 6]]) # 多维切片. 注: 切片都是引用, 不是拷贝 a[1:3, 1:3] array([[5, 6], [8, 9]]) ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:4:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"布尔索引 # masking, mask条件可以使用 |, \u0026 等合并 a = np.array([1,2,3,1,2,3,1,2]) mask = (a == 1) | (a == 2) mask array([ True, True, False, True, True, False, True, True], dtype=bool) # mask的长度必须和数组的(第一个)维度一致 a[mask] array([1, 2, 1, 2, 1, 2]) ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:5:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"element-wise 函数 这里只列举一些常用的. ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:6:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"一元函数 函数 简介 sqrt 平方根 exp 指数 log, log1p log(x), log(x+1) sign 符号 isinf 无穷大的mask ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:6:1","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"二元函数 函数 简介 add(A, B) 求和 subtract(A, B) element-wise的A减去B multiply(A, B) element-wise乘法 power(A, B) element-wise的A的B次方 isinf 无穷大的mask greater(A, B) A\u003eB的element-wise的mask ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:6:2","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"where指令 x = np.array([0, 1, 2, 3, 4]) y = np.array([5, 6, 7, 8, 9]) flag = np.array([True, False, True, False, True]) np.where(flag, x, y) array([0, 6, 2, 8, 4]) flag是一个mask数组, 值为True的赋予x中的值, 值为False的赋予y中的值 x和y可以是两个数, 也可以是两个数组, 其大小甚至可以不相等 ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:7:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"统计学函数 只介绍一下sum, 其他函数的用法类似. 其他常用函数还有mean, (arg)min, (arg)max, var, std等, 见中文版P104 a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) # 在行的维度上求和 a.sum(axis=0) array([22, 26, 30]) # 在列的维度上求和. 注意此时变成了形状为(1, 4)的数组 a.sum(axis=1) array([ 6, 15, 24, 33]) # 在列的维度上求和时, 同时保持原数组的形状(4, 1) a.sum(axis=1, keepdims=True) array([[ 6], [15], [24], [33]]) ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:8:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"线性代数库 懒得介绍了, 翻书吧, 中文版P109 ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:9:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"随机数 所有函数都在np.random模块下. 函数 说明 shuffle(a) in-place随机打乱序列 rand(d1, d2..dn) 按(d1,d2..dn)产生(0,1)均匀分布 randint(low, high, size=(d1, d2..dn)) 从(floor, ceil)内随机取整数, 产生(d1,d2..dn)分布 ","date":"2017-09-21","objectID":"/2017-09-21-numpy/:10:0","tags":["numpy","python-data-analysis"],"title":"Python数据分析笔记1 numpy","uri":"/2017-09-21-numpy/"},{"categories":null,"content":"自己写的老物了, 整理一下发出来, 可能会修改 ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:0:0","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"1. 任意损失函数的Boosting 损失函数的一般表示是: $$ L(y_i, f(x_i)) $$ 考虑使用前向分步算法求解一个任意损失函数: $$ (\\beta_m, \\gamma_m) = arg\\min\\limits_{\\beta, \\gamma} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i;\\gamma)) \\tag{4.1}$$ 既然 $\\beta b(x_i;\\gamma)$ 和 $f_{m-1}(x_i)$ 相比是等价无穷小量, 使用 泰勒级数 在 $f_{m-1}(x_i)$ 附近展开: $$ L \\approx \\frac{1}{N} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i)) + \\beta \\sum\\limits_{i=1}^N \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} b(x_i;\\gamma) \\tag{4.2}$$ 为 (2) 添加正则化项防止 $b(x_i;\\gamma)$ 变得太大, 既然我们已经有了 $\\beta$ 去调整这个项的大小了: $$\\begin{aligned} L \u0026 \\approx \\frac{1}{N} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i)) + \\frac{\\beta}{2} \\sum\\limits_{i=1}^N \\left. { 2 \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} b(x_i;\\gamma) + b^2(x_i;\\gamma) \\\\ \\text{(Strip the constants)} \u0026 = \\beta \\sum\\limits_{i=1}^N 2 \\frac{\\partial L}{\\partial s} b(x_i;\\gamma) + b^2(x_i;\\gamma) \\\\ \u0026 = \\beta \\sum\\limits_{i=1}^N (b(x_i;\\gamma) + \\frac{\\partial L}{\\partial s})^2 - (\\frac{\\partial L}{\\partial s} )^2 \\end{aligned}\\tag{4.3}$$ 参考了林轩田的机器学习技法MOOC ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:0:1","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"2. Gradient Boosting 现在可以最小化损失函数: 求解 $b(x_i;\\gamma)$. 从 (3) 可知: $$\\gamma_m = arg\\min\\limits_\\gamma \\beta \\sum\\limits_{i=1}^N \\left(b(x_i;\\gamma) + \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} \\right)^2$$ 也就是在每一步 $m$ 中, 利用损失函数的梯度 $-\\frac{\\partial L(y_i, s)}{\\partial s}$ 训练基分类器 $b(x_i;\\gamma_m)$. 这就是为什么它被称为梯度提升算法 $$ \\text{fit } b(x_i;\\gamma_m) = - \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)}$$ 2. 求解 $\\beta$ $$\\beta_m = arg\\min\\limits_\\beta \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i;\\gamma_m))$$ 既然我们已经有了 $y_i$, $f_{m-1}(x_i)$ 和 $b(x_i;\\gamma_m)$, 那原问题就变成了一个简单的一维变量最优化问题, 那就很容易解决了整个算法的思想很简单, 最常用的基分类器是决策树, 称为gradient boosting decision tree (GBDT) ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:0:2","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"3. GBDT回归算法 基分类器是CART回归树 输入: 训练集${\\displaystyle {(x_{i},y_{i})}_{i=1}^{n},}$可导损失函数${\\displaystyle L(y,F(x)),}$基分类器个数/迭代个数$M$ 输出: 集成回归器$ F_M(x) $ 初始化 $$ F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n L(y_i, \\gamma) $$ 对m个基分类器/回归器 1. 计算损失函数对每个样本的一阶导数近似: $$ r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad {i=1, \\ldots, n}$$ 使用${(x_i, r_{im})}_{i=1}^n$训练下一个回归树 $${\\displaystyle h_{m}(x)=\\sum _{j=1}^{J_{m}}c_{jm}I(x\\in R_{jm}),}$$ 用一维线性搜索最优化基分类器在每个区间的输出$$ c_{mj} = \\underbrace{arg; min}_{c}\\sum\\limits_{x_i \\in R_{tj}} L(y_i,f_{m-1}(x_i) +c) $$ 其中$R_{tj}, j =1,2,…, J$, $J$为叶子节点的数量 更新强学习器 $$ F_{M}(x) = f_{m-1}(x) + \\sum\\limits_{j=1}^{J}c_{mj}I(x \\in R_{tj}) $$ ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:0:3","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"4. GBDT分类算法 二分类算法 定义预测模型 $$ p(x)=\\frac{e^{F(x)}}{e^{F(x)}+e^{-F(x)}} $$ $$ F(x)=\\frac{1}{2}log(\\frac{p(x)}{1-p(x)}) $$ 预测时将$F(x)$转换成$p(x)$, 然后比较大小. 在此之上定义对数损失函数(MLE损失) 多分类算法 类似于Softmax $$ p_k(x) = \\frac{\\exp(f_k(x))} { \\sum\\limits_{l=1}^{K} exp(f_l(x))} $$ $$ F_k(x) = log(p_k(x)) - \\frac{1}{K} \\sum\\limits_{i=1}^K log(p_i(x))$$ 损失函数为: $$ L(y, f(x)) = - \\sum\\limits_{k=1}^{K}y_klog;p_k(x) $$ 集合上两式，我们可以计算出第t轮的第i个样本对应类别k的负梯度误差为: $$ r_{tik} = -\\bigg[\\frac{\\partial L(y, f(x_i)))}{\\partial f(x_i)}\\bigg]_{f_k(x) = f_{k, t-1};; (x)} = y_{il} - p_{k, t-1}(x_i) $$ 过程: 初始化先验分布为均匀分布$ F_k(x) = 1/K $ 对 $t = 1, 2, \\cdots, T$ a. 对每个label, 计算$p_k(x)$ b. 对每个label, 计算损失函数$f_{l, t}$在每个样本$i$处的一阶导数 c. 分别拟合一阶导数, 更新$F_k(x)$ ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:0:4","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"5. 正则化 Shrinkage $$ f_{k}(x) = f_{k-1}(x) + \\nu h_k(x) $$采用步长$0\u003c \\upsilon \u003c1$降低每个模型的贡献, 往往伴随着$M$的增加, 即缩减每个基分类器的贡献, 同时使用更多的基分类器, 降低过拟合 Stochastic gradient boosting 类似于Bagging的手段, 每棵树生成的时候都使用一部分训练数据 Number of observations in leaves 在停止条件里加入叶节点的样本数量, 小于等于就停止 Penalize Complexity of Tree ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:0:5","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"6. 一些其他内容 DART(Dropout Addictive Regression Tree): 为了解决Boosting过程中, 首先产生的树拥有更大的贡献, 采取类似于Dropout的方法: 每次新加一棵树，这棵树要拟合的并不是之前全部树ensemble后的残差，而是随机抽取的一些树ensemble；同时新加的树结果要规范化一下. ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:0:6","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"XGBoost 两篇文章已经介绍得很好了: 关于特征选择: GBDT算法原理深入解析 关于特征重要度: Tree ensemble算法的特征重要度计算 一句话, 在子树中, 某一个特征的重要度就是所有使用该特征分裂的节点的损失的减少值, 亦即:$$ Gain= - \\frac12 \\left[ \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda} - \\frac{G_L^2}{H_L+\\lambda} - \\frac{G_R^2}{H_R+\\lambda} \\right] - \\gamma $$对某个特征求gain之和, 再对所有基分类器求算术平均值即可得到特征重要度. 梯度提升树(GBDT)原理小结 ","date":"2017-09-04","objectID":"/2017-09-04-gradient-boosting/:1:0","tags":["gbdt"],"title":"梯度提升(Gradient Boosting)","uri":"/2017-09-04-gradient-boosting/"},{"categories":null,"content":"不学好函数式编程, 总觉得人生有遗憾 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:0:0","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"什么是函数 python函数可以返回一个值或对象, 但返回一个容器对象的时候看起来像是返回了多个值: return 'abc', 'def'实际上是返回了一个有两个元素的元组而已 函数有标准调用和关键字调用两种形式. 有关可变长度参数调用参考后文 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:1:0","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"创建函数 def foo(): print('in foo()') bar() def bar(): print('in bar()') \u003e\u003e\u003e foo() \u003e\u003e\u003e in foo() \u003e\u003e\u003e in bar() python和c++不太一样的地方: 不存在前向引用的问题. 在foo()中对bar()进行的调用出现在bar()之前, 但foo()本身不是在bar()声明之前被调用的, 在调用foo()时bar()已经存在了, 因此调用成功 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:2:0","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"函数属性(attribute) 函数也拥有自己的名空间, 使用函数名+.标识 def foo(): 'foo -- property created doc string' def bar(): pass bar.__doc__ = 'Oops, forgot the doc string' bar.version = 0.1 # 利用名空间赋值 \u003e\u003e\u003e print(bar.version) 0.1 内嵌函数 def foo(): def bar(): print 'bar() called' print 'foo() called' bar() \u003e\u003e\u003e foo() foo() called # 调用foo输出的 bar() called # foo中调用bar输出的 \u003e\u003e\u003e bar() # 直接调用bar出错 ... NameError: name 'bar' is not defined 注意内嵌函数的整个函数体都在外部函数的名空间内, 因此如果没有任何对bar()的外部引用, 那么除了函数体内, 任何地方都不能对其进行调用 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:3:0","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"装饰器(decorator) ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:4:0","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"装饰器的产生背景 2.2版本时类方法被引入python, 实现很笨拙: class MyClass(object): def staticFoo(): # 静态方法省略了self关键字 pass # 使用内建staticmethod将其转化为静态方法 staticFoo = staticmethod(staticFoo) 使用装饰器之后, 可以用以下语法替换掉上面的 class MyClass(object): @staticmethod def staticFoo(): pass 甚至还可以将装饰器和函数调用一样堆叠起来: @deco2 @deco1 def func(): pass # 等价于 func = deco2(deco1(func)) ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:4:1","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"带参数的装饰器 @deco1(deco_arg) @deco2 def func(): pass # 等价于 func = deco1(deco_arg)(deco2(func)) ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:4:2","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"装饰器举例 装饰器及闭包接受一个函数, 返回一个修改后的函数对象, 将其重新赋予原来的标识符, 并永久失去对原始函数的访问. 以下例子对函数增加了输出调用时间的功能 from time import ctime def time_func(func): def wrapper(*arg, **kw): # 不知道func的参数 print('[%s] %s called' %\\ (ctime(), func.__name__)) return func(*arg, **kw) # 传入相应参数并调用func return wrapper @time_func def foo(): print(\"it's foo() running\") \u003e\u003e\u003e foo() [Fri Aug 18 16:49:11 2017] foo called it's foo() running 以下例子增加了传入参数并输出的功能 from time import ctime, sleep def time_log(text): def time_func(func): def wrapper(*arg, **kw): # 不知道func的参数 print('[%s] %s %s' % (ctime(), func.__name__, text)) return func(*arg, **kw) # 传入相应参数并调用func return wrapper return time_func @time_log('called') def foo(): print(\"it's foo() running\") \u003e\u003e\u003e foo() [Fri Aug 18 17:05:08 2017] foo called it's foo() running 这里体现了装饰器的一个好处: 装饰器内的函数在装饰器的名空间内, 因此可以直接调用传入的text而不需要写在参数列表里 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:4:3","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"可变长度参数 函数接受参数的顺序: 必须参数 默认参数 可变长度参数: 用*args表示, 当然名字任意, 但星号必须 关键字变量参数(字典): 用**kw表示, 规则同上 一个小例子: def foo(arg1, arg2=0.0, *args, **kw): 'display regular args and all variable args' print(arg1, arg2) for _ in args: print('additional non-keyword arg: ', _) for _ in kw: print('additional non-keyword arg: %s: %s' % \\ (_, kw[_])) \u003e\u003e\u003e foo('arg1', 1.0, 2.0, 3.0, kw1=4, kw2=5, kw3=6) ('arg1', 1.0) ('additional non-keyword arg: ', 2.0) ('additional non-keyword arg: ', 3.0) additional non-keyword arg: kw1: 4 additional non-keyword arg: kw3: 6 additional non-keyword arg: kw2: 5 注意字典是无序的 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:5:0","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"函数式编程(functional programming) ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:6:0","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"lambda表达式 def add(x, y=2): return x + y # 等价于 lambda x, y=2: x+y ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:6:1","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"内建函数 # 将func作用于每个seq元素, 返回所有令func返回True的元素 filter(func, seq) # 将func作用于每个seq元素ele, 返回func(ele)组成的列表 map(func, seq) # 将含有两个参数的函数func作用于seq的前两个元素, 然后将返回值和seq的下一个元素配对, 再赋予func, 以此往复 reduce(func, seq) 例子就懒得举了…自行搜索 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:6:2","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"偏函数 偏函数(partial function) 将任意数量(顺序)参数的函数转化成另一个带剩余参数的函数对象. 下面是一个带关键字参数的FPA: from functools import partial baseTwo = partial(int, base=2) # 这里int为工厂函数int() baseTwo('10010') # 等价于int('10010', base=2) \u003e\u003e\u003e 18 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:6:3","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"变量作用域/闭包 参考stackoverflow上的文章 A closure occurs when a function has access to a local variable from an enclosing scope that has finished its execution. 一个闭包的例子: def make_counter(): i = 0 def counter(): # counter() is a closure nonlocal i i += 1 return i return counter \u003e\u003e\u003e c1 = make_counter() \u003e\u003e\u003e c2 = make_counter() \u003e\u003e\u003e print (c1(), c1(), c2(), c2()) \u003e\u003e\u003e 1 2 1 2 在调用make_counter()之后, 一个函数栈帧被创造, 其中counter()和i都在函数的名空间之内. 接着函数返回counter(). 然而因为counter()refer了i变量, 因而在make_counter()返回之后, i依然保持, 没有随着函数栈帧的返回而被销毁. 这种携带了整个enclosing函数的名空间的函数就是一个闭包. 闭包的后期绑定(late binding): def foo(): return [lambda x: i*x for i in range(4)] \u003e\u003e\u003e for func in foo(): \u003e\u003e\u003e ... print(func(2)) 6 6 6 6 为何返回的不是0 2 4 6而是6 6 6 6? 原因是定义一个函数，函数内的变量并不是立刻就把值绑定了，而是等调用的时候再查找这个变量. 此谓后期绑定. 因为调用func(2)的时候lambda表达式的名空间里只有x而没有i, 在foo()返回之后i的值是3, 因此全部返回4. 根据闭包的性质, 如果lambda表达式内部的名空间拥有i, 则可以返回0 2 4 6: def foo(): return [lambda x, i=i: i*x for i in range(4)] \u003e\u003e\u003e for func in foo(): \u003e\u003e\u003e ... print(func(2)) 0 2 4 6 此时在创建lambda匿名函数的时候就要获取默认参数的值，放到 lambda 的名空间中 参考文章 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:6:4","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"生成器(Generator) 从语法上讲, 生成器是一个带yield语句的函数, 当生成器的next()或__next__()(python2或3)被调用时, 他会准确地从离开的地方继续. 以下为生成器的几个主要方法: generator.__next__() Starts the execution of a generator function or resumes it at the last executed yield expression. When a generator function is resumed with a next() method, the current yield expression always evaluates to None. The execution then continues to the next yield expression, where the generator is suspended again, and the value of the expression_list is returned to next()’s caller. If the generator exits without yielding another value, a StopIteration exception is raised. This method is normally called implicitly, e.g. by a for loop, or by the built-in next() function. generator.send(value) Resumes the execution and “sends” a value into the generator function. The value argument becomes the result of the current yield expression. The send() method returns the next value yielded by the generator, or raises StopIteration if the generator exits without yielding another value. When send() is called to start the generator, it must be called with None as the argument, because there is no yield expression that could receive the value. 对于生成器, 需要注意以下几点: 除了一开始, 对generator调用next()时, 当前yield语句返回None 接着运行直到遇见下一次yield, 将该表达式的值作为函数返回值 send()改变的是恢复之后的yield语句的返回值, 返回的是generator生成的下一个值 两个例子比较: def counter(start=0): count = start while True: val = (yield count) if val is not None: count = val else: count += 1 \u003e\u003e\u003e count = counter(5) \u003e\u003e\u003e count.next() 5 \u003e\u003e\u003e count.next() 6 \u003e\u003e\u003e count.send(9) 9 \u003e\u003e\u003e count.next() 10 看一看调用count.send(9)之后发生了什么: val返回了9, if语句成立, count = 9 循环继续, 遇到yield并返回9 def adder(): x = 1 while True: y = (yield x) x += y \u003e\u003e\u003e g = adder() \u003e\u003e\u003e g.next() 1 \u003e\u003e\u003e g.send(3) 4 \u003e\u003e\u003e g.send(10) 14 为什么此处返回4呢? 看一看发生了什么: 调用next()之后, 函数返回1, 此时x = 1 调用send(3)之后, yield语句返回3, 即y = 3 x += 3之后为4, 接下来返回4, 注意此时x = 4 调用send(10)之后, yield语句返回10, 即y = 10 x += 10之后为14, 最后返回14 此处，生成器函数 adder 用 yield 表达式，将处理好的 x 发送给生成器的调用者；与此同时，生成器的调用者通过 send 函数，将外部信息作为生成器函数内部的 yield 表达式的值，保存在 y 当中，并参与后续的处理。 这一特性是使用 yield 在 Python 中使用协程的基础。 参考文章: 官方文档 Python 中的黑暗角落（一）：理解 yield 关键字 ","date":"2017-08-20","objectID":"/2017-08-20-python-functional-programming/:6:5","tags":["python"],"title":"Python核心编程笔记2 函数式编程","uri":"/2017-08-20-python-functional-programming/"},{"categories":null,"content":"这本书翻译稀烂, 然而我还是忍着看完了第一部分 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:0:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"Python基础 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:1:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"基本风格指南 跨行代码可以使用反斜杠\\ 变量赋值通过引用传递 python不支持++, --, 请使用+=和-= 多元赋值: 实际上是元组赋值: a, b = 1, 2等价于(a, b) = (1, 2). 在python中, 没有括号的分组默认是元组, 而不是列表 下划线标识符: _xxx: 此类属性不会被from module import *导入. 另外python不提倡使用此语句导入包, 容易造成名空间混乱 __xxx__: 类的特殊方法 _xxx: 类的私有变量名, 在子类中建议使用, 可以避免和父类冲突 __name__: 如果module被导入, 变量值为module名, 如果被执行, 为__main__ ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:1:1","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"内存管理 python的垃圾收集器实际上是引用计数器+循环辣鸡收集器 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:1:2","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"Python对象 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:2:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"Built-in 类型 内建类型略 切片对象的复制问题 对象值的比较可以用id(), 类似于c中的取地址操作\u0026, a is b可以比较a与b的地址 python不支持函数或者方法重载 类型工厂函数/内建函数: 看上去是函数, 实际上是生成了该类型的一个实例: int, float, complex, str, list, turple, dict, type, bool, set, super等 可变类型: 列表和字典. 不可变类型: 数字, 字符串, 元组 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:2:1","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"数字 数字的隐式类型转换和c++相似 几个取整的常用工厂函数: round(3.141, 2) = 3.15: 按四舍五入取整 int(3.14) = 3: 直接截取整数部分, 返回int floor(-0.2) = -1.0: 取最接近但小于原数的整数, 返回float ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:2:2","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"序列 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:3:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"字符串 切片索引的一个小技巧: 用None作为索引值, 可以使用一个变量作为索引从第一个遍历到最后一个, 即s[:None] = s s = 'abcde' for i in [None] + range(-1, -len(s), -1): print s[:i] # 输出 abcde abcd abc ab a 字符串比较是按ASCII值比较的, 因此大写字母一定比小写字母小. 即cmp('Z', 'a' ) == -1 string库的常用方法和属性: 替换'abcde'.replace('abc', 'xyz') = 'xyzde' 分割'a/bxxx/c'.split('/') = ['a', 'bxxx', 'c'] 删除头尾空格' abc '.strip() = 'abc' ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:3:1","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"列表 删除元素时, 知道元素值用del(element), 不知道元素只知道序号用remove(index) 列表的sort()和reversed()都是inplace操作, 返回None, 而sorted()和reversed()是工厂函数, 可以作为值返回 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:3:2","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"元组 单元素元组的坑: 只有一个元素的元组需要加逗号, 否则容易和工厂函数混淆: \u003e\u003e\u003e tuple('abc') ('a', 'b', 'c') \u003e\u003e\u003e ('abc', ) ('abc',) 所有多对象, 用逗号分隔的, 没有明确符号定义的集合默认为元组 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:3:3","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"浅拷贝, 深拷贝和可变/不可变对象的例子 \u003e\u003e\u003e person = ['name', ['savings', 100.00]] \u003e\u003e\u003e hubby = person[:] # 切片复制 \u003e\u003e\u003e wifey = list(person) # 工厂函数复制 \u003e\u003e\u003e [id(x) for x in person, hubby, wifey] # 查看地址 [1, 2, 3] # 为了方便使用了虚构的地址, 可见三个变量不同 \u003e\u003e\u003e hubby[0] = 'joe' \u003e\u003e\u003e wifey[0] = 'jane' \u003e\u003e\u003e hubby, wifey (['joe', ['savings', 100.0]], ['jane', ['savings', 100.0]]) \u003e\u003e\u003e hubby[1][1] = 50 \u003e\u003e\u003e hubby, wifey (['joe', ['savings', 50.0]], ['jane', ['savings', 50.0]]) hubby, wifey这两个对象是新的, 但他们的内容不一定 序列类型对象的默认拷贝是浅拷贝, 可以通过切片, 工厂函数, **copy()**实现 不可变对象(例子中是字符串)的拷贝, 实际上是删除变量引用, 创建新的对象, 然后转移引用到新变量 可变对象(例子中是列表)的拷贝只是复制了引用 想要深拷贝请使用copy.deepcopy() 对原子元素组成的元组进行深拷贝无效 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:3:4","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"映像和集合类型 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:4:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"字典 只需要字典名就可以遍历字典key, 但key是无序的 检查key是否存在使用in和not in, has_key()即将被弃用 key必须是可hash的, 也就是不可变对象. 可以使用hash()检验 字典的常用方法有 iteritems()返回一个迭代器, 而不是数组 values()返回一个包含字典所有值的列表 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:4:1","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"集合 无序, 只能用工厂函数构造 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:4:2","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"条件和循环 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:5:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"循环语句 三元组表达式: A if EXP else B, 等价于exp? A:B else可以放在循环之后, 只要循环是正常结束的, 会执行else中的语句 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:5:1","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"迭代器 定义: Iterator类是实现了__iter__()和next()的类.例子出处 class Yes(collections.Iterator): def __init__(self, stop): self.x = 0 self.stop = stop def __iter__(self): return self def next(self): if self.x \u003c self.stop: self.x += 1 return 'yes' else: # Iterators must raise when done, else considered broken raise StopIteration __next__ = next # Python 3 compatibility 在for循环中迭代该类, 会依次调用迭代器的next()方法, 直到StopIteration抛出, for循环会捕捉处理. 2. 将其他序列转化成迭代器可以直接使用iter()工厂函数. 如果传递两个参数给iter(), 他会重复调用func, 直到迭代器的下个值等于sentinel ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:5:2","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"列表解析式 只说一点, 列表解析式可以套双层循环: \u003e\u003e\u003e [(x+1, y+1) for x in range(3) for y in range(5) if x != y] \u003e\u003e\u003e [(1, 2), (1, 3), (1, 4), (1, 5), (2, 1), (2, 3), (2, 4), (2, 5), (3, 1), (3, 2), (3, 4), (3, 5)] scala也有类似的语法, 也许是从函数式语言中引进的 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:5:3","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"生成器表达式 和生成器类似, 有关生成器的内容见后文, 这里只给出和列表解析式的区别: # 列表解析式 [expr for iter_var in iterable if cond_expr] # 生成器表达式 (expr for iter_var in iterable if cond_expr) 可见只是将方括号换成了圆括号. 对生成器表达式迭代会引发和生成器类似的效果, 即延迟计算(lazy evaluation), 也可以理解为懒惰的列表解析, 是对内存更友好的结构. ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:5:4","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"文件和输入输出 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:6:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"内建函数open()和file() 常见访问模式 r: 只读 w: 写入,若文件存在则首先清空 a: 写入, 若文件存在则追加 w+/a+: 读写模式,参考前两个模式 ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:6:1","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"内建方法 readline(): 读取打开文件的第一行, 不会删除行结束符 readlines(): 读取所有行并把他们作为一个字符串列表返回 write(): 写入, 但不会自动加入换行符 writelines(): 将列表写入, 但不会自动加入换行符 文件对象可以直接作为文件迭代器: f = open(`abc.txt`, 'r') for eachLine in f: print eachLine f.close() ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:6:2","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"错误和异常 有些行为, 比如用户退出或者系统退出并不应该被捕获, 需要提前处理: try: pass except (KeyboardInterrupt, SystemExit): # 系统或者用户退出 raise except Exception: # 处理真正的错误 2.5版本之后, KeyboardInterrupt和SystemExit被从Exception里移出和Exception平级, 因此不必单独处理这两种情况: try: pass except Exception, e: # 处理错误 最后, Exception的父类是BaseException ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:7:0","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"上下文管理 with open('abc.txt', 'r') as f: pass ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:7:1","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"断言 一个用try语句捕捉断言产生的错误的例子: try: assert 1 == 0, \"That's reall silly\" except AssertionError, args: print ('%s: %s' % args.__class__.__name__, args) ","date":"2017-08-18","objectID":"/2017-08-18-core-python-programming-1/:7:2","tags":["python"],"title":"Python核心编程笔记1 基础知识","uri":"/2017-08-18-core-python-programming-1/"},{"categories":null,"content":"泊松分布是随机过程中的一个重要分布 ","date":"2017-08-08","objectID":"/2017-08-08-poisson/:0:0","tags":["stat"],"title":"Poisson Distribution Summary","uri":"/2017-08-08-poisson/"},{"categories":null,"content":"Poisson分布的直观解释 定义 $$ {\\displaystyle P(k{\\text{ events in interval}})=e^{-\\lambda }{\\frac {\\lambda ^{k}}{k!}}} $$ that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event. 以上是wiki的定义: 如果已知事件的发生的平均频率$\\lambda$, 而且事件之间条件独立, 那么在固定的时／空段内发生给定次数(k)事件的概率符合Poisson分布. 继续拿wiki举例: 已知一个人平均每天收到4封邮件, 如果邮件之间条件独立, 那么每天收到的邮件数目符合Poisson分布. 而这个概率就是Poisson分布的唯一参数$\\lambda$. 把前面的例子扩展一下, 每两天收到的邮件数目的概率符合$\\lambda = 8$的Poisson分布 ","date":"2017-08-08","objectID":"/2017-08-08-poisson/:1:0","tags":["stat"],"title":"Poisson Distribution Summary","uri":"/2017-08-08-poisson/"},{"categories":null,"content":"Poisson分布和其他分布的关系 ","date":"2017-08-08","objectID":"/2017-08-08-poisson/:2:0","tags":["stat"],"title":"Poisson Distribution Summary","uri":"/2017-08-08-poisson/"},{"categories":null,"content":"与二项分布的关系 The binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while the product np remains fixed or at least p tends to zero. Therefore, the Poisson distribution with parameter λ = np can be used as an approximation to B(n, p) of the binomial distribution if n is sufficiently large and p is sufficiently small. According to two rules of thumb, this approximation is good if n ≥ 20 and p ≤ 0.05, or if n ≥ 100 and np ≤ 10. 对一个二项分布, 在p较小, n较大的情况下, 可以作如下等价: 每次Bernoulli实验都相当于在一定时间范围内发生1次事件的概率是p, 那么重复n次之后, 使用Poisson分布来定义的话就是: 在n时间范围内, 平均发生$np$次的事件, 发生给定次数的概率分布, 符合$\\lambda=np$的Poisson分布. ","date":"2017-08-08","objectID":"/2017-08-08-poisson/:2:1","tags":["stat"],"title":"Poisson Distribution Summary","uri":"/2017-08-08-poisson/"},{"categories":null,"content":"与指数分布的关系 If for every t \u003e 0 the number of arrivals in the time interval [0, t] follows the Poisson distribution with mean λt, then the sequence of inter-arrival times are independent and identically distributed exponential random variables having mean 1/λ. 泊松过程中，第k次随机事件与第k+1次随机事件出现的时间间隔服从参数为$\\frac{1}{\\lambda}$的指数分布. Proof: 设两次随机事件之间间隔的时间为$X$, 第k次随机事件之后长度为t的时间段内，第k+1次随机事件出现的概率等于1减去这个时间段内没有随机事件出现的概率, 而这个概率即为$N \\sim Poi(t\\lambda)$中的$P(N = 0)$ $$ P(X\u003et) = 1 - \\frac{(\\lambda t)^0}{0!} e^{-\\lambda t} = 1-e^{-\\lambda t}$$ 即指数分布的PDF 待续 ","date":"2017-08-08","objectID":"/2017-08-08-poisson/:2:2","tags":["stat"],"title":"Poisson Distribution Summary","uri":"/2017-08-08-poisson/"},{"categories":null,"content":"好像是谷歌面试题…中比较弱智的 输入: 一个m*n矩阵ary, 矩阵元素只有0和1 输出: 寻找从左上到右下的连续路径中最长的路径长度(也就是只能往右或往下或往右下走) 定义: 只要1的周围8个点有1就定义为连续路径 状态矩阵定义: $dp[i][j]$代表在$ary[:i][:j]$子矩阵中, 经过点$arr[i][j]$的路径的最大值 状态转移方程: $$ dp[i][j] = \\begin{cases} max \\lbrace \u0026 dp[i-1][j], \\\\ \u0026 dp[i][j-1], \\\\ \u0026 dp[i-1][j-1] \\rbrace + 1, \u0026 ary[i][j] = 1 \\\\ 0, \u0026\u0026 ary[i][j] = 0 \\end{cases}$$ 首先计算出dp第一行第一列的值, 再从左向右从上到下计算整个dp数组, 最后统计最大值即可 {% raw %} #include \u003ciostream\u003e #include \u003cvector\u003e using namespace std; class Solution { public: int dp_sol(vector\u003cvector\u003cint\u003e \u003e \u0026 ary) { int max_route = 0; auto m = ary.size(); auto n = ary[0].size(); vector\u003cvector\u003cint\u003e\u003e dp(m, vector\u003cint\u003e(n, 0)); // init 1st row for (int i = 0; i \u003c n; ++i) { if (ary[0][i] == 1) { if ((i != 0) \u0026\u0026 (ary[0][i-1] == 1)) { dp[0][i] = dp[0][i-1] + 1; } else {dp[0][i] = 1;} } } // init 1st col for (int i = 0; i \u003c m; ++i) { if (ary[i][0] == 1) { if ((i != 0) \u0026\u0026 (ary[i-1][0] == 1)) { dp[i][0] = dp[i-1][0] + 1; } else {dp[i][0] = 1;} } } // dp for (int i = 1; i \u003c m; ++i) { for (int j = 1; j \u003c n; ++j) { if (ary[i][j] == 1) { dp[i][j] = max(max(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1]) + 1; } } } // find max for (int i = 0; i \u003c m; ++i) { for (int j = 0; j \u003c n; ++j) { if (dp[i][j] \u003e max_route) {max_route = dp[i][j];} } } return max_route; } }; int main() { vector\u003c vector\u003cint\u003e \u003e a = {{0, 1, 1, 0}, {1, 0, 0, 0}, {1, 0, 1, 0}, {1, 0, 0, 1}}; Solution s = Solution(); cout\u003c\u003cs.dp_sol(a)\u003c\u003cendl; return 0; } {% endraw %} 拓展: 如果允许往任意方向走, 那就成了一个搜索最大的连通分量的题 ","date":"2017-08-07","objectID":"/playground/2017-08-07-dp1/:0:0","tags":["面试题"],"title":"某动态规划小题","uri":"/playground/2017-08-07-dp1/"},{"categories":null,"content":" 包括书和paper和感兴趣的topic ","date":"2017-08-03","objectID":"/2017-08-03-redinglist/:0:0","tags":null,"title":"18年秋季阅读计划","uri":"/2017-08-03-redinglist/"},{"categories":null,"content":"推荐系统实战 好的推荐系统 利用用户行为数据 推荐系统冷启动问题 利用用户标签数据 利用上下文信息 利用社交网络数据 推荐系统实例 评分预测问题 (已读完, 总结中) ","date":"2017-08-03","objectID":"/2017-08-03-redinglist/:0:1","tags":null,"title":"18年秋季阅读计划","uri":"/2017-08-03-redinglist/"},{"categories":null,"content":"贝叶斯方法: 概率编程与贝叶斯推断 贝叶斯推断的哲学 (8/9) 进一步了解PyMC (8/10-8/12) 打开MCMC的黑盒子 从未言明的最伟大定理 失去一只手臂还是一条腿 弄清楚先验 贝叶斯A/B测试 ","date":"2017-08-03","objectID":"/2017-08-03-redinglist/:0:2","tags":null,"title":"18年秋季阅读计划","uri":"/2017-08-03-redinglist/"},{"categories":null,"content":"Python核心编程 (8/14-8/18读完第一部分) 欢迎来到Python世界 快速入门 Python基础 Python对象 数字 序列：字符串、列表和元组 映像和集合类型 条件和循环 文件和输入输出 错误和异常 函数和函数式编程 模块 面向对象编程 执行环境 正则表达式 网络编程 网络客户端编程 多线程编程 图形用户界面编程 Web编程 数据库编程 扩展Python 其他话题 ","date":"2017-08-03","objectID":"/2017-08-03-redinglist/:0:3","tags":null,"title":"18年秋季阅读计划","uri":"/2017-08-03-redinglist/"},{"categories":null,"content":"利用Python进行数据分析 准备工作 引言 IPython：一种交互式计算和开发环境 NumPy基础：数组和矢量计算 pandas入门 数据加载、存储与文件格式 数据规整化：清理、转换、合并、重塑 绘图和可视化 数据聚合与分组运算 时间序列 金融和经济数据应用 NumPy高级应用 ","date":"2017-08-03","objectID":"/2017-08-03-redinglist/:0:4","tags":null,"title":"18年秋季阅读计划","uri":"/2017-08-03-redinglist/"},{"categories":null,"content":"深度学习 引言 线性代数 概率与信息论 数值计算 机器学习基础 深度前馈网络 深度学习中的正则化 深度模型中的优化 卷积网络 序列建模：循环和递归网络 实践方法论 应用 线性因子模型 自编码器 表示学习 深度学习中的结构化概率模型 蒙特卡罗方法 直面配分函数 近似推断 深度生成模型 ","date":"2017-08-03","objectID":"/2017-08-03-redinglist/:0:5","tags":null,"title":"18年秋季阅读计划","uri":"/2017-08-03-redinglist/"},{"categories":null,"content":"Paper Wide \u0026 Deep Learning for Recommender Systems: 谷歌利用深度学习做推荐的思路 Factorization Machines: 广告click model的一个强大解决方案 待续 ","date":"2017-08-03","objectID":"/2017-08-03-redinglist/:0:6","tags":null,"title":"18年秋季阅读计划","uri":"/2017-08-03-redinglist/"},{"categories":null,"content":" I used to love ruining the weekend alone. But now it just drives me mad 感谢基友抽空一起浪费这个周末, 没有他在前面当T我可能就挤不出场馆了( ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:0:0","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"上海新国际博览中心(SNIEC) 规模: 7个主场馆(主要集中在E馆)构造成回形结构, 按顺序逛完, 设计体验很棒. 空调很猛, 但是人实在是太多了.体验类的展台基本上没机会尝试 ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:1:0","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"游戏类 ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:2:0","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"国产游戏 题材1: 武侠 端游, 页游, 手游. 同质化严重. 这么多年了武侠题材无论画风, 剧情和游戏内容都是熟悉的味道, 以致于绕到另一家还以为自己走回头了. 仙剑和武林外传这对老ip在手游的加成下还能焕发第二春吗? 我对此持悲观态度. 很少有手游商是具备做出诸如Deemo, 纪念碑谷这样好游戏的觉悟的. 计算好运营周期, 捞够了钱就走, 掉头寻找下一个目标, 宛如入侵文明星球的异形. 至于活了多久, 口碑有多好, 却是很少被关心的. 不过这也无可厚非. 不这么干的都饿死了. 只是一众好ip被如此消耗, 着实让人惋惜 题材2: 动漫 主要集中在手游/页游上. 游戏模式依然以集换式卡牌为主, 扭蛋成为主要收入方式. 在运营上怎样平衡非核心玩家的投入, 以及核心玩家的粘性? 无保底的阴阳师被饱为诟病, 说明这个问题没有一个完美的答案. 而游戏内容上无非是把现成的热门作品重新拿出来炒一遍, FGO, 碧蓝航线, 不一而足. 腾讯的甲铁城等身手办 好的作品, 能留下一些遗产的作品, 大约都是有着优秀的原创内容的, 所以漫改很少能够长寿, 除非和原作有优良的互动. 阴阳师虽然后期疲软, 但也算是个正面例子: 原创设定的平安世界, 还凭借剧情凑了不少知名cp. 相似的还有Deemo, 崩坏学园等, 都是由游戏带动周边的典范. 国外就更多了, 漫威/DC的一票巨型IP都是泛ACG遍地开花. 学习他们的模式, 道阻且长. 题材3: 电子竞技 企鹅大礼包. CF, LOL, 农药一应俱全. 人太多, 也没兴趣, 没看. ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:2:1","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"海外游戏 代理商阵营: 以网易代理的暴雪游戏, 和完美世界代理的valve游戏为主. 当场切磋/大屏对战吸引了不少观众, 声势浩大. CSGO 在近期结束的克拉科夫锦标赛中赚足了人气 游戏商阵营: EA, 育碧, 乃至日本的万代都有参加. 昆特牌据说还请到了柯洁. 没有什么是一局昆特牌不能解决的, 如果有, 那就再来一局 你可见过从天而降的扎古 价格不菲的实体模型 ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:2:2","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"其他 参展的还有: 硬件商(Intel, 英伟达, AMD, SONY, DELL): Intel包了一整个场馆展示他们的Core i9, 装机秀也很有观赏性. 此外, 以HTC VIVE为首的一票VR设备也很火. Alienware霸气的爆炸图 英伟达在游戏/深度学习领域遍地开花 头戴式VR设备竞争激烈 渠道商(4399, 17178): 童年回忆系列, 不表. 直播平台(欢聚时代, 熊猫tv): 简直把场馆搞得和走秀一样, 惨不忍睹 输入法和百度外卖: 觉厉….你们为什么要来… ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:2:3","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"ACFun和Bilibili 参展的主要目的其实是来这两个展区( ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:3:0","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"Bilibili Bilibili原先只是一群对ACFun不满意的用户自娱自乐建立的小站. 10年从MikuFans改名为Bilibili. 没有自己的视频播放权, 靠第三方网站上传视频并通过盗取流量的方式转播(所谓的战渣浪, 战优酷). 虽然行为不值得提倡, 但也促进了所谓低帧率高码率, 后黑等高清视频播放技巧的推广. 大肆宣传自营手游的bilibili B站最初主要内容是鬼畜/音MAD/MMD, 后来发展为各大字幕组搬运新番的追番视频站, 以其独特的ACG内容+弹幕吐槽模式吸引了大量高粘性用户, 但依然没有自己的转播权. 近几年来正版化之后找到了自己的路线. 在与各大视频网站争夺独家转播权的同时大力发展自营游戏, 举办线下聚会, 成为了估值100亿的优秀的ACG-oriented视频网站. 话说回来, 这也和强力竞争对手ACFun的没落不无关系. 关于AC的故事放到后面说. B站与Fate系列有着不小的渊源, 展台前水泄不通 我从12年初入驻b站, 至今已5个年头. 使用程度一直不深不浅, 但也亲身经历了几个大事件: 12年4月Fate/Zero转播, 万人空巷(同时在线\u003e50万) 由于无视频播放许可证, 中途被勒令撤销转播, bishi透露每年亏损200万以上(记不清具体数字了) 出台购买新番功能, 据说资金并不能落实到发行方, 引起广泛争议 2233娘, 拜年祭, BML, 形象宣传愈演愈烈 最后, 作为天朝泛ACG领域的门户, 希望他们能够不忘初心. ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:3:1","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"ACFun AC娘的包子头在展区到处都是. 可惜A站的坎坷, 恐怕没有几个包子头知道. 除了开启了国内最早的弹幕评论, ACFun最有特色的是评论区. 从评论区形成了独特的社群, 这点是B站至今都不具备的, 因为只有用户基数少的前提下才能形成这样的氛围, 所谓小圈子. 成也小圈子, 败也小圈子. 周边销售区的长队. 至少AC表情包和包子头还是很火的 A站自从07年成立后, 因为其反人类的界面, 吹电风扇般的审核, 优质p主的流失, 形成了恶性循环, 再加上B站的异军突起吸引走了大量新用户, 一路衰败到难以维系的地步. ACFun在2012年开启了频繁的改版, 也引起了很多不满, 不过一直在跌跌撞撞中前进着. 所幸文章区和A岛维系了最基本的用户圈子, 使得A站一直坚持到了今天. A站也加入了抢夺独家的战争中, 然而势单力薄, 热门作品稀少 其实ACFun也有过辉煌的历史. 孵化出了国内最早的直播平台: 斗鱼. A站曾经有两个弯道超车的机会，一次是直播，也确实做成功了，AcFun孵化出了斗鱼，但是被利益方无情剥离。另一次是移动客户端，由于有了斗鱼的前车之鉴，AcFun的用户连同大量up主集体抵制独立品牌的移动客户端，AcFun的武汉团队没有得逞，过了没多久就把AcFun转给了北京团队。 https://www.zhihu.com/question/24389642/answer/203338340 独立品牌的名字叫爱稀饭, 当时遭到了强烈抵制(笑 展台人潮稀疏, 然而热情丝毫不输, 甚至还有跟着起舞的(笑 A站自从2014年改版之后就相对稳定了下来. 频繁更换的首页banner和层出不穷的主题文章为其增添了很多新鲜度.AC的用户群体也相对较老, 大多是从B站成立之前就注册的人群(毕竟之后注册的都去B站了). 虽然大家都不那么\"二次元\"了, 但更多只是把那份热情放到了心底, 毕竟生活还是要过的不是( 评论区里只要提到相关内容, 即使是20世纪的古旧新番都有人可以娓娓道来. A站这么惨, 一定是隔壁的错(笑 最后, 拿战利品收尾吧 认真你就输了 AC在, 爱一直在 ","date":"2017-08-01","objectID":"/2017-08-01-chinajoy/:3:2","tags":["photograph"],"title":"ChinaJoy 2017纪实 多图杀猫","uri":"/2017-08-01-chinajoy/"},{"categories":null,"content":"很久没有动blog了. 修整了一下, 希望能做得有趣一点, 别老更新冷冰冰的技术文, 也写点别的, 狗窝也得有个窝样是不是(x 更换主题(感谢原作者Litten, 伊莉雅真快) (done) 添加Disqus(多说已死), 并支持国内直接访问 (难支持国内访问) 重新整理tags和categories (done) 重新组织侧边栏 (done) 跟换DNSPod, 做到国内使用coding pages部署加速 (done) 百度可爬 (困难) 注: 8月3日解决 使用hexo-renderer-marked加速markdown渲染, npm包版本过旧, 手动更新到0.3.0 9/22更新 改变了代码框高亮主题, 有空pr一下highlight.scss sudo rm -rf ~ sudo reboot ... echo WELCOME BACK! ","date":"2017-07-27","objectID":"/2017-07-26-new-start/:0:0","tags":null,"title":"工坊装修记录","uri":"/2017-07-26-new-start/"},{"categories":null,"content":"Seaborn是基于matplotlib的封装, 使得制作图表更为简便 ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:0:0","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"一维数据的分析 %matplotlib inline import numpy as np import pandas as pd from scipy import stats, integrate import matplotlib.pyplot as plt import seaborn as sns sns.set(color_codes=True) ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:1:0","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"1. displot()绘制柱状图 bins={int}: 柱子的密度 kde={True, False}: 是否显示KDE拟合密度曲线 rug={True, False}: 是否显示条状密度标志 x = np.random.normal(size=100) sns.distplot(x, bins=20, kde=True, rug=True) \u003cmatplotlib.axes._subplots.AxesSubplot at 0x11693bdd0\u003e ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:1:1","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"2. jointplot()绘制二维柱状图 x: x坐标 y: y坐标 data: DataFrame数组 mean, cov = [0, 1], [(1, .5), (.5, 1)] data = np.random.multivariate_normal(mean, cov, 200) df = pd.DataFrame(data, columns=[\"x\", \"y\"]) sns.jointplot(x=\"x\", y=\"y\", data=df) \u003cseaborn.axisgrid.JointGrid at 0x11923bbd0\u003e ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:1:2","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"二维数据的分析 ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:2:0","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"1. regplot()做回归分析 参数和jointplot()类似 tips = sns.load_dataset(\"tips\") sns.regplot(x=\"total_bill\", y=\"tip\", data=tips) \u003cmatplotlib.axes._subplots.AxesSubplot at 0x1169a6cd0\u003e ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:2:1","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"2. lmplot()更高自由度的二维图绘制 fit_reg=True: 设置为False即可关掉线性回归 还有各种参数见文档, regplot()是lmplot()的一个子集 ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:2:2","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"3. 用pairplot()展示不同特征的关系 hue参数可以在数据点上用颜色标注某一特征的不同值. iris = sns.load_dataset(\"iris\") sns.pairplot(iris, hue=\"species\") \u003cseaborn.axisgrid.PairGrid at 0x11c290610\u003e ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:2:3","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"4. heatmap()展示不同特征的协方差 corrmat = iris.corr() sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}) \u003cmatplotlib.axes._subplots.AxesSubplot at 0x11f1aaf10\u003e ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:2:4","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"分类数据的分析 ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:3:0","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"1. stripplot()统计各个类的数据 注: jitter=True参数可以引入横向噪音, 用于显示分布密度, 避免样本堆叠 tips = sns.load_dataset(\"tips\") sns.stripplot(x=\"day\", y=\"total_bill\", data=tips) \u003cmatplotlib.axes._subplots.AxesSubplot at 0x11dc739d0\u003e ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:3:1","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"2. boxplot()绘制箱形图 sns.boxplot(x=\"day\", y=\"total_bill\", data=tips) \u003cmatplotlib.axes._subplots.AxesSubplot at 0x11ddf3ad0\u003e ","date":"2017-03-29","objectID":"/code/2017-03-29-seaborn/:3:2","tags":["seaborn","matplotlib","python"],"title":"数据可视化工具Seaborn指南","uri":"/code/2017-03-29-seaborn/"},{"categories":null,"content":"pandas, python栈的数据处理最强package, 没有之一(? # -*- coding: utf-8 -*- import pandas as pd import numpy as np import matplotlib.pyplot as plt ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:0","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"1. Series() s = pd.Series([1,3,5,np.nan,6,8]) 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 一些实用方法 sort_values(): 按值排序 sort_index(): 按行名排序 ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:1","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"2. date_range() dates = pd.date_range('20130101', periods=6) dates DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:2","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"3. DataFrame() 第一个参数是numpy数组, 第二个参数是行号(index), 第三个参数是列号(culumns), 行列号都是list类 df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD')) df ABCD2013-01-01-0.507010-2.1960630.0860580.4890022013-01-02-0.572061-0.552969-0.4891840.0904942013-01-03-0.9819790.1992651.2830720.6320272013-01-040.406212-1.425934-0.3619030.5888642013-01-05-0.614261-0.3938620.320324-0.1324432013-01-06-0.186325-0.269089-1.798570-0.271104 也可以使用字典形式初始化DataFrame类 df2 = pd.DataFrame({ 'A' : 1., ....: 'B' : pd.Timestamp('20130102'), ....: 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), ....: 'D' : np.array([3] * 4,dtype='int32'), ....: 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), ....: 'F' : 'foo' }) df2 ABCDEF01.02013-01-021.03testfoo11.02013-01-021.03trainfoo21.02013-01-021.03testfoo31.02013-01-021.03trainfoo 也可以使用访问域的方法 df.A 2013-01-01 -0.507010 2013-01-02 -0.572061 2013-01-03 -0.981979 2013-01-04 0.406212 2013-01-05 -0.614261 2013-01-06 -0.186325 Freq: D, Name: A, dtype: float64 也可以使用访问字典的方法插入新的列 df3 = df.copy() df3['E'] = ['one', 'one','two','three','four','three'] df3 ABCDE2013-01-01-0.507010-2.1960630.0860580.489002one2013-01-02-0.572061-0.552969-0.4891840.090494one2013-01-03-0.9819790.1992651.2830720.632027two2013-01-040.406212-1.425934-0.3619030.588864three2013-01-05-0.614261-0.3938620.320324-0.132443four2013-01-06-0.186325-0.269089-1.798570-0.271104three ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:3","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"4. DataFrame类的常用域和方法 dtypes: 显示每列的数据类型 index: 显示行名 columns: 显示列名 values: 显示值 4.1 查看 head(), tail(): 显示头部和尾部, 第一个参数是显示多少行 describe(): 显示数据的统计信息 info(): 显示详细信息 4.2 sort sort_index(axis=1, ascending=False): 按列名排序 sort_values(by='col_name'): 按列内容排序 4.3 Indexing df['A']: 按column名选择 df[0:3]: 按行号选择[0,1,2]三行 df.loc[dates[0]]: 按行名选择(自定义了行名) df.loc[:,['A','B']]: 类似于numpy的切片 df.iloc[3:5,0:2]: 使用整数切片 df.iloc[[1,2,4],[0,2]]: 使用整数数组指定切片 df.iloc[1,1]: 得到固定位置的值 4.4 Boolean Indexing df[df.A \u003e 0]: 列出A列\u003e0的所有行 df[df \u003e 0]: 所有小于等于0的元素都置为NaN 4.5 Filtering df3['E'].isin(['two','four']) 2013-01-01 False 2013-01-02 False 2013-01-03 True 2013-01-04 False 2013-01-05 True 2013-01-06 False Freq: D, Name: E, dtype: bool 返回一个boolean列表之后, 可以使用列表切片, 类似于掩码的形式(masking) df3[df3['E'].isin(['two','four'])] ABCDE2013-01-03-0.9819790.1992651.2830720.632027two2013-01-05-0.614261-0.3938620.320324-0.132443four 4.6 更改值 df.at[dates[0],'A'] = 0: 利用label搜索 df.iat[0,1] = 0: 利用整数数组搜索 甚至可以利用np数组修改 df.loc[:,'D'] = np.array([5] * len(df)) df ABCD2013-01-01-0.507010-2.1960630.08605852013-01-02-0.572061-0.552969-0.48918452013-01-03-0.9819790.1992651.28307252013-01-040.406212-1.425934-0.36190352013-01-05-0.614261-0.3938620.32032452013-01-06-0.186325-0.269089-1.7985705 4.7 其他 施工中 ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:4","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"5. 缺失值处理 5.1 reindex() df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E']) # 把E列填入一些非NaN的值 df1.loc[dates[0]:dates[1],'E'] = 1 df1 ABCDE2013-01-01-0.507010-2.1960630.08605851.02013-01-02-0.572061-0.552969-0.48918451.02013-01-03-0.9819790.1992651.2830725NaN2013-01-040.406212-1.425934-0.3619035NaN 5.2 dropna() dropna()可以去掉包含NaN值的行. 参数how的用法待考 注意: 不会改变df本身, 返回的是新的df df1.dropna(how='any') ABCDE2013-01-01-0.507010-2.1960630.08605851.02013-01-02-0.572061-0.552969-0.48918451.0 5.3 fillna(value=VALUE_TO_FILL) 填充空值, 其中value可以是平均值,常用. ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:5","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"6. 统计操作 1. mean(axis=None, skipna=None, level=None, numeric_only=None, **kwargs)[source] 返回平均值. axis : {index (0), columns (1)} 0返回每个特征的平均值(竖着求和), 1返回每个样本的平均值(横着求和), 默认是0 ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:6","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"7. 文件存取 df.to_csv('foo.csv'): 保存到csv pd.read_csv('foo.csv'): 读取csv 待续 ","date":"2017-03-29","objectID":"/code/2017-03-29-pandas-10min/:0:7","tags":["pandas"],"title":"pandas速查","uri":"/code/2017-03-29-pandas-10min/"},{"categories":null,"content":"先看Refer, 该介绍的都介绍到位了: 17/10/09 更新了预备知识和详细计算过程 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA) 强大的矩阵奇异值分解(SVD)及其应用 机器学习西瓜书 CS231n翻译 深度学习(花书) ","date":"2017-03-27","objectID":"/2017-03-27-pca/:0:0","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"预备知识 ","date":"2017-03-27","objectID":"/2017-03-27-pca/:1:0","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"坐标变换 设$\\vec x$在$\\Bbb R^d$的坐标为 $$ \\vec x = [x_1, x_2, x_3, \\cdots, x_d]^T $$ $W$是$\\Bbb R^d$的一组正交标准基: $$ W = [\\vec w_1, \\vec w_2, \\vec w_3, \\cdots, \\vec w_{d’}] $$ 假设$d’\u003cd$, 则$W$的列空间$\\Bbb R^{d’}$是参考系的子空间, 或者说是参考系中的超平面. $\\vec x$在超平面上的投影即为$\\vec x$在$\\Bbb R^{d’}$的坐标, 设为$\\vec c$: $$ \\vec c = W^T \\vec x \\tag{1}$$ 而$\\vec c$在$\\Bbb R^d$的坐标是 $$ \\hat x = W \\vec = WW^T \\vec x c \\tag{2}$$ ","date":"2017-03-27","objectID":"/2017-03-27-pca/:1:1","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"迹和矩阵 矩阵的迹和Frobenius范数的关系: $$ ||A||_F = \\sqrt{tr(AA^T)} \\tag{3} $$ 矩阵乘积顺序不改变迹的结果 $$ tr(ABC) = tr(CAB) = tr(BCA) \\tag{4} $$ ","date":"2017-03-27","objectID":"/2017-03-27-pca/:1:2","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"协方差矩阵特征值分解方法 PCA需要满足的优化目标为: 最近重构性: 样本点到超平面/投影点的距离最近 最大可分性: 样本点在超平面上的距离足够分开(即方差最大) 可以证明两个目标是等价的. ","date":"2017-03-27","objectID":"/2017-03-27-pca/:2:0","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"优化目标 假设样本进行了中心化 $$ \\sum_i x_i = 0 \\tag{5}$$ 根据最近重构性写出优化目标 $$ \\begin{align} arg \\min || \\vec x - \\hat x || \u0026 = || \\vec x - W \\vec c || \\\\ \u0026 = || \\vec x - W W^T \\vec x || \\end{align} \\tag{6} $$ 假设样本由$m$个列向量组成: $$ X = [\\vec x_1, \\vec x_2, \\cdots, \\vec x_m] \\tag{7} $$ 优化目标转化为 $$ \\begin{align} arg \\min || X - \\hat X ||_F \u0026 = || X - W C ||_F \\\\ \u0026 = || X - W W^T X ||_F \\\\ \u0026 = tr((X - W W^T X)(X - W W^T X)^T) \\\\ \u0026 = tr((X - W W^T X)(X^T - X^TWW^T)) \\\\ \\end{align} \\tag{8} $$ 此处使用了矩阵Frobenius范数的迹表示. 展开后得到 $$ \\begin{align} tr(XX^T - XX^TWW^T - WW^TXX^T + WW^TXX^TWW^T) \\end{align} \\tag{9} $$ $XX^T$和优化目标无关, 可忽略 $tr(XX^TWW^T) = tr(WW^TXX^T) = tr(W^TXX^TW)$ 第4项可简化为 $$ \\begin{align} tr(WW^TXX^TWW^T) \u0026= tr(WW^TWW^TXX^T) \\\\ \u0026= tr(WW^TXX^T) \\\\ \u0026= tr(W^TXX^TW) \\end{align} $$ 最后得到优化结果: $$ arg\\min_W -tr(W^TXX^TW) \\Leftrightarrow arg\\max_W tr(W^TXX^TW) \\tag{10} $$ ","date":"2017-03-27","objectID":"/2017-03-27-pca/:2:1","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"两个条件的等价性 注意到$(10)$中$W^TX = C$, $X^TW = C^T$, $W^TXX^TW = CC^T$, 而$tr(CC^T) = tr(C^TC) = \\sum\\limits_{i=1}^m \\vec c_i^T \\vec c_i = \\sum\\limits_{i=1}^m || \\vec c_i ||^2$, 这正是最大可分性的优化目标, 因此两者是等价的 ","date":"2017-03-27","objectID":"/2017-03-27-pca/:2:2","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"特征值分解 由于$W$有$d’$个正交标准基, 假设$d’ = 1$, 取$\\vec w_1$观察优化目标: $$ arg\\max_{\\vec w_1} tr(\\vec w_1^TXX^T\\vec w_1) $$ 由于结果是标量, 设 $$ tr(\\vec w_1^TXX^T\\vec w_1) = \\vec w_1^TXX^T\\vec w_1 = \\lambda_1 = \\vec w_1^T \\lambda_1 \\vec w_1 $$ 即 $$ XX^T\\vec w_1 = \\lambda_1 \\vec w_1 $$ 对协方差矩阵$XX^T$做特征值分解, 使$\\lambda_1$最大即为找到最大的特征值与特征向量. 推广至$W$, 即为找到前$d’$组最大的特征值与特征向量. ","date":"2017-03-27","objectID":"/2017-03-27-pca/:2:3","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"SVD方法 实践中常常直接对$X$进行SVD分解: $$X_{d \\times m} \\approx U_{d \\times r} \\Sigma_{r \\times r} V^T_{r \\times m}$$ 其中$U$和$V$都是正交标准矩阵, $\\Sigma$是对角方阵. 因此 $$ \\begin{align} XX^T \u0026= U_{d \\times r} \\Sigma_{r \\times r} V^T_{r \\times m} V_{m \\times r} \\Sigma_{r \\times r} U^T_{r \\times d} \\\\ \u0026= U_{d \\times r} \\Sigma_{r \\times r}^2 U^T_{r \\times d} \\end{align} $$ 因此这里的$U$即为上一节的$W$. 稍作变换有: $$ XX^T U_{d \\times r} = U_{d \\times r} \\Sigma_{r \\times r}^2$$ 这意味着对列进行了压缩, 从$d$列压缩到了$r$列, 这里的$r$实际上就是PCA中的$d'$ 此外, 分析一下压缩空间中的点可以发现: $$ CC^T = (W^TX)(W^TX)^T = W^TXX^TW = W^T W \\Sigma^2 W^TW = \\Sigma^2 $$ 因为$\\Sigma$是对角阵, 因此压缩空间中的元素彼此无关(协方差为0) ","date":"2017-03-27","objectID":"/2017-03-27-pca/:3:0","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"总结 主成分分析(Principal Component Analysis, PCA)是无监督算法, 使用了特征值分解, SVD等方法, 通过选择方差最大的投影来压缩矩阵信息, 去掉不重要/线性相关的信息, 保留重要/正交信息 线性判别分析(Linear Discriminant Analysis, LDA)是有监督算法, 目标是投影后类内方差小, 类间方差大 ","date":"2017-03-27","objectID":"/2017-03-27-pca/:4:0","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"Numpy实现 除了PCA以外, 也包括了一些常用的数据预处理 均值减法(Mean subtraction): 将均值变为0, 也叫中心化 X -= np.mean(X, axis=0) 归一化(Normalization): 将方差变为1 X /= np.std(X, axis=0) PCA 假设输入数据矩阵X的尺寸为[N x D], 在中心化之后: 得到数据的协方差矩阵 cov = np.dot(X.T, X) / X.shape[0] 作SVD分解, U的列是特征向量，S是装有奇异值的1维数组 U,S,V = np.linalg.svd(cov) PCA: 取前100个基进行压缩 Xrot_reduced = np.dot(X, U[:,:100]) 白化(whiten): Xwhite = Xrot_reduced / np.sqrt(S + 1e-5) 这个算法中让压缩矩阵的每个维度都除以其特征值, 使得协方差相等, 表现在数据中就是使得整个数据符合高斯分布 ","date":"2017-03-27","objectID":"/2017-03-27-pca/:5:0","tags":["pca"],"title":"PCA","uri":"/2017-03-27-pca/"},{"categories":null,"content":"Logistic Regression的小结 ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:0:0","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"第一部分 Logistic Regression ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:1:0","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"1. 线性回归的缺点 线性回归不适用于分类问题: 容易过拟合 $h_\\theta$ can be \u003e1 or \u003c0 ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:1:1","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"2. Logisitc Regression 2.1 模型假设 Sigmoid函数: $$h_\\theta (x) = \\frac {1}{1 + e^{-\\theta^T x}}$$ 概率学解释: 给定$\\theta^T x$之后$y=1$的概率. 可以和SVM中的几何间隔类比一下. $\\theta^T x$接近于正无穷的时候概率值接近1 此外, 既然是概率分布, 显然$P(y=0|x=\\theta) + P(y=1|x=\\theta) = 1$ 2.2 似然函数 使用极大似然法估计模型参数, 对于每一个$(x^{(i)}, y^{(i)})$, 似然函数为: $$ [h_\\theta (x^{(i)})]^{y^{(i)}} [1-h_\\theta (x^{(i)})]^{1-y^{(i)}}$$ 对于整个训练集, 似然函数为 $$ \\prod\\limits_{i = 1}^n [h_\\theta (x^{(i)})]^{y^{(i)}} [1-h_\\theta (x^{(i)})]^{1-y^{(i)}}$$ 似然函数很直观, 若$y=1$, 那么$h_\\theta (x)$越接近1, 似然函数越大, 反之亦然. 2.3 损失函数 最大化似然函数就是最小化损失函数. 对似然函数取负就转化为损失函数了. 为了简化计算, 取log将似然函数的连乘转换为求和的形式. $$-Cost(h_\\theta (x), y) = y log(h_\\theta (x)) + (1 - y)log(1 - h_\\theta (x))$$ 连乘转化为求和,就得到了总的损失函数 $$ J(\\theta) = -\\frac{1}{n}\\sum\\limits_{i=1}^{n}[y^{(i)}\\log h_\\theta(x^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))] $$ ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:1:2","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"3. 最优化算法 3.1 使用梯度下降最优化 $J(\\theta)$ $$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^{n}[(h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}]$$ $$\\theta_j := \\theta_j - \\alpha \\frac{1}{n} \\sum\\limits_{i=1}^{n}[(h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}]$$ 其中$\\alpha$为学习率, 也就是每一步梯度下降的步长 3.2 其他最优化方法 梯度下降之外的其他优化方法 (不需要手动调参 $\\alpha$): Conjugate descent IIS 拟牛顿法 BGFS L-BFGS 3.3 多类别分类: One-vs-all 对于y,有超过0和1两个值的, 对每一个值做LR回归 ,然后输出每个模型的概率, 取最大的那个 ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:1:3","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"4. Softmax ufldl的参考资料 4.1 模型假设 logistic regression中的sigmoid函数可以写成: $$h_\\theta (x) = \\frac {e^{\\theta^T x}}{1 + e^{\\theta^T x}}$$ 如果$y_i$不是二元变量, 而是$K$元变量(例如$y \\in \\lbrace 1, 2, 3, \\cdots, K \\rbrace$), 自然推广到了softmax回归: $$ h_\\theta (x) = \\frac{1}{\\sum\\limits_{i = 1}^{K} e^{\\theta_i^T x}} \\begin{bmatrix}e^{\\theta_1^T x}\\\\e^{\\theta_2^T x}\\\\ \\vdots \\\\ e^{\\theta_K^T x} \\end{bmatrix} $$ 其中 $$ \\theta = \\begin{bmatrix} \\theta_1^T \\\\ \\theta_2^T \\\\ \\vdots \\\\ \\theta_K^T \\end{bmatrix}, \\frac{ e^{\\theta_j^T x^{(i)}}}{\\sum\\limits_{l = 1}^{K} e^{\\theta_l^T x^{(i)}}} = P(y^{(i)}=j |x^{(i)}, \\theta ) $$ 也就是softmax针对每一个可能的输出值都估计了一个概率值 4.2 损失函数 $$ J(\\theta) = -\\frac{1}{n} \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{K} I(y^{(i)} = j) log \\frac{ e^{\\theta_j^T x^{(i)}}}{\\sum\\limits_{l = 1}^{K} e^{\\theta_l^T x^{(i)}}} $$ 简单解释: 内层循环把输出和$y^{(i)}$符合的值全部统计到了损失函数中, 他们构成了让损失函数降低的要素(其他值都是0), 外层累加了所有samples 4.3 参数冗余 实际上对$\\theta$的每一行加上同一个向量之后输出值不会改变, 因此参数是可以压缩的. 而其Hessian 矩阵是奇异的/不可逆的, 因此使用牛顿法会优化会遇到困难, 虽然$J(\\theta)$是凸的 4.4 正则化 加入L2正则项之后顺带解决了Hessian 矩阵不可逆的问题 $$ J(\\theta) = -\\frac{1}{n} \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{K} I(y^{(i)} = j) log \\frac{ e^{\\theta_j^T x^{(i)}}}{\\sum\\limits_{l = 1}^{K} e^{\\theta_l^T x^{(i)}}} + \\frac{\\lambda}{2} \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{K} \\theta_{ij}^2 $$ 梯度是 $$ \\nabla_{\\theta_j} J(\\theta) = -\\frac{1}{n} \\sum\\limits_{i=1}^{n} x^{(i)}(I(y^{(i)} = j) - P(y^{(i)}=j |x^{(i)}, \\theta )) $$ 这是一个$n$维向量, 和$x$规格一致, 第$i$个元素代表了$\\frac{\\partial J(\\theta)}{\\partial \\theta_{ji}}$, 即$J(\\theta)$对$\\theta_j$第$i$个分量的偏导数. ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:1:4","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"5. 正则化(Regularzation) 过拟合的问题(看图吧) 处理过拟合问题 降低特征维度 (manually/model selection) 正则化: add panalize terms to some less important parameters, or panalize all the parameters(except for $\\theta_0$) Regularized Linear Regression $$\\displaystyle \\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j$$ For normal equation, regularization would also make $X^T X$ invertible Regularized Logistic Regression Literally the same as Regularized Linear Regression except for the form of $h_\\theta (x)$ ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:1:5","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"第二部分 最大熵模型 施工中 ","date":"2017-03-27","objectID":"/2017-03-27-logistic-regression/:2:0","tags":["logistic regression","machine learning"],"title":"Logistic Regression, Softmax与最大熵","uri":"/2017-03-27-logistic-regression/"},{"categories":null,"content":"决策树的小结 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:0:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"第一部分: 熵, 条件熵和信息增益 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:1:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"1. 熵 熵的定义和变量的概率分布有关, 和变量本身的值无关, 定义如下: $$ H(X) = - \\sum\\limits_{i=1}^{n} p_i log p_i \\\\ \\text{其中} P(X=x_i) = p_i, i= 1,2,\\cdots, n$$ $H(X)$的一个主要特点是$x$的分布越平均, 熵越大, 因此熵代表了分布的混乱程度 另外, 对数底一般取2或e, 其单位为bit或nat ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:1:1","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"2. 条件熵 假设两个随机变量$(X, Y)$的联合概率分布为$$P(X=x_i, Y=y_j) = p_{ij} \\\\ |X| = n, |Y| = m$$ 根据贝叶斯公式, $Y$对于$X$的条件概率为$$ P(Y = y_j | X=x_i) = \\frac{P(X=x_i, Y=y_j)}{P(X=x_i)} = \\frac{p_{ij}}{p_i}$$ 那么在给定$X= x_i$的条件下$Y$的条件概率分布的熵为 $$ \\begin{align} H(Y | X=x_i) \u0026 = - \\sum\\limits_{j=1}^{m} P(Y = y_j | X=x_i) log P(Y = y_j | X=x_i) \\\\ \u0026 = - \\sum\\limits_{j=1}^{m} \\frac{p_{ij}}{p_i}log \\frac{p_{ij}}{p_i} \\end{align} $$ 最后, $H(Y | X=x_i)$对于$X$的数学期望就是$Y$对于$X$的条件熵: $$ H(Y | X) = \\sum\\limits_{i=1}^{n} p_i H(Y | X=x_i)$$ ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:1:2","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"3. 一个例子 条件熵的概念, 可以通过例子理解 假设$X$是某位长者前去魔都的概率分布, 去是1, 不去是0 $$P(X=1) = 0.5, P(X=0) = 0.5$$ 假设$Y$是魔都天气的概率分布, 1是晴, 0是雨 $$P(Y=1) = 0.5, P(X=0) = 0.5$$ 也就是说, 去不去看心情, 天气好不好看老天. 然而 $$ P(Y=1 |X=1 ) = 1 \\\\ P(Y=1 |X=0 ) = 0 \\\\ P(Y=0 |X=1 ) = 0 \\\\ P(Y=0 |X=0 ) = 1$$ 也就是说主席一来, 天气晴朗, 主席一走, 立马下雨 那么求一下$Y$的条件概率分布的熵 $$H(Y | X=1) = - (\\frac{P_{11}}{P_{1}} log \\frac{P_{11}}{P_{1}} + \\frac{P_{01}}{P_{1}} log \\frac{P_{01}}{P_{1}}) = -2 \\\\ H(Y | X=不去) = - (\\frac{P_{10}}{P_{0}} log \\frac{P_{10}}{P_{0}} + \\frac{P_{00}}{P_{0}} log \\frac{P_{00}}{P_{0}}) = -2 $$ 最后得到条件熵 $$ H(Y | X) =p_1 H(Y | X=1) + p_0H(Y | X=0) = -2$$ 总结一下, $H(X) = H(Y) = 1$, 然而$H(Y | X) = -2$, 也就是在主席到来的约束下, $Y$的条件熵下降了, 也就是天气分布的不确定性下降了, 可见主席来不来对天气好坏是有很大影响的 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:1:3","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"4. 信息增益 信息增益表示得知特征$X$的信息而使得类Y的信息的不确定性减少的程度. 拿之前的例子来说, 选择主席是否来这个特征对天气数据集的信息增益为 $$ g(Y, X) = H(Y) - H(Y|X) = 1 - (-2) = 3$$ 决策树的特征选择基础就是建立在选择最大信息增益的特征上的. ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:1:4","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"5. 信息增益比 信息增益的一个缺点是决策树会偏向于选择取值较多的特征. 例如假设每个$X$的取值对应于独一无二的值(item_id类型的数据)有: $$ \\begin{align} H(Y | X=x_i) \u0026 = - \\sum\\limits_{j=1}^{m} P(Y = y_j | X=x_i) log P(Y = y_j | X=x_i) \\\\ \u0026 = - \\sum\\limits_{j=1}^{m} \\frac{p_{ij}}{p_i}log \\frac{p_{ij}}{p_i} \\\\ \u0026 = - \\frac{1/N}{1/N}log \\frac{1/N}{1/N} = 0 \\end{align} $$ $$ H(Y | X) = \\sum\\limits_{i=1}^{n} p_i H(Y | X=x_i) = 0 $$ 条件熵降到了0, 显然信息增益是最大的. 但这样的分类毫无意义, 属于极端过拟合, 为了解决这个问题, 引入信息增益比. 首先由于$X$的划分行为, 导致数据集label$Y$的熵增加了(每一类内部的熵减少了) $$ H_X(Y) = - \\sum\\limits^{n}_{i=1} P(X = x_i) log P(X = x_i) $$ $$ g_R(Y, X) = \\frac{H(Y) - H(Y|X)}{H_X(Y)} $$ 这样一来使用之前的例子, $ H_X(Y) = - n \\times \\frac{1}{n} log \\frac{1}{n} = - log \\frac{1}{n} $, $n$越大, 分得越细, 信息增益的分母越大, 信息增益比越小. ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:1:5","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"第二部分: ID3和C4.5算法 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:2:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"1. ID3 Input: Train set $S$, feature set $A$, tolerance $\\epsilon$ Output: Decision Tree $T$ If $s_i \\in C_j$ for all $i$: $label = c_k$, return T If $A = \\varnothing$: $label=c_j: \\max(|c_j|)$, return T Else: calculate $g(S, A_i)$ for all $i$, choose $A_g$ in which $g = argmax(g(S, A_i))$ If $g(S, A_g) \u003c \\epsilon$: $label= c_j: \\max(|c_j|)$, return T Else: Split $S$ into $S_i$ for each $a_i$ in $A_g$, $label_i= c_j: \\max(|c_j|), c_j \\in S_i$, return T For each $a_i$ as a node, train set = $S_i$, feature set = $A - A_g$, run (1) ~ (5) recursively, return $T_i$ ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:2:1","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"2. C4.5 使用 信息增益率替代信息增益. 但信息增益率倾向于选择特征取值较少的, 故用启发式算法: 先选择信息增益高于平均水平的特征,再从中选择信息增益率最高的 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:2:2","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"第三部分: CART ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:3:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"1. 回归树 训练集: $$D = \\lbrace (X^{(1)}, y^{(1)}), (X^{(2)}, y^{(2)}), \\cdots, (X^{(N)}, y^{(N)}) \\rbrace \\\\ X = (x_1, x_2, \\cdots, x_n)$$ 损失函数: 平方损失函数 $$\\sum\\limits_i (y^{(i)} - f(X^{(i)}))^2$$ 划分手段: 对于第$f$个维度的特征$X_f$的某个值$v$(假设特征的值都是离散值), 将所有样本划分为两部分: $$ R_1(f, v) = \\lbrace X | X_f \\leq v \\rbrace \\\\ R_2(f, v) = \\lbrace X | X_f \u003e v \\rbrace $$ 搜索最佳特征$f$和值$v$ $$ \\min\\limits_{f, v}[\\sum\\limits_{X^{(i)} \\in R_1} (y^{(i)} - \\hat c_1 ) + \\sum\\limits_{X^{(i)} \\in R_2} (y^{(i)} - \\hat c_2 )] $$ 其中$\\hat c_1, \\hat c_2$是划分区域内部所有样本的$y^{(i)}$的均值, 这样使得平方误差最小. ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:3:1","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"2. 分类树 基尼指数: 从数据集中随机抽取两个样本, 其数据标记不一样的概率. 假设$y$有$K$个类, 每个类有$C_k$个样本, 样本容量为|D|, 基尼指数被定义为: $$ \\begin{align} Gini(p) \u0026 = \\sum\\limits^{K}_{i = 1} p_i(1-p_i) \\\\ \u0026 = 1 - \\sum\\limits^{K}_{i = 1}p_i^2 \\\\ \u0026 = 1 - \\sum\\limits^{K}_{i = 1} (\\frac{|C_i|}{D})^2 \\end{align} $$ 和条件熵一样, 当数据集$D$被某一特征$A$按照是否取某一个值a的规则分割为两个集合$D_1, D_2$时, “条件基尼指数\"为: $$ Gini(D, A) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2) $$ CART分类树的构造规则和ID3类似, 选择条件基尼指数最大的特征递归构造 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:3:2","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"第四部分: 剪枝 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:4:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"1. ID3/C4.5的剪枝 损失函数为: $$ C_{\\alpha}(T) = \\sum\\limits_{t=1}^{|T|} N_t H_t(T) + \\alpha|T| $$ 其中$N_t$代表第$t$个叶结点的容量, $H_t(T)$代表第$t$个叶结点的熵, 而$\\alpha$即为正则化参数 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:4:1","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"2. CART的剪枝 预剪枝: 比较剪枝前后在验证集上的精度来决定是否剪枝 后剪枝: 自下而上替换节点, 欠拟合风险小, 泛化能力优于预剪枝 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:4:2","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"第五部分: 连续与缺失值处理 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:5:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"1. 连续值: 连续特征离散化 二分法(bi-partition), C4.5使用 对特征值排序:{啊,吖,阿,( ⊙ o ⊙ )啊！} 取两个样本的中点作为分割点, 一共有n-1个分割点, 找到使得信息增益/率最大的分割点, 作为二分类特征处理 与离散特征不同, 连续特征使用过的还可以继续使用 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:5:1","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"2. 缺失值 对每个特征, 只取不缺失的行, 假设占总行数的比例为$\\rho$ 改变信息增益计算方法: 在信息增益上乘以系数$\\rho$即可 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:5:2","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"第六部分: 多变量决策树 使用特征的线性组合作为新的特征, 可以做到斜划分 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:6:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"第七部分: python实现 源代码, 注释施工中 ","date":"2017-03-27","objectID":"/2017-03-26-decision-tree/:7:0","tags":["decision tree","machine learning"],"title":"决策树","uri":"/2017-03-26-decision-tree/"},{"categories":null,"content":"Adaboost的总结 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:0:0","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"第一部分: AdaBoost简介 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:1:0","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"1.1 强可学习和弱可学习 在概率近似正确(PAC)学习框架中, 一个类如果存在: 一个多项式复杂度的学习算法,正确率略大于随机猜测(例如二分类问题中大于1/2),称弱可学习的类 一个多项式复杂度的学习算法,并且正确率很高,称强可学习的类 Kearns和Valiant证明了强可学习和弱可学习是等价的 Adaboost算法就是将弱学习器组成强学习器的算法 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:1:1","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"1.2 组合弱分类器 测试集: $$T = \\left\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(N)}, y^{(N)})\\right\\rbrace, \\quad x \\in \\chi \\subseteq R^N, y \\in \\lbrace +1, -1 \\rbrace \\\\ \\text{with weights: } D_i = (w_{i1}, w_{i2}, \\cdots, w_{iN})$$ 分类器 $$G_m(x): \\chi \\to \\lbrace +1, -1\\rbrace, \\quad G(x) = \\sum\\limits^M_{i=1} \\alpha_m G_m(x) \\\\ \\text{with weights: } A = (\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{M}) $$ ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:1:2","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"1.3 AdaBoost 算法 输入: 训练集 $T$, m个弱分类器 $G_m(x)$ 输出: 集成分类器 $G(x)$ 初始化权重: $$ D_1 = (w_{11}, w_{12}, \\cdots, w_{1N}), \\quad w_{1i} = \\frac{1}{N} $$ For $m = 1,2, \\cdots, M$: (a) 对具有权重分布 $D_m = (w_{m1}, w_{m2}, \\cdots, w_{mN})$ 的训练集 $T$ 训练出弱分类器 $G_m(x)$ (b) 计算弱分类器 $G_m(x)$ 在 $T$ 上的误差率: $$ e_m = P(G_m(x^{(i)}) \\neq y^{(i)}) = \\sum\\limits^N_{i=1} w_{mi}I(G_m(x^{(i)}) \\neq y^{(i)}) \\tag{1}$$ (c) 计算 $G_m(x)$ 的系数 $\\alpha_m$: $$ \\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m} \\tag{2}$$ (d) 更新训练集的权重分布 $D_{m+1}$: $$ D_{m+1} = (w_{m+1, 1}, w_{m+1, 2}, \\cdots, w_{m+1, N}) $$ $$w_{m+1, i} =\\begin{cases} \\frac{w_{mi}}{Z_m} e^{-\\alpha_m}, \u0026 G_m(x^{(i)}) = y^{(i)} \\\\ \\\\ \\frac{w_{mi}}{Z_m}e^{\\alpha_m}, \u0026 G_m(x^{(i)}) \\neq y^{(i)} \\end{cases} = \\frac {w_{mi}e^{-\\alpha_m y^{(i)} G_m(x^{(i)})}} {Z_m} \\tag{3}$$ $$Z_m = \\sum\\limits^N_{i=1}w_{mi}e^{-\\alpha_m y^{(i)} G_m(x^{(i)})} $$ 这里 $Z_m$ 是规范化因子, 使得 $D_{m+1}$ 成为一个概率分布, 保证了 $\\sum\\limits^N_{i=1}w_{m+1, i} = 1$ 最后,组合所有弱分类器 $G_m(x)$: $$ f(x) = \\sum\\limits^M_{i=1} \\alpha_m G_m(x), \\quad G(x) = sign(f(x))$$ ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:1:3","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"1.4 实现细节 注意到每次迭代测试集中的样本都具有不同的权重, 实现方法有: 在每个弱分类器计算损失函数的时候, 对相应样本的loss乘以权重 缺点: 需要修改弱分类器, 在loss中引入权重 不需要修改弱分类器的方案: 直接修改训练集 每次迭代都使用 $D_i$ 作为概率分布从原始数据集中生成新的数据集进行训练 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:1:4","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"1.5 直观解释 对模型来说, 这里显示了弱分类器 $G_m(x)$的权重, $ \\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m} $ 与误差 $e_m$, 变化的关系. 显然更精确的弱分类器具有更大的权重 相反, 对训练集数据来说, 从 (3) 可知被误分类的样本的权重会增加, 被正确分类的样本权重会降低, 增加/降低的速度都是指数级别的. ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:1:5","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"第二部分: Adaboost与前向分步算法 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:2:0","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"2.1 加法模型与前向分步算法 考虑一个与adaboost相似的累加模型 (additive model) : $$ f(x) = \\sum\\limits_{m=1}^M \\beta_m b(x; \\gamma_m) $$ in which $b(x; \\gamma_m)$ 是基分类器, $\\gamma_m$ 是基分类器的参数, $\\beta_m$ 是基分类器的权重. 为了最小化损失函数, 提出前向分步算法(forward stagewise algorithm), 每一步只学习一个基函数及其系数, 即每一步中把其他所有模型看成常数, 只优化一个模型. 这是前向分步算法的核心概念. ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:2:1","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"2.2 前向分步算法 输入: 训练集 $T = \\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(N)}, y^{(N)}) \\rbrace$, 损失函数$L(y, f(x))$, 基分类器 $b(x; \\gamma)$ 输出: 累加模型 $f(x)$, 初始化 $f_0(x) = 0$ For $m = 1,2,\\cdots,M$: 最小化损失函数: $$ (\\beta_m, \\gamma_m) = arg\\min\\limits_{\\beta, \\gamma} \\sum\\limits_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + \\beta b(x^{(i)};\\gamma)) $$ 更新 $f(x)$: $$ f_m(x) = f_{m-1}(x) + \\beta_m b(x^{(i)};\\gamma_m)$$ 生成累加模型: $$ f(x) = \\sum\\limits_{m=1}^M \\beta_m b(x; \\gamma_m) $$ ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:2:2","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"2.3 Adaboost与前向分步算法 以下证明, adaboost算法实际上就是一个使用累加模型, 且损失函数为指数函数的前向分步算法 假设损失函数为指数损失函数(exponential loss function): $$ L(y, f(x)) = exp(-yf(x)) $$ 可以证明指数损失函数最小化, 则分类错误率也最小化, 说明指数损失函数是分类任务0/1损失函数的一致替代损失函数. 但它连续可微, 因此更适合优化. 在第$m$次迭代中: $$ f_m(x) = f_{m-1}(x) + \\alpha_mG_m(x), \\\\ \\text{其中 } f_{m-1}(x) = \\sum\\limits_{m=1}^{M-1} \\alpha_mG_m(x) $$ 为了最小化 $$\\begin{align} (\\alpha_m, G_m) \u0026 = arg\\min\\limits_{\\alpha, G} \\sum\\limits_{i=1}^N exp \\left\\lbrace -y^{(i)}(f_{m-1}(x^{(i)}) + \\alpha_m G_m(x^{(i)}))\\right\\rbrace \\\\ \u0026 = arg\\min\\limits_{\\alpha, G} \\sum\\limits_{i=1}^N w_{mi} exp \\left\\lbrace -y^{(i)} \\alpha_m G_m(x^{(i)}) \\right\\rbrace \\end{align} $$ 其中 $w_{mi} = exp(-y^{(i)} f_{m-1}(x^{(i)}))$, 则损失函数仅依赖于$\\alpha$ 和 $G$. 为了证明算法中的$\\alpha_m^*, G_m^*$ 与adaboost中的 $\\alpha_m, G_m$ 等价: 对任意 $\\alpha \u003e 0$, 使损失函数最小的 $G_m^*(x)$ 由下式得到: $$ G_m^*(x) = arg\\min\\limits_{G} \\sum\\limits_{i=1}^N w_{mi} I(y^{(i)} \\neq G(x^{(i)})) $$ 此分类器即为adaboost算法中的基分类器, 即 $G_m^*(x) = G_m(x)$ 求 $\\alpha_m$ $$\\begin{align} L(\\alpha, G_m) \u0026 = \\sum\\limits_{i=1}^N w_{mi} exp \\left\\lbrace -y^{(i)} \\alpha G_m(x^{(i)}) \\right\\rbrace \\\\ \u0026 = (e^{\\alpha} - e^{-\\alpha}) \\sum\\limits_{\\neq} w_{mi} + e^{-\\alpha} \\sum\\limits w_{mi} \\end{align} $$ 其中$\\sum\\limits_{\\neq}$ 是$\\sum\\limits_{y^{(i)} \\neq G(x^{(i)})}$的缩写, $\\sum$ 是$\\sum\\limits_{i=1}^N$ 的缩写 对$\\alpha$求导并使导数为0, 即得到使损失函数最小的$\\alpha$ $$\\frac {\\partial L(\\alpha, G_m)} {\\partial \\alpha} = (e^{\\alpha} + e^{-\\alpha}) \\sum\\limits_{\\neq} w_{mi} - e^{-\\alpha} \\sum\\limits w_{mi} = 0$$ s.t. $$(e^{\\alpha} + e^{-\\alpha}) \\frac{\\sum\\limits_{\\neq} w_{mi}}{\\sum\\limits w_{mi}} - e^{-\\alpha} = 0$$ s.t. $$(e^{\\alpha} + e^{-\\alpha}) \\epsilon_m - e^{-\\alpha} = 0$$ s.t. $$ \\alpha^* = \\frac{1}{2}ln \\frac{1-\\epsilon_m}{\\epsilon_m} $$ 其中 $\\epsilon_m = \\frac{\\sum\\limits_{\\neq} w_{mi}}{\\sum\\limits w_{mi}}$, 即为 $G_m^*(x)$ 的损失函数 最后我们得到 $(\\alpha_m, G_m)$ 是和adaboost中的一致的 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:2:3","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"第三部分: Code Review 项目源码见github ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:0","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.1 数据集介绍 Horse Colic Data Set 马是否得了疝气的预测 数据集大小: 68 特征数目: 24 数据集是否完整: 有缺失数据, 用0补全 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:1","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.2 项目结构 基分类器使用决策树桩 基分类器的数据结构: 采用字典结构 基分类器的训练: 损失函数为0-1损失函数, 找到最好阈值 adaboost算法 集成分类器的数据结构: 采用类保存权重list和基分类器list 算法训练 保存/载入模型的功能: 读取/保存为json文件 模型评估: 准确率和ROC曲线 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:2","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.3 基分类器:决策树桩 import numpy as np def stump_classifier(data_matrix, feature_index, threshold, rule='lt'): \"\"\" :param: data_matrix: 测试集, 样本按行排列 :param: feature_index: 用来分类的特征序号 :param: threshold: 阈值 :param: rule: 规则, 默认情况下是小于阈值被分类为-1.0 决策树桩 输入: 测试集 输出: 预测结果(以1,-1标注) \"\"\" results = np.ones((data_matrix.shape[0], 1)) feature_values = data_matrix[:, feature_index] if rule == 'lt': results[np.where(feature_values \u003c= threshold)[0]] = -1.0 elif rule == 'gt': results[np.where(feature_values \u003e threshold)[0]] = -1.0 else: print('ERROR: rule not recognized, use default as lt.') results[np.where(feature_values \u003c= threshold)[0]] = -1.0 return results ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:3","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.4 基分类器训练过程: 寻找最小损失的阈值 import numpy as np def create_stump(data_matrix, labels, data_weights, step_number=10): \"\"\" :param: data_matrix: 测试集, 样本按行排列 :param: labels: 标注 :param: data_weights: 训练集样本权重 :param: step_number: 迭代次数, 亦即设置阈值每一步的步长 :return: stump: 决策树桩, 用dict实现 :return: return_prediction: 预测的标注值 :return: min_error: 最小损失函数值 决策树桩训练函数 输入: 训练集, 训练集权重, 迭代次数 输出: 决策树桩, 输出值, 最小损失 \"\"\" m, n = np.shape(data_matrix) stump = {} return_prediction = np.zeros((m, 1)) min_error = np.inf # 遍历特征 for i in range(n): min_value = data_matrix[:, i].min() max_value = data_matrix[:, i].max() step_size = (max_value - min_value) / float(step_number) # 按步长设定阈值 for j in range(-1, step_number + 1): for rule in ['lt', 'gt']: threshold = min_value + j * step_size prediction = stump_classifier(data_matrix, i, threshold, rule) # is_error用来存放是否错误的标记, 即I(prediction = labels) is_error = np.ones((m, 1)) is_error[prediction == labels] = 0 # 损失乘以归一化的权重 weighted_error = np.dot(data_weights.T, is_error) if weighted_error \u003c min_error: min_error = weighted_error return_prediction = prediction.copy() stump['feature_index'] = i stump['threshold'] = threshold stump['rule'] = rule return stump, return_prediction, min_error ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:4","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.5 集成分类器数据结构 import numpy as np class Model: \"\"\" :return: model_weights: np数组,弱分类器权重 :return: model_list: weak model list, 其中每个弱分类器用dict实现 \"\"\" def __init__(self, size=10): self.model_weights = np.zeros((size, 1)) self.model_list = [] ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:5","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.6 Adaboost训练集成分类器 import numpy as np def adaboost_train(data_matrix, labels, iteration=40): \"\"\" :param: data_matrix: (m,n) np数组, 训练集, 样本按行排列 :param: labels: (m,1) np数组 标注 :param: iteration: int 弱分类器个数 输入训练集和弱分类器个数, 输出模型 \"\"\" number = data_matrix.shape[0] data_weights = np.ones((number, 1)) / number # 初始化训练集权重为1/number m = Model(iteration) # 初始化模型权重为0 for i in range(iteration): stump, predictions, weighted_error = st.create_stump(data_matrix, labels, data_weights) m.model_list.append(stump) m.model_weights[i] = 0.5 * math.log((1.0 - weighted_error) / max(weighted_error, 1e-16)) data_weights = data_weights * np.exp(-1.0 * m.model_weights[i] * labels * predictions) data_weights = data_weights / np.sum(data_weights) return m ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:6","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.7 集成分类器 import numpy as np def adaboost_classify(input_matrix, m): \"\"\" :param: data_matrix: (m,n) np数组,测试集, 样本按行排列 :param: m: 模型 :return: models_output: (m,1) np数组,强分类器输出值 ensemble model, 输入训练集, 返回输出结果 \"\"\" models_output = np.zeros((input_matrix.shape[0], 1)) for i in range(len(m.model_list)): model_prediction = st.stump_classifier(input_matrix, m.model_list[i]['feature_index'], m.model_list[i]['threshold'], m.model_list[i]['rule']) models_output += m.model_weights[i] * model_prediction return models_output ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:7","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.8 分类器测试函数 import numpy as np def adaboost_test(data_matrix, labels, m): \"\"\" :param: data_matrix: 测试集, 样本按行排列 :param: labels: 标注 输入测试集和模型, 输出模型参数, 输出结果和正确率, 返回输出结果 \"\"\" models_output = adaboost_classify(data_matrix, m) i_vec = (np.sign(models_output) == labels).astype(int) error_rate = 1 - np.count_nonzero(i_vec) / float(data_matrix.shape[0]) print 'model weights:'+'\\n', m.model_weights, '\\n'+'models_output:'+'\\n', models_output, '\\n'+'error rate: ', error_rate return models_output ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:8","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.9 保存模型参数到json文件 import json def save_model(m): weights_json = json.dumps(m.model_weights.tolist(), indent=4, separators=(',', ': ')) models_json = json.dumps(m.model_list, indent=4, separators=(',', ': ')) with open(\"model/model_weights.json\", 'w') as fp: fp.write(weights_json) with open(\"model/model_list.json\", 'w') as fp: fp.write(models_json) def load_model(): with open(\"model/model_weights.json\") as fp: weights_json = json.loads(fp.read()) weights_list = np.array(weights_json) size = weights_list.shape[0] model = ab.Model(size) model.model_weights = weights_list with open(\"model/model_list.json\") as fp: model_list = json.loads(fp.read()) model.model_list = model_list return model ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:9","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.10 测试用例 import numpy as np import adaboost as ab # adaboost训练脚本 import test_toolkit as tt # 测试用工具包, 主要用于数据预处理(未展示) import plot_roc as plot # 绘制ROC曲线, 评估模型优劣(未展示) # 载入数据 data_matrix = tt.load_data('test_data/horseColicTest2.txt') # 预处理, 测试集/训练集分为8:2 train_matrix, cv_matrix, test_matrix = tt.split_data(data_matrix, (0.8, 0.0)) train_matrix_x, class_vector = tt.separate_x_y(train_matrix) # ------------------1. 自己训练模型---------------------- m = ab.adaboost_train(train_matrix_x, class_vector) # -----------------2. 载入预训练模型---------------------- # m = load_model() # -------------------测试模型泛化能力--------------------- test_matrix_x, test_labels = tt.separate_x_y(test_matrix) results = ab.adaboost_test(test_matrix_x, test_labels, m) # 绘制ROC曲线 plot.plot_roc(results, test_labels) # 保存模型 save_model(m) ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:10","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"3.11 训练结果 models_output: [[ 1. -1. 1. 1. -1. -1. 1. 1. 1. 1. 1. 1. 1. 1.]] labels: [[ 1. -1. 1. 1. -1. -1. 1. 1. 1. 1. 1. 1. 1. 1.]] error rate: 0.0 AUC曲线完美, 准确率也是100%.在如此数据缺失, 且特征多的情况下还能达到这么高的效果, 显示了adaboost的强大 ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:3:11","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"第四部分 梯度提升(Gradient Boosting) ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:4:0","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"4.1 任意损失函数的Boosting 上一节里我们使用使用累加模型, 且损失函数为指数函数的前向分步算法实现了adaboost算法. 损失为指数函数时比较容易处理, 但其他损失函数就不是那么容易处理了. 损失函数的一般表示是: $$ L(y_i, f(x_i)) $$ 考虑使用前向分步算法求解一个任意损失函数: $$ (\\beta_m, \\gamma_m) = arg\\min\\limits_{\\beta, \\gamma} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i;\\gamma)) \\tag{4.1}$$ 既然 $\\beta b(x_i;\\gamma)$ 和 $f_{m-1}(x_i)$ 相比是等价无穷小量, 使用 泰勒级数 在 $f_{m-1}(x_i)$ 附近展开: $$ L \\approx \\frac{1}{N} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i)) + \\beta \\sum\\limits_{i=1}^N \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} b(x_i;\\gamma) \\tag{4.2}$$ 为 (2) 添加正则化项防止 $b(x_i;\\gamma)$ 变得太大, 既然我们已经有了 $\\beta$ 去调整这个项的大小了: $$\\begin{align} L \u0026 \\approx \\frac{1}{N} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i)) + \\frac{\\beta}{2} \\sum\\limits_{i=1}^N \\left. { 2 \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} b(x_i;\\gamma) + b^2(x_i;\\gamma) \\\\ \\text{(Strip the constants)} \u0026 = \\beta \\sum\\limits_{i=1}^N 2 \\frac{\\partial L}{\\partial s} b(x_i;\\gamma) + b^2(x_i;\\gamma) \\\\ \u0026 = \\beta \\sum\\limits_{i=1}^N (b(x_i;\\gamma) + \\frac{\\partial L}{\\partial s})^2 - (\\frac{\\partial L}{\\partial s} )^2 \\end{align} \\tag{4.3}$$ ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:4:1","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"4.2 Gradient Boosting 现在可以最小化损失函数: 求解 $b(x_i;\\gamma)$. 从 (3) 可知: $$\\gamma_m = arg\\min\\limits_\\gamma \\beta \\sum\\limits_{i=1}^N \\left(b(x_i;\\gamma) + \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} \\right)^2$$ 也就是在每一步 $m$ 中, 利用损失函数的梯度 $-\\frac{\\partial L(y_i, s)}{\\partial s}$ 训练基分类器 $b(x_i;\\gamma_m)$. 这就是为什么它被称为梯度提升算法 $$ \\text{fit } b(x_i;\\gamma_m) = - \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)}$$ 求解 $\\beta$ $$\\beta_m = arg\\min\\limits_\\beta \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i;\\gamma_m))$$ 既然我们已经有了 $y_i$, $f_{m-1}(x_i)$ 和 $b(x_i;\\gamma_m)$, 那原问题就变成了一个简单的一维变量最优化问题, 那就很容易解决了 整个算法的思想很简单, 最常用的基分类器是决策树, 称为gradient boosting decision tree (GBDT) ","date":"2017-03-24","objectID":"/2017-03-24-adaboost/:4:2","tags":["boosting","machine learning","python"],"title":"Adaboost算法 + python实现","uri":"/2017-03-24-adaboost/"},{"categories":null,"content":"旧文章的翻译, 主要参考自李航的统计学习方法一书. ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:0:0","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"第一部分 基本概念 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:1:0","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"1.1 超平面(Hyperplane) 考虑一个二分类问题: $$\\text{Training set: } T = \\left \\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) …(x^{(N)}, y^{(N)}) \\right \\rbrace \\\\ \\text{in which }x^{(i)} \\in R^n, y^{(i)} \\in \\lbrace +1, -1 \\rbrace, i=1,2…N$$ 假设样本空间线性可分, 则有超平面: $$ w \\cdot x + b = 0 \\text{, in which }w, b \\in R^N$$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:1:1","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"1.2 间隔(Margin) 在希尔伯特空间中, 点到平面的距离是: $$ \\gamma^{(i)} = \\left \\vert \\frac{w}{\\Vert w \\Vert} \\cdot x^{(i)} + \\frac{b}{\\Vert w \\Vert} \\right \\vert \\tag{1.1}$$ 为了去掉绝对值, 令在超平面法向量一侧的$x^{(i)}$标记为+1, 另一侧的标记为-1, $\\gamma^{(i)}$ 可以被定义为: $$ \\gamma^{(i)} = y^{(i)} \\left ( \\frac{w}{\\Vert w \\Vert} \\cdot x^{(i)} + \\frac{b}{\\Vert w \\Vert} \\right ) \\tag{1.2}$$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:1:2","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"1.3 寻找更好的分隔 预测函数 (Predicting function) 换句话说如果我们已经找到了超平面 $ w^{*} \\cdot x + b^{*} = 0 $, 给定一个样本点 $x^{(i)}$, 预测函数是: $$y^{(i)} = f(x^{(i)}) = sign(w^{*} \\cdot x^{(i)} + b^{*}) \\tag{1.3}$$ 几何间隔 (Geometric margin) 几何间隔就是点到平面的最大距离: $$ \\gamma = \\min \\limits_{i = 1,2…N} \\quad \\gamma^{(i)} \\tag{1.4}$$ 函数间隔 (Functional margin) 函数间隔被定义为: $$ \\hat \\gamma^{(i)} = y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) = {\\Vert w \\Vert} \\gamma^{(i)} \\tag{1.5}$$ 函数间隔可以被考虑为分类的置信度, 可以被用来简化运算 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:1:3","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"1.4 硬间隔最大化: 找到最大间隔是SVM的目标 $$ \\begin{align} \u0026 \\max \\limits_{w,b} \\quad \\gamma \\\\ \u0026 s.t. \\quad y^{(i)} \\left ( \\frac{w}{\\Vert w \\Vert} \\cdot x^{(i)} + \\frac{b}{\\Vert w \\Vert} \\right ) \\ge \\gamma , \\quad i=1,2,…,N \\tag{1.6} \\end{align}$$ 使用函数间隔, 原问题可以被定义为: $$ \\begin{align} \u0026 \\max \\limits_{w,b} \\quad \\frac{\\hat \\gamma}{\\Vert w \\Vert} \\\\ \u0026 s.t. \\quad y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) \\ge \\hat \\gamma , \\quad i=1,2,…,N \\end{align} \\tag{1.7}$$ 给定一个超平面, $\\Vert w \\Vert$ 和 $\\Vert b \\Vert$ 是可变的: $w$ 和 $b$ 只要保持比例相同,则超平面不变. 为了限制这两个变量,令 $\\hat\\gamma = 1$(这样就可以让 $\\Vert w \\Vert$ 保持不变), 问题最终被定义为: $$ \\begin{align} \u0026 \\min \\limits_{w,b} \\quad \\frac{1}{2} {\\Vert w \\Vert}^2 \\\\ \u0026 s.t. \\quad y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) - 1 \\ge 0, \\quad i=1,2,…,N \\end{align} \\tag{1.8}$$ $\\min \\limits_{w,b} \\frac{1}{2} {\\Vert w \\Vert}^2$ 和原始问题 $ \\max \\limits_{w,b} \\frac{1}{\\Vert w \\Vert} $ 是等价的. ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:1:4","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"1.6 软间隔最大化 如果样本空间线性不可分, 为每个样本设置一个松弛变量 $\\xi \\in R^n$ 那么 (1.8) 成为: $$ y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) + \\xi_i - 1 \\ge 0 \\tag{1.9}$$ 对每个样本支付代价 $\\xi_i$ 目标函数变为: $$ \\frac{1}{2} {\\Vert w \\Vert}^2 + C \\sum\\limits_{i=1}^{N}\\xi_i \\tag{1.10}$$ $C$ 称为惩罚参数, 更大的$C$ 意味着对误分类的样本惩罚更大 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:1:5","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"1.7 Support Vector (支持向量) 顾名思义, 支持向量是\"支持\"超平面的向量 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:1:6","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"第二部分: 解决硬间隔最大化问题 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:2:0","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"2.1 解决硬间隔最大化问题 (1.8) 是个凸二次规划问题 使用拉格朗日乘子法, (1.8) 可以表示为: $$ L(w,b,\\alpha) = \\frac{1}{2} {\\Vert w \\Vert}^2 + \\sum\\limits_{i=1}^{N} \\alpha_i (1-y^{(i)}(w \\cdot x^{(i)} + b)), \\alpha_i \\ge 0 \\tag{2.1}$$ 他的对偶问题是: $$\\max\\limits_{\\alpha} \\ \\min\\limits_{w,b} L(w,b,\\alpha) \\tag{2.2}$$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:2:1","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"2.2 求解 $\\min\\limits_{w,b} L(w,b,\\alpha)$ $$ \\nabla_w L(w,b,\\alpha) = w - \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} x^{(i)} = 0 \\\\ \\nabla_b L(w,b,\\alpha) = \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0 \\tag{2.3}$$ 组合(2.3) 和 (2.1), 得到如下对偶问题: $$ \\begin{align} \u0026 \\max\\limits_{\\alpha} \\left\\lbrace - \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} (x^{(i)} \\cdot x^{(j)}) + \\sum\\limits_{i=1}^{N} \\alpha_i \\right\\rbrace \\\\ \u0026 s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\alpha_i \\ge 0 \\end{align} \\tag{2.4}$$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:2:2","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"2.3 求解 $\\alpha$ $$ \\begin{align} \u0026 \\min\\limits_{\\alpha} \\left\\lbrace \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} (x^{(i)} \\cdot x^{(j)}) - \\sum\\limits_{i=1}^{N} \\alpha_i \\right\\rbrace \\\\ \u0026 s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\alpha_i \\ge 0 \\end{align} \\tag{2.5}$$ $\\alpha^{*}$ 需要满足KKT条件 (2.3), (2.4) 和 $ \\alpha_i^{*} (1-y^{(i)}(w^{*} \\cdot x^{(i)} + b^{*})) = 0 $ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:2:3","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"2.4 求解 $w,b$ 一旦 $\\alpha^{*}$ 已经求解出, 那么: $$ w^{*} = \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} x^{(i)} \\\\ b^{*} = y^{(j)} - \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} (x^{(i)} \\cdot x^{(j)}) \\tag{2.6}$$ 其中$(x^{(j)}, y^{(j)})$ 满足: $ 1-y^{(j)}(w^{*} \\cdot x^{(j)} + b^{*}) = 0 $, 这意味着该点是支持向量 注意到当 $\\alpha_i = 0$, $(x^{(i)}, y^{(i)})$ 对 $w$ 和 $b$ 没有任何贡献, 这就意味着只有少数满足$\\alpha_i \u003e 0$, 即 $ 1-y^{(i)}(w^{*} \\cdot x^{(i)} + b^{*}) = 0 $ 的支持向量决定了超平面. ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:2:4","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"第三部分: 解决软间隔最大化问题 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:3:0","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"3.1 求解 $\\min\\limits_{w,b} L(w,b,\\xi,\\alpha,\\mu)$ 二次凸优化问题是: $$ \\begin{align} \u0026 \\min\\limits_{w, \\xi} \\quad \\frac{1}{2} {\\Vert w \\Vert}^2 + C \\sum\\limits_{i=1}^{N}\\xi_i \\\\ s.t. \\quad \u0026 y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) + \\xi_i - 1\\ge 0 \\ \u0026 \\xi_i \\ge 0\\tag{1.10} \\end{align}$$ 对(1.10)使用拉格朗日乘子法, (1.10) 可以被表示为: $$ \\begin{align} \u0026 L(w,b,\\xi,\\alpha,\\mu) = \\frac{1}{2} {\\Vert w \\Vert}^2 + C \\sum\\limits_{i=1}^{N}\\xi_i + \\sum\\limits_{i=1}^{N} \\alpha_i (1 -\\xi_i - y^{(i)}(w \\cdot x^{(i)} + b)) + \\sum\\limits_{i=1}^{N} \\mu_i(-\\xi_i) \\tag{3.1} \\end{align}$$ $$ \\nabla_w L(w,b,\\xi,\\alpha,\\mu) = w - \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} x^{(i)} = 0 \\\\ \\nabla_b L(w,b,\\xi,\\alpha,\\mu) = \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0 \\\\ \\nabla_{\\xi_i} L(w,b,\\xi,\\alpha,\\mu) = C-\\alpha_i - \\mu_i = 0 \\tag{3.2}$$ 幸运的是他们和 (2.4) 以及 (2.5) 有相同的形式 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:3:1","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"3.2 求解 $\\alpha$ 软间隔最大化的对偶问题是: $ \\begin{align} \u0026 \\min\\limits_{\\alpha} \\quad \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} (x^{(i)} \\cdot x^{(j)}) - \\sum\\limits_{i=1}^{N} \\alpha_i \\end{align} \\tag{3.2}$ $ s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\quad \\alpha_i \\ge 0, \\quad \\mu_i \\ge 0, \\quad C- \\alpha_i-\\mu_i =0 \\tag{3.3} $ (3.3) 可以被简化为: $$ s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\quad 0 \\le \\alpha_i \\le C \\tag{3.4}$$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:3:2","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"3.3 求解 $w,b$ 一旦 $\\alpha^{*}$ 被求解, 有: $$ w^{*} = \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} x^{(i)} \\\\ b^{*} = y^{(j)} - \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} (x^{(i)} \\cdot x^{(j)}) \\tag{3.5}$$ 其中$(x^{(j)}, y^{(j)})$ 满足: $ 0 \\le \\alpha_j \\le C $, 亦即 $ 1 - \\xi_i -y^{(j)}(w^{*} \\cdot x^{(j)} + b^{*})) = 0 $, 意味着这些向量满足在分类边界的区间之内. ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:3:3","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"3.4 SVM的损失函数: 合页损失函数(Hinge loss function) 令 $\\quad [ 1 - y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) ]_+ = \\xi_i $, (1.10) 可以被表达为 $$\\min\\limits_{w,b} \\quad \\sum\\limits_{i=1}^N \\xi_i + \\lambda {\\Vert w \\Vert}^2 \\tag{3.6}$$ 令 $\\lambda = \\frac{1}{2C}$, 和 (1.10) 是等价的, 那么损失函数可以被表示为: $$ \\sum\\limits_{i=1}^N[ 1 - y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) ]_+ + \\lambda {\\Vert w \\Vert}^2 \\tag{3.7}$$ 称合页损失函数 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:3:4","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"第四部分: 序列最小最优化算法 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:4:0","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"4.1 问题描述 优化问题是: $ \\begin{align} \u0026 \\min\\limits_{\\alpha} \\quad \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} K(x^{(i)}, x^{(j)}) - \\sum\\limits_{i=1}^{N} \\alpha_i \\end{align} \\tag{3.2}$ $ s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\quad 0 \\le \\alpha_i \\le C \\tag{3.4}$ 为了高效地解决这个优化问题, 引入序列最小最优化算法(Sequantial Minimal Optimization) ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:4:1","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"4.2 序列化求解凸二次规划问题 假设在所有 $\\alpha_i$ 中, 只把 $\\alpha_1, \\alpha_2$ 当成变量, 所有其他 $\\alpha_i(i \\neq 1, 2)$ 都是常量, $\\alpha_2^{i}$ 是第 $\\alpha_2$ 的第$i$次迭代结果, 为了表示方便首先定义一些变量: $$ L = \\begin{cases} max\\lbrace 0, \\alpha_2^i - \\alpha_1^i \\rbrace, \u0026 y^{(1)} \\neq y^{(2)} \\\\ max\\lbrace 0, \\alpha_2^i + \\alpha_1^i - C \\rbrace, \u0026 y^{(1)} = y^{(2)} \\end{cases} $$ $$ H = \\begin{cases} min \\lbrace C, \\alpha_2^i - \\alpha_1^i + C \\rbrace, \u0026 y^{(1)} \\neq y^{(2)} \\\\ min \\lbrace C, \\alpha_2^i + \\alpha_1^i \\rbrace, \u0026 y^{(1)} = y^{(2)} \\end{cases} $$ $$E_i = g(x^{(i)}) - y^{(i)} = \\left( \\sum\\limits_{j=1}^{N} \\alpha_j y^{(j)} K(x^{(i)}, x^{(j)})+b \\right)- y^{(i)} \\tag{SMO.1}$$ 那么利用 $H$ 和 $L$ 去约束 $\\alpha_2$, 在第 $(i+1)$ 次迭代中: $$ \\alpha_2^{i+1, unc} = \\alpha_2^{i} + \\frac { y^{(2)}(E_1 - E_2)}{K_{11}+K_{22}-2K_{12}} \\tag{SMO.2}$$ $$ \\alpha_2^{i+1} = \\begin{cases} H, \u0026 \\alpha_2^{i+1, unc} \u003e H \\\\ \\alpha_2^{i+1, unc}, \u0026 L \\le \\alpha_2^{i+1, unc} \\le H \\\\ L, \u0026 \\alpha_2^{i+1, unc} \u003c L \\end{cases} \\tag{SMO.3} $$ $$ \\alpha_1^{i+1} = \\alpha_1^{i} + y^{(1)}y^{(2)}(\\alpha_2^{i} - \\alpha_2^{i+1}) \\tag{SMO.4} $$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:4:2","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"4.3 启发式选择 $\\alpha_1, \\alpha_2$ 对每个 $\\alpha_i$, KKT约束是: $$ \\begin{align} \\alpha_i = 0 \\quad \u0026 \\Leftrightarrow \\quad y^{(i)} g(x^{(i)}) \\ge 1 \\quad \\text{(Out of the border)} \\\\ 0 \u003c \\alpha_i \u003c C \\quad \u0026 \\Leftrightarrow \\quad y^{(i)} g(x^{(i)}) = 1 \\quad \\text{(On the border)} \\\\ \\alpha_i = C \\quad \u0026 \\Leftrightarrow \\quad y^{(i)} g(x^{(i)}) \\le 1 \\quad \\text{(Inside the border)} \\end{align} \\tag{SMO.5} $$ 选择 $\\alpha_1$: 遍历所有满足$0 \u003c \\alpha_i \u003c C$的点, 检查他们是否满足KKT条件, 若满足则再遍历整个训练集 选择 $\\alpha_2$: 寻找一个样本 $(x^{(i)}, y^{(i)})$ 使得 $\\vert E^i_1 - E^i_2 \\vert$ 有最大的变动. 为了节省运算量, 提前缓存 $E_i$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:4:3","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"4.4 计算新的 $b$ 和 $E_i$ 计算 $b$ $$ b_1^{i+1} = b^i -[E_1^i + y^{(1)}K_{11}(\\alpha_1^{i+1} - \\alpha_1^{i}) + y^{(2)}K_{21}(\\alpha_2^{i+1} - \\alpha_2^{i}) ] \\\\ b_2^{i+1} = b^i -[E_2^i + y^{(1)}K_{12}(\\alpha_1^{i+1} - \\alpha_1^{i}) + y^{(2)}K_{22}(\\alpha_2^{i+1} - \\alpha_2^{i}) ] \\tag{SMO.6} $$ $$b^{i+1} = \\begin{cases} b_{1}^{i+1} = b_{2}^{i+1}, \u0026 0 \u003c \\alpha_1^{i+1}, \\alpha_2^{i+1} \u003c C \\\\ \\frac {1}{2} (b_{1}^{i+1} + b_{2}^{i+1}), \u0026 \\alpha_1^{i+1}, \\alpha_2^{i+1} = 0 ; or ; C \\end{cases} $$ 2. Calculate $E_i$ using $\\alpha_1^{i+1}, \\alpha_2^{i+1}, b^{i+1}$ $$ E_i = \\sum\\limits_{j \\in S} \\alpha_j y^{(j)} K(x^{(i)}, x^{(j)})+b^{i+1} - y^{(i)} $$ ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:4:4","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"第五部分: 核函数方法 施工中 ","date":"2017-03-24","objectID":"/2017-03-24-svm-cn/:5:0","tags":["SVM","machine learning"],"title":"支持向量机(SVM)","uri":"/2017-03-24-svm-cn/"},{"categories":null,"content":"朴素贝叶斯(Naive Bayes) 如此蛤意盎然的算法, 居然一直没写 ","date":"2017-03-22","objectID":"/2017-03-22-naive-bayes/:0:0","tags":["bayes","machine learning"],"title":"朴素贝叶斯","uri":"/2017-03-22-naive-bayes/"},{"categories":null,"content":"原理 训练集: $T = \\lbrace (X^{(1)}, y_1), (X^{(2)}, y_2), \\cdots, (X^{(N)}, y_N) \\rbrace$, 其中 $ X = (X_1, X_2, \\cdots, X_n), y \\in \\lbrace c_1, c_2, \\cdots, c_K \\rbrace $ 学习过程: 根据贝叶斯模型, 需要学习的参数有 $$ P(y = c_k) = \\frac{\\sum\\limits^{N}_{i=1}I(y_i = c_k)}{N} \\tag{1}$$ $$ \\begin{align} P(X_j = X^{(i)}_j | y = c_k) \u0026 = \\frac{P(X_j = X^{(i)}_j, y = c_k)}{P(y = c_k)} \\\\ \u0026 = \\frac{\\sum\\limits^{N}_{i=1}I(X_j = X^{(i)}_j, y_i = c_k)} {\\sum\\limits^{N}_{i=1}I(y_i = c_k)} \\end{align} \\tag{2} $$ 此外, $$ P(X = X^{(i)} | y = c_k) = \\prod\\limits^{n}_{j=1} P(X_j = X^{(i)}_j | y = c_k) \\tag{3}$$ 其中(3)式利用了朴素贝叶斯的条件独立性假设 预测过程: 对于给定的实例$X = A$ $$ \\begin{align} P(y = c_k | X = A) \u0026= \\frac{P(X = A | y = c_k)P(y = c_k)}{P(X = A)} \\\\ \u0026 = \\frac{P(X = A | y = c_k)P(y = c_k)}{\\sum\\limits_{k} P(X = A | y = c_k)P(y = c_k)} \\end{align}$$ 由于分母对任意$c_k$都一样, 因此贝叶斯分类结果是 $$ y = arg \\max\\limits_{c_k} P(X = A | y = c_k)P(y = c_k) \\tag{4}$$ ","date":"2017-03-22","objectID":"/2017-03-22-naive-bayes/:1:0","tags":["bayes","machine learning"],"title":"朴素贝叶斯","uri":"/2017-03-22-naive-bayes/"},{"categories":null,"content":"贝叶斯估计 如果样本比较少, 出现$P(X_j = X^{(i)}_j | y = c_k) = 0$ 的情况, 那整个$P(X = X^{(i)} | y = c_k) = 0$, 这会影响后验概率的计算结果. 使用常数项并保持概率分布的特性可以解决这一问题 $$ P_{\\lambda}(X_j = X^{(i)}_j | y = c_k) = \\frac{\\sum\\limits^{N}_{i=1}I(X_j = X^{(i)}_j, y_i = c_k) + \\lambda}{\\sum\\limits^{N}_{i=1}I(y_i = c_k) + S_j\\lambda} $$ $$ P(y = c_k) = \\frac{\\sum\\limits^{N}_{i=1}I(y_i = c_k) + \\lambda}{N + K\\lambda} $$ 其中$S_j$代表$X$的第$j$个特征$X_j$有多少种可能的取值. 一般$\\lambda$取1, 称为拉普拉斯平滑 ","date":"2017-03-22","objectID":"/2017-03-22-naive-bayes/:2:0","tags":["bayes","machine learning"],"title":"朴素贝叶斯","uri":"/2017-03-22-naive-bayes/"},{"categories":null,"content":"挺有意思的, 存在blog里参考, 以后说不定用得上. 不过这个作业应该有点历史了, 出处未考 USTC Spring 2017: Data Mining Assignment 1 SA16225220 ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:0:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 1 Classify the following attributes as binary, discrete, or continuous. Also classify them as qualitative (nominal or ordinal) or quantitative (interval or ratio). Some cases may have more than one interpretation, so briefly indicate your reasoning if you think there may be some ambiguity. Example: Age in years. Answer: Discrete, quantitative, ratio Speed of a vehicle measured in mph. Continuous Quantitiative Ratio Explanation: It possesses a meaningful (unique and non-arbitrary) zero value. 0 mph means motionless, 2 mph is twice as fast as 1 mph. Altitude of a region. Continuous Quantitiative Interval Explanation: Zero altitude is not fixed, it’s defined relatively so I’d rather take it as interval, which means at an autitude of 20m doesn’t mean it’s twice as tall as 10m. Intensity of rain as indicated using the values: no rain, intermittent rain, incessant rain. Discrete Qualitative Ordinal Explanation: These 3 values are comparable. Brightness as measured by a light meter. Continuous Quantitiative Ratio Explanation: When measured as lumen, 0 lumen means pure darkness, so it possesses a meaningful (unique and non-arbitrary) zero value. Barcode number printed on each item in a supermarket. Discrete Qualitative Nominal Explanation: Not comparable numbers 注: 主要是nominal, ordinal, interval 和 ratio四个measurements的比较, 其中ratio在零点有意义, interval在零点无意义所以只可相比, 没有倍数关系. 例如 20摄氏度不比10摄氏度热两倍, 但是用开尔文就可以, 所以摄氏度是interval, 开尔文是ratio ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:1:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 2 The population for a clinical study has 500 Asian, 1000 Hispanic and 500 Native American people. What is good way of sampling this population to ensure that the distribution of various sub- populations is maintained if only 100 samples have to be chosen? Give the distribution of the various sub-populations in the final sample. Answer: Asian: 25 Hispanic: 50 Native American: 25 注: 没啥好说的, 按比例分配样本 ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:2:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 3 Justify your answers for the following: Is the Jaccard coefficient for two binary strings (i.e., string of 0s and 1s) always greater than or equal to their cosine similarity? Answer: Assuming 2 strings could be represented as $A = (a_1, a_2, \\cdots, a_n)$, $B = (b_1, b_2, \\cdots, b_n)$, in which $a, b \\in \\lbrace 0, 1\\rbrace$, then we have: $$ J(A, B) = \\frac{M_{11}}{M_{01}+M_{10}+M_{11}} = \\frac{A \\cdot B}{n}$$ $$cos\\theta =\\frac{A \\cdot B}{|A||B|} \\geq \\frac{A \\cdot B}{\\sqrt{n}\\sqrt{n}} = \\frac{A \\cdot B}{n}$$ So $$cos\\theta \\geq J(A, B)$$ So it’s WRONG 注: 一个意义不太大的不等式证明, 主要是介绍了二分类下的Jaccard coefficient The cosine measure can range between [-1,1]. Give an example of a type of data for which the cosine measure will always be non-negative. Answer: Since $$cos\\theta =\\frac{A \\cdot B}{|A||B|}$$ Let $A$ \u0026 $B$ be the opposite direction, such as $$ A = -B $$ then we could always have non-negative value -1: $$ cos\\theta =\\frac{A \\cdot (-A)}{|A||-A|} = -1 $$ 注: 只要两个向量夹角是钝角就行了. 高中知识. ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:3:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 4 The similarity between two undirected graphs G1 and G2 that have the same n vertices can be defined using: $$ S(G_1, G_2) = \\frac{\\sum_i min(deg(v_i \\in G_1), deg(v_i \\in G_2))}{2 \\times max(|G_1|, |G_2|)} $$ where $deg(v ∈ G)$ indicates the degree of a vertex $v$ in graph $G$ and $|G|$ indicates the number of edges in $G$. If $S(G1, G2) = 1$, are the two graphs equivalent? Provide an example to justify your answer. Answer: No. Example: Same degree for each vertex, same number of edges in each graph, $S(G_1, G_2) = \\frac{12}{12} = 1$, but the two graphs are not equivalent. 注: 每个顶点的度数相同, 边也相同那么这里的相似度就是1, 但两个图并不一定一样. 只要举个反例就行了. ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:4:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 5 For every item i in a grocery store, a set $s_i$ is used to represent the IDs of transactions in which $i$ is purchased. Assume that the data set to be analyzed contains hundreds of thousands of such transactions. In order to analyze the proximity between any two of these sets $s_i$ and $s_j$, which measure, Jaccard or Hamming, would be more appropriate and why ? Answer: Since we want to find the similarity between 2 sets, Jaccard coefficient is more preferable. In order to analyze the proximity between any two of these sets $s_i$ and $s_j$ for items i and j that are often brought together (example: milk, bread), which measure, Jaccard or Hamming, would be more appropriate and why ? Answer: Since item $i$ and $j$ that are often brought together, they have high similarity, then we should focus on the difference between these 2 sets. So Hamming distance is more preferable. 注: Jaccard主要关心相似度, Hamming主要关注不同, 知道这点这题就没什么了. 注意这两者的适用背景即可. ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:5:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 6 For the data set described below, give an example of the types of data mining questions that can be asked (one for each classification, clustering, association rule mining, and anomaly detection task) and the description of the data matrix (what are the rows and columns). If necessary, briefly explain the features that need to be constructed. Note that, depending on your data-mining question, the row and column definitions may be different. A clinical dataset containing various measures like temperature, blood pressure, blood glucose and heart rate for each patient during every visit, along with the diagnosis information. Answer: DM Task: Classification of disease Question: Which disease does the patient have? Row: The body measures of a patient. Column: Temperature, blood pressure, blood glucose, heart rate, etc. for each patient. DM Task: Clustering similar patients Question: What are the patients with similar disease? Row: A patient’s diagnosis information. Column: Temperature, blood pressure, blood glucose, heart rate, etc. for each patient. DM Task: Association rule mining Question: What are the diseases tend to appear together? For example, high blood pressure and heart attack. Row: A patient’s diagnosis information. Column: Diagnosis information, especially the disease names. DM Task: Anomaly detection Question: Is the patient’s body measures mistaken? For example, mistaken by typo during input, like temperature = 50℃ Row: The body measures of a patient. Column: Temperature, blood pressure, blood glucose, heart rate, etc. for each patient. 注: 主要是举点数据挖掘例子 USTC Spring 2017: Data Mining Homework 2 ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:6:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 1 [30 points: 5 for each part] Consider the training examples shown in Table 1 for a binary classification problem. (a) Compute the Entropy for the overall collection of training examples. Answer: $$\\begin{align} H(Class) \u0026= -\\sum_{C0,C1} (P_{C0}logP_{C0} + P_{C1}logP_{C1}) \\\\ \u0026 = -(0.5log0.5 + 0.5log0.5) \\\\ \u0026= 1 \\end{align}$$ (b) Compute the Entropy for the Movie ID attribute. Answer: Each Movie ID has its distinct number of ID, so apparently it has no contribution to the infomation gain of Class, so $$H(Class | Movie ID) = 0$$ (c) Compute the Entropy for the Format attribute. Answer: $$\\begin{align} \u0026 H(Class | Format = DVD) \\\\ \u0026= -\\sum_{C0,C1} (P_{C0, DVD}logP_{C0, DVD} + P_{C1, DVD}logP_{C1, DVD}) \\\\ \u0026 = -(\\frac{6}{8}log\\frac{6}{8} + \\frac{2}{8}log\\frac{2}{8}) \\\\ \u0026= 0.8112 \\end{align}$$ $$\\begin{align} \u0026 H(Class | Format = Online) \\\\ \u0026= -\\sum_{C0,C1} (P_{C0, Online}logP_{C0, DVD} + P_{C1, Online}logP_{C1, Online}) \\\\ \u0026 = -(\\frac{4}{12}log\\frac{4}{12} + \\frac{8}{12}log\\frac{8}{12}) \\\\ \u0026= 0.9182 \\end{align}$$ $$ \\begin{align} \u0026 H(Class|Format)\\\\ \u0026 = P_{DVD}H(Class | DVD) + P_{Online}H(Class | Online) \\\\ \u0026 = \\frac{8}{20} \\times 0.8112 + \\frac{12}{20} \\times 0.9182 \\\\ \u0026 = 0.8754 \\end{align} $$ (d) Compute the Entropy for the Movie Category attribute using multiway split. Answer: $$\\begin{align} \u0026 H(Class | Entertainment) \\\\ \u0026= -\\sum_{C0,C1} (P_{C0, En}logP_{C0, En} + P_{C1, En}logP_{C1, En}) \\\\ \u0026 = -(\\frac{1}{4}log\\frac{1}{4} + \\frac{3}{4}log\\frac{3}{4}) \\\\ \u0026= 0.8112 \\end{align}$$ Similarly, $$ H(Class | Comedy) = 0.5435 \\\\ H(Class | Documentation) = 0.8112 $$ So $$ \\begin{align} \u0026 H(Class| Movie Category)\\\\ \u0026 = P_{En}H(Class | En) + P_{Com}H(Class | Com) + P_{Doc}H(Class | Doc)\\\\ \u0026 = 0.2 \\times 0.81125 + 0.4 \\times 0.5435+ 0.4 \\times 0.81125 \\\\ \u0026 = 0.7042 \\end{align} $$ (e) Which of the three attributes has the lowest Entropy? Answer: Movie ID which is 0 (f) Which of the three attributes will you use for splitting at the root node? Briefly explain your choice. Answer: As for infomation gain, $$\\begin{align} H(Class) - H(Class|Movie ID) \u0026 = 1 \\\\ H(Class) - H(Class|Format) \u0026 = 1 - 0.8754 = 0.125 \\\\ H(Class) - H(Class|Movie Category) \u0026 = 1 - 0.7042 = 0.296 \\end{align} $$ Choose Movie Category is better because it has more info gain, although Movie ID has the most infomation gain (1), but it has overfitting. 注: 关于条件熵和信息增益, 参考之前的blog ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:7:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 2 [19 points: 6+5+8] Consider the decision tree shown in Figure 1, and the corresponding training and test sets in Tables 2 and 3 respectively. (a) Estimate the generalization error rate of the tree using both the optimistic approach and the pessimistic approach. While computing the error with pessimistic approach, to account for model complexity, use a penalty value of 2 to each leaf node. Answer: Optimistic error rate: 0 Pessimistic error rate: $$ ErrorRate = \\frac {e(T) + \\Omega(T)}{N_t} = \\frac{0 + 6 \\times 2}{15} = 0.8$$ (b) Compute the error rate of the tree on the test set shown in Table 3 Answer: Since (0,0,0,0) and (0,0,0,1) are wrong, $$ ErrorRate = \\frac{4+3}{15} = 0.47$$ (c) Comment on the behaviour of training and test set errors with respect to model complex- ity. Comment on the utility of incorporating model complexity in building a predictive model. Answer: With model complexity increasing, the performance of the model will increase on the training set but decrease on the testing set Pruning the data through pessimistic error rate will get a better performance on testing set, as well as a simpler model, less computing complexity. 注: 关于剪枝的判断, 主要是叶节点个数(复杂度)和分类错误率 ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:8:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 3 [20 points: 10 for each part] Given the data sets shown in Figure 2, explain how the decision tree and k-nearest neighbor (k-NN) classifiers would perform on these data sets. Answer: Decision tree: Discriminating attributes and noise attributes make decision tree prone to overfitting k-NN: If k is too big, it will label the central samples as A rather than B, which is the right label. ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:9:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 4 [16 points: 4 for each part] Consider the problem of predicting if a given person is a defaulted borrower (DB) based on the attribute values: Home Owner = Yes, No Marital Status = Single, Married, Divorced Annual Income = Low, Medium, High Currently Employed = Yes, No Suppose a rule-based classifier produces the following rules: Home Owner = Yes → DB=Yes Marital Status = Single → DB = Yes Annual Income = Low → DB = Yes Annual Income = High, Currently Employed = No → DB = Yes Annual Income = Medium, Currently Employed = Yes → DB = No Home Owner = No, Marital Status = Married → DB = No Home Owner = No, Marital Status = Single → DB = Yes Answer the following questions. Make sure to provide a brief explanation or an example to illustrate the answer. (a) Are the rules mutually exclusive ? Answer: No. Example: Home Owner = No, Marital Status = Single, Annual Income = Low/High → DB = Yes (b) Is the rule set exhaustive ? Answer: No. There is no rule for Marital Status = Divorced (c) Is ordering needed for this set of rules ? Answer: Better have an order to avoid conflict: Home Owner, Marital Status, Annual Income, Currently Employed (d) Do you need a default class for the rule set ? Answer: Yes, since it’s not set exhaustive. 注: 基于规则的方法 ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:10:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"Question 5 [15 points] Consider the problem of predicting whether a movie is popular given the following attributes: Format (DVD/Online), Movie Category (Comedy/Documentaries), Release Year, Number of world-class stars, Director, Language, Expense of Production and Length. If you had to choose between RIPPER and a k-nearest neighbor classifier, which would you prefer and why? Briefly explain why the other one may not work so well? Answer: I’d prefer RIPPER. RIPPER is a rule based method. It has a good performance on noisy and unbalanced data. Apparently the number of popular films is far less than the non-popular ones so the training set would be unbalanced as well as noisy. Since the attributes has different types and the number of attributes is relatively big, using k-NN would couse overfitting on the training set if its popularity is not large enough, or even fail to have reasonable cluster if the feature space is too sparse. (like empty cluster, etc.) ","date":"2017-03-18","objectID":"/2017-03-18-data-mining-assn1/:11:0","tags":null,"title":"存档-数据挖掘作业","uri":"/2017-03-18-data-mining-assn1/"},{"categories":null,"content":"主要参考了这篇文章 How To Serve Flask Applications with uWSGI and Nginx on Ubuntu 14.04 ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:0:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"安装nginx + uwsgi + flask sudo apt-get update sudo apt-get install python-pip python-dev nginx 启动virtualenv, 以下操作都在virtualenv中进行 pip install uwsgi flask ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:1:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"下载flask应用代码 git clone your_flask_git_repo ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:2:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"在repo下新建wsgi.py, 内容如下: from visualization import app as application if __name__ == \"__main__\": application.run() 注: 把app换成application否则可能会出错 ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:3:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"在repo下新建uWSGI配置文件visualization.ini(文件名任意), 内容如下: [uwsgi] module = wsgi master = true processes = 5 socket = myproject.sock chmod-socket = 660 vacuum = true die-on-term = true 注: 关键是socket, 这是和nginx沟通的unix端口 ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:4:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"配置nginx sudo vim /etc/nginx/sites-available/myproject server { listen 80; server_name server_domain_or_IP; location / { include uwsgi_params; uwsgi_pass unix:/home/user/myproject/myproject.sock; } } 注: server_domain_or_IP需要填写服务器ip unix:/home/user/myproject/myproject.sock;段需要填写之前visualization.ini中的sock文件绝对地址 ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:5:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"启动nginx服务器 先建立软链接: sudo ln -s /etc/nginx/sites-available/myproject /etc/nginx/sites-enabled 检查conf文件的正确性: sudo nginx -t 启动: sudo service nginx restart 此时访问服务器的80端口会看到502错误, 因为uWSGI还没启动 ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:6:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"启动uWSGI服务 uwsgi --ini \u003cini文件的绝对地址\u003e --daemonize \u003clog文件的绝对地址\u003e 在后台运行, 会将log输出到规定的文件 再刷新一下地址就可以看到结果了 ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:7:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"一些坑 falsk中的APScheduler貌似不好用, 最后自己使用丑陋的方法写了个独立的py文件处理生成的json, 再用crontab定时运行 每次重启服务器没法启动uWSGI, 参考文章的配置文件似乎因为权限问题没有启动 待续 ","date":"2017-03-16","objectID":"/code/2017-03-16-nginx-uwsgi-flask/:8:0","tags":["flask","nginx","uwsgi","python"],"title":"nginx + uwsgi + flask部署应用","uri":"/code/2017-03-16-nginx-uwsgi-flask/"},{"categories":null,"content":"源码 项目地址 ","date":"2017-03-13","objectID":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/:0:0","tags":["mongodb","flask","echarts","python"],"title":"scrapy + mongodb + flask + echarts 数据可视化","uri":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/"},{"categories":null,"content":"项目框架 scrapy负责定时抓取数据到mongodb中 每小时定时生成echarts需要的数据, 以json格式保存 flask读取json数据之后用jinja2渲染到echarts里作图, 使用的都是修改过的(毫无美感的)官方例子 整个过程大致如图: spider pipelines.py shixiseng.com------\u003escrapy-----------\u003emongodb | | model.py template.html view.py v echart\u003c-------------flask\u003c----------.json ","date":"2017-03-13","objectID":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/:1:0","tags":["mongodb","flask","echarts","python"],"title":"scrapy + mongodb + flask + echarts 数据可视化","uri":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/"},{"categories":null,"content":"Prerequisite: 定时工具: crontab(调度scrapy), apscheduler(调度flask) 数据库: mongodb + pymongo 后端: flask + flask-pymongo(负责flask和mongodb的对接, 其实也可以不用) 绘图: echarts ","date":"2017-03-13","objectID":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/:2:0","tags":["mongodb","flask","echarts","python"],"title":"scrapy + mongodb + flask + echarts 数据可视化","uri":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/"},{"categories":null,"content":"安装 (Ubuntu 14.04/16.04) apscheduler apscheduler官方文档 $ pip install apscheduler mongodb \u0026 pymongo mongodb官方文档, pymongo官方文档 $ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 # Ubuntu 14.04 $ echo \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list # Ubuntu 16.04 $ echo \"deb http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list $ sudo apt-get update $ sudo apt-get install -y mongodb-org # pymongo $ python -m pip install pymongo flask \u0026 flask-pymongo flask中文文档, flask-pymongo官方文档 $ sudo pip install Flask # 推荐使用virtualenv, 这里懒得用了 $ pip install Flask-PyMongo ","date":"2017-03-13","objectID":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/:3:0","tags":["mongodb","flask","echarts","python"],"title":"scrapy + mongodb + flask + echarts 数据可视化","uri":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/"},{"categories":null,"content":"测试 cd到git目录下 $ sudo service mongod start # 启动mongod (其实不用, 因为已经预加载了json) $ python visualization/view.py 进入127.0.0.1:5000下预览吧 (2023/02/14更新: 以下本来是一个 echarts 图标源数据, 需要安装插件渲染, 但因为年久失修没有维护, 我已经禁用了它, 请参考文末的链接) ","date":"2017-03-13","objectID":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/:4:0","tags":["mongodb","flask","echarts","python"],"title":"scrapy + mongodb + flask + echarts 数据可视化","uri":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/"},{"categories":null,"content":"一些坑 这篇blog中插入了echarts图表, 参见这篇文章, 并做了一些魔改以适应复杂的图表 falsk中的APScheduler貌似不好用, 最后自己使用丑陋的方法写了个独立的py文件处理生成的json, 再用crontab定时运行 待续 ","date":"2017-03-13","objectID":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/:5:0","tags":["mongodb","flask","echarts","python"],"title":"scrapy + mongodb + flask + echarts 数据可视化","uri":"/code/2017-03-13-mongodb-flask-echarts-data-visualize/"},{"categories":null,"content":"最近连续被问到两三次, 可惜时间紧张没能帮上什么忙, 这里就记一下解决过程吧 小明向他的女友仙仙求婚, 在求婚戒指上刻了一个大大的爱心. 仙仙看到爱心想考验一下小明, 出了一道题. 方程 $(x^2 + y^2 - 1)^3 -x^2y^3 = 0$ 能画出一个美丽的爱心, 现在给定一个点 $(X, Y)$, 其中 $ X \\sim N(\\mu_1, \\sigma_1^2) $, $ Y \\sim N(\\mu_2, \\sigma_2^2) $, 这个点在这个爱心里面的概率是多少? 精确到0.1 这是一道概率题, 起初想到的是直接在心形线内对概率积分, 后来经过同学的提醒才知道可以直接用实验模拟. 现在介绍一下这两种方法 蒙特卡洛方法 其实就是一个单变量概率模拟, 直接按给定的概率$(u1, u2, s1, s2)$生成坐标, 然后判断一下在不在心形线内部, 进行足够迭代次数之后就可以统计概率了. 很直观的做法 import numpy as np def calc_prob(u1, u2, s1, s2, iter_num): in_circle = 0 for i in range(iter_num): x = np.random.normal(u1, s1) y = np.random.normal(u2, s2) if ((x**2 + y**2 - 1)**3 - (x**2) * (y**3)) \u003c 0: in_circle += 1 return float(in_circle)/iter_num if __name__ == '__main__': print calc_prob(0,0,1,1,10000) 输出结果大约收敛在0.424 二重积分计算 首先观察一下曲线: 看右半边显然曲线是右凸的, 为了避免分段, 决定先从y轴再从x轴积分 为了取得x轴的上下限. 由于正态分布的中心对称性, x从0开始积分就行了.至于上限, 获得了方程之后求x的极值即可: 化简方程为: $x^2 + y^2 - 1 = x^{2/3}y$, 这是心形线的右半部分 隐函数求导, 解出$\\frac{dx}{dy}$: 3. $ 2xdx + 2ydy = \\frac{2}{3}x^{-\\frac{1}{3}}ydx + x^{\\frac{2}{3}}dy$ 4. $(2x-\\frac{2}{3}x^{-\\frac{1}{3}}y) \\frac{dx}{dy} = x^{\\frac{2}{3}} - 2y$ 5. 令$\\frac{dx}{dy} = 0$, 得到$x^2 = 8y^3$, 联立方程组解得上限$x_{max} \\approx 1.13903$ y轴的上下限容易计算, 直接求解化简的方程就行了. $ y_{min} = \\frac{1}{2}(x^{\\frac{2}{3}} - \\sqrt{x^{\\frac{4}{3}} - 4 x^2 + 4)} $ $ y_{max} = \\frac{1}{2}(x^{\\frac{2}{3}} + \\sqrt{x^{\\frac{4}{3}} - 4 x^2 + 4)} $ 最后求积分即可 $$ \\frac{1}{\\pi\\sigma_1\\sigma_2} \\int_{x=0}^{x=1.139} \\int_{y_{min}}^{y_{max}} e^{-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2} - \\frac{(y- \\mu_2)^2}{2\\sigma_2^2}} dydx $$ 将第一种方法中的参数代入之后得到结果如下 计算$\\frac{1.33368}{\\pi} = 0.4245$, 得到的结果相似. ","date":"2017-03-08","objectID":"/playground/2017-03-08-online-interview/:0:0","tags":["面试题"],"title":"一道阿里在线面试算法题--落入心形线的概率","uri":"/playground/2017-03-08-online-interview/"},{"categories":null,"content":"后记 x的最大值大概在哪? x的解析解是什么? $$ \\frac{1}{8 \\sqrt{ \\frac{3}{193 + \\sqrt[3]{55873 - 1536 \\sqrt{1299}} + \\sqrt[3]{55873 + 1536 \\sqrt{1299}}}}} $$ 真正的心形线(Cardioid)长这个样子, 曲线方程也优美得多. 这道题使用数学手段计算毫无美感. 阿里的HRG里没有强迫症改一下嘛w 把女友换成男友, 这个题目的剧情会更合理一些(たぶん ","date":"2017-03-08","objectID":"/playground/2017-03-08-online-interview/:1:0","tags":["面试题"],"title":"一道阿里在线面试算法题--落入心形线的概率","uri":"/playground/2017-03-08-online-interview/"},{"categories":null,"content":"最近老是要配置ss, 顺手记下来以便存档. 为了方便0基础人士阅读极其傻瓜 参考官方文档 1. 服务端(只是使用的话可以跳过这节直接看第二节) ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:0:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"1.1 安装 apt-get install python-pip pip install shadowsocks 注意pip安装出了错千万别用sudo, 否则会有无数的(权限)火坑等着你. 好好上stackoverflow查 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:1:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"1.2 使用配置文件启动方案 配置文件建议写在~/下, 可以避免一些潜在的权限问题 cd mkdir shadowsocks touch ~/shadowsocks/shadowsocks.json # 创建配置文件 touch ~/shadowsocks/ss.log # 创建log(可选, 使用方案2需要) 配置文件格式: { \"server\":\"my_server_ip\", \"server_port\":8388, \"local_address\": \"127.0.0.1\", \"local_port\":1080, \"password\":\"mypassword\", \"timeout\":300, \"method\":\"aes-256-cfb\", \"fast_open\": false } 后台运行: ssserver -c /etc/shadowsocks.json -d start ssserver -c /etc/shadowsocks.json -d stop ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:2:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"1.3 直接后台运行方案 nohup ssserver -p 443 -k password -m aes-256-cfb \u003e ~/shadowsocks/ss.log 2\u003e\u00261 \u0026 之所以不使用文档里的后台运行指令是因为会出现一些权限问题. 在腾讯云上只能使用方案2, 在digitalocean上则可以完全按照文档来. 2. 客户端使用 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:3:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"2.1 windows If you want to keep a secret, you must also hide it from yourself. ^ repo上有趣的介绍 23333 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:4:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"2.1.1 下载最新版本 目前的最新版本是3.4, 猛戳下载 在这里查看最新版本 解压后只有一个shadowsocks.exe, 直接双击启动就行 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:4:1","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"2.2.2 使用 在任务栏找到Shadowsocks图标, 一个纸飞机 在服务器菜单添加多个服务器. 具体填写内容见下图 选择启用系统代理来启用系统代理, 此时如果有浏览器里的代理插件请关闭,或者使用system proxy, 否则会出现代理循环 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:4:2","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"2.2 OS X ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:5:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"2.2.1 下载最新版本 目前的最新版本是2.6.3, 猛戳下载 作者自从被请喝茶之后就没更新了, 可以预见以后也不会了. 安装一路双击就行了 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:5:1","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"2.2.2 使用 在Applications里找到shadowsocksX, 打开, 右上任务栏里会出现一个小纸飞机. 点击纸飞机 Servers -\u003e Open Server Preferences -\u003e 点左下的+按钮 -\u003e 填写各种配置信息, 填错了可以用-按钮删除配置重新填 Servers -\u003e 勾上刚刚创建的服务器配置, 如果是第一次配置的话应该已经默认勾选了 勾上 Global Mode Turn Shadowsocks On, 启动 Turn Shadowsocks Off, 关闭 3. chrome浏览器使用proxyOmega插件连接shadowsocks Q: 为什么要使用proxyOmega? A: 简单说proxyOmega可以只让你的浏览器走代理, 系统的其他部分全部使用正常连接, 配置起来方便. 否则就需要使用上一节的方法启用系统代理, 让所有流量都走代理 Q: 我是mac用户, 我用Safari还要看吗 A: 不用了, 乖乖用shadowsocksX就行 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:5:2","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"3.1 安装 插件传送门 直接点击添加到chrome就ok. 国内没法上chrome应用商店怎么办? 呵呵呵翻墙工具在墙外怎么办? 我能怎么办? 我也很绝望啊 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:6:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"3.2 配置 点击浏览器右上角那个小圆圈-\u003eoptions-\u003e在左边profiles一栏选择proxy Scheme Protocol Server Port (default) SOCKS5 127.0.0.1 1080 这么填好之后点左下角绿颜色的的Apply Changes就行了 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:7:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"3.3 使用 没运行shadowsocks的先运行 点击浏览器右上角那个小圆圈 选择proxy, 进入代理模式 选择direct, 退出代理模式 ","date":"2017-02-23","objectID":"/code/2017-02-23-shadowsocks-chrome-config/:8:0","tags":["shadowsocks"],"title":"shadowsocks+chrome代理配置速查手册","uri":"/code/2017-02-23-shadowsocks-chrome-config/"},{"categories":null,"content":"scrapy的官方中文文档 源代码 Scrapy框架数据流 上图是scrapy的数据流, 看不懂没关系, 接下来会简单介绍scrapy爬取的流程. pip install scrapy # 安装scrapy scrapy startproject shixiseng # 启动一个新项目,命名为shixiseng 接下来会在目录下看到以下结构: shixiseng/ scrapy.cfg shixiseng/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... 我们重点关注的是以下几个文件: spiders目录下的脚本负责解析和爬取网页信息 -\u003e 将信息封装成items.py中定义好的格式(类似于python的dict) -\u003e 传给pipelines.py处理得到的Item, 可以写入文件, 也可以传入数据库 spider部分的构造 在spiders目录下新建一个脚本, 命名为intern_spider.py 看看实习僧主页, 列出了一堆关键词, 随便找一个进入, 发现url格式很简单, 为http://www.shixiseng.com/interns?k=IOS\u0026p=1, 其中k=搜索关键词, p=显示页数. 这个结构很简单, 不涉及复杂的js/ajax功能, 例如需要下拉才能刷新出信息等. 省却了很多麻烦 发现置空关键词的话, 只能显示出500页信息, 而实习信息远远不止这个数, 所以还是乖乖按关键词搜索吧. 把第一行的关键词输入一个list, 按照list一个一个爬就可以了, 见下: # 爬取各种关键词 key_words = ['软件', 'IOS', '数据库', 'C#/.NET', 'Hadoop', 'Android', '算法', 'IT运维', 'Python', '云计算/大数据', 'Node.js', '数据挖掘', 'PHP', 'Ruby/Perl', '测试', 'Java', 'C/C++', '前端'] start_urls = ['http://www.shixiseng.com/interns?k=#\u0026p=1'.replace('#', x) for x in key_words] 其中start_urls就是scrapy开始爬取的链接, 我们组装出了18个关键词, 每次从搜索结果的第一页开始爬取, 一直爬到最后一页. 然而最后一页是第几页? 在不知情的情况下, 我们只能一直增加页数, 直到新的页面不能显示出实习信息为止, 像这样: 此时使用xpath搜不到显示实习信息的div就可以停止爬取了. 所以我们的parse()函数, 即爬取一页信息的信息定义如下: def parse(self, response): # 初始化选择器和item sel = Selector(response) item = InternItem() # 如果超出最大页数, 就不执行抓取 if sel.xpath(\".//*[@id='load_box_item']/div\").extract_first() is not None: # 爬取一页的职业信息, 返回一个list title = sel.xpath(\".//*[@id='load_box_item']/div[*]/div/div/a/h3/text()\").extract() # 爬取指向每个信息的岗位职责子页面的链接 dec_link = sel.xpath(\".//*[@id='load_box_item']/div[*]/div/div/a/@href\").extract() # 将爬取的unicode转换成utf-8字节码 # 每一页所有的候选框依次清洗并封装成wrapped_dict并缓存在item_list中 item_list = [] for i in range(len(title)): # 把相对路径拼接成绝对路径 abs_link = 'http://www.shixiseng.com' + str(dec_link[i].encode('utf-8') wrapped_dict = {'dec_link': abs_link), 'title': title[i].encode('utf-8'), } item_list.append(wrapped_dict) # 按照每一个title对应的url爬取岗位职责, 同时通过meta传入打包好的item给parse_dec() for i in range(len(item_list)): yield Request(item_list[i]['dec_link'], callback=self.parse_dec, meta={'item': item_list[i]}) # 拼接好下一页的url并爬取下一页 next_page_index = int(response.url[-1]) + 1 next_page = response.url[:-1] + str(next_page_index) yield Request(next_page, callback=self.parse) else: yield item # 直接返回空的item 介绍代码之前, 首先说明一下需要爬取的信息(源代码要复杂一点, 这里是简化版): title(职位), link(打开实习信息的链接), dec(实习信息, 需要在子页面爬取). 其中链接是在存储入数据库的时候用来区分是否重复的依据. sel = Selector(response)这行把爬取到的信息初始化为一个选择器, 可以通过调用xpath()(xpath选择器), css()(css选择器), re()(正则表达式)等方法提取需要的信息 首先通过xpath()方法利用xpath提取需要的信息, 再使用extract()提取得到的文本list, 否则会返回一个Object而不是文本. 获取title和dec_link之后, 配对封装成wrapped_dict, 放到item_list中备用 提取到每个实习信息的dict, 通过Request(item_list[i]['dec_link'], callback=self.parse_dec, meta={'item': item_list[i]})发出请求爬取子页面 解释一下这里的Request()函数: 首先, Request()返回一个response类, 其中包含了请求获得的网页信息, 以及自己传入的一些参数. 第一个参数是需要爬取的url 第二个参数是调用相应的函数处理爬取的信息, 这里会调用自己写好的parse_dec()函数处理这个Request的response. 第三个参数meta={'item': item_list[i]})将前面爬到的信息传入这里的response. 下一步, 在parse_dec()中将爬取子页面的信息和之前传入的信息合并并交给pipelines处理, 见之后介绍parse_dec()的部分 处理结束之后, 只爬取到了一页的信息, 接下来还要去下一页, 拼接好next_page之后, 使用Request(next_page, callback=self.parse)调用自己爬取下一页, 如果下一页没有信息了, 就直接返回空的item交给pipelines. # 爬取岗位职责页面, 和之前的信息拼接成item并传给pipelines def parse_dec(self, response): sel = Selector(response) # # 接收之前传入的meta item = response.meta['item'] # 职位描述和截止日期 dec_list = sel.xpath(\".//*[@id='container']/div[1]/div[1]/div[3]//text()\").extract() item['dec_content'] = [x.encode('utf-8') for x in dec_list] yield item # 返回爬取一个候选框的信息给pipeline处理 介绍一下这段代码 首先, 之前爬取的职位和链接信息都包含在了response的meta参数里面了, 通过item = response.meta['item']把他们传入item. 其次, 通过新增'dec_content'键, 继续往item里添加爬取到的信息, 最后返回item, 给pipeline处理. 整个爬取的流程如下 parse()page1 --item(title1,link1)--\u003e parse_dec()爬取子页面1 --item(title1,link1,dec1)--\u003e pipelinesc处理 --item(title2,link2)--\u003e parse_dec()爬取子页面2 --item(title2,link2,dec2)--\u003e pipelinesc处理 ... --item(title9,link9)--\u003e parse_dec()爬取子页面9 --item(title9,link9,dec9)--\u003e pipelinesc处理 parse()进入第二页爬取 parse()page2 --item(title1,link1)--\u003e parse_dec()爬取子页面1 --item(title1,link1,dec1)--\u003e pipel","date":"2017-02-18","objectID":"/code/2017-02-18-scrapy/:0:0","tags":["scrapy","python"],"title":"Scrapy简介(附项目:爬取实习僧实习信息)","uri":"/code/2017-02-18-scrapy/"},{"categories":null,"content":"MongoDB 是一种文件导向的 NoSQL 数据库，由 C++ 撰写而成。 ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:0:0","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"MongoDB简介 什么是NoSQL? 看看这篇zhihu问题足矣. 我的粗浅理解就是NoSQL和SQL不同的地方是, SQL需要定义好一行的标签, 每行数据的维度都是固定的, 格式也是固定的,非常严格. 而NoSQL每个单元的数据都是长短可变的, 甚至每个key对应的value也都是可变的 为什么用MongoDB? 对我而言只是因为他的document文档格式和python的dict以及json格式无缝衔接, 数据单元可伸缩性好, 入门门槛极低. 非常适合小规模的web开发. ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:1:0","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"mongo shell 简介 官方文档 mongo shell是个数据库控制台, 拿mysql简单理解就是登录mysql之后出现的控制台, 可以进行各种数据库操作, mongodb也一样有他自己的控制台, 即mongo shell. 安装见官方文档, 这里不多说了(有时间再补充吧) ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:2:0","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"0. mongodbd的数据结构 database -\u003e collection1 -\u003e document1 -\u003e document2 -\u003e document3 -\u003e ... -\u003e collection2 -\u003e document1 -\u003e document2 -\u003e document3 -\u003e ... -\u003e ... mongodb中的最小存储单元是文档(document), 文档组成集合(collection), 集合再组成数据库(database), 其中数据库和集合是可以命名的. document的内容类似于JSON, 或者pyhton中的字典(dict), 由field: value的键值对组成.例子如下. 详细信息请参考官方文档关于document1的说明 { \"_id\": \"oranges\", \"qty\": { \"in stock\": 8, \"ordered\": 12 }, \"tag\": \"food\" } ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:2:1","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"1. 启动/关闭mongod和mongo(OSX系统下,linux类似) $ mongod # 方法1: 启动mongod, 切换至另一个终端开启控制台(因为这个终端会用作打log信息) $ mongod --fork --logpath /var/log/mongod.log # 方法2: 静默启动, 把log信息存入/var/log/mongod.log下 $ mongo \u003cdatabase\u003e # 连接数据库database, 进入mongo shell $ mongo --eval \"db.getSiblingDB('admin').shutdownServer()\" # 关闭数据库 $ sudo service mongod start # linux下的启动指令 mongod指令启动数据库, mongo指令登录数据库控制台. 这么说就好理解了, 所以要开两个终端, 要不就用第二种静默启动, 把log保存在文件里的方法继续在同一个终端下工作. ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:2:2","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"2. mongo shell下的基本操作 \u003e db # 显示现在使用的数据库 \u003e show dbs # 查看数据库 \u003e use \u003cdatabase\u003e # 切换数据库 \u003e show collections # 查看collections \u003e db.myCollection.insert( { x: 1 } ); # 新建一个名为myCollection的collection, 插入一个document. 注意末尾的分号 \u003e db[\"3test\"].find() # 使用find列出名为3test的collection的文档 \u003e db.getCollection(\"3test\").find().pretty() # 使用pretty优化输出 \u003e quit() # 退出, 也可以按CTRL+C ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:2:3","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"3. MongoDB 的CRUD操作 CURD: Create, Read, Update, Delete 创建/插入文档 db.collection.insertOne( { x: 1 } ): 创建collection db.collection.insertMany( [{ x: 1 }, { x: 2 }] ): 插入两个文档, 表示方式类似于python中的数组 db.collection.insert(): 以上两种方法都集成到了insert()中 查询文档 db.collection.find( { field1: \u003cvalue\u003e, field2: \u003cvalue\u003e ... } ): 查询符合键/值对的文档 db.collection.find().limit(5): 利用cursor modifier限制搜索范围为5个结果 更新文档 db.collection.update(\u003cfilter\u003e, \u003cupdate/replacment\u003e, \u003coptions\u003e) mongodb提供了更新操作符(update operator)来更新文档, 格式如下: { \u003coperator1\u003e: { \u003cfield1\u003e: \u003cvalue1\u003e, ... }, \u003coperator2\u003e: { \u003cfield2\u003e: \u003cvalue2\u003e, ... }, ... } 操作符介绍 name function $inc 增加值 $mul 乘以值 $rename 重命名 $setOnInsert 如果更新导致插入一个文档, 则set某个值. 如果更新只是修改某个已经存在的文档则不起作用 $set set文档的某个值 $unset 移除文档的某个field $min 如果指定的值比已存在的值小, 就更新该值 $max 如果指定的值比已存在的值大, 就更新该值 $currentDate 将某个field的值设定为日期 其他操作符见官方文档 一个更新的例子 db.inventory.update( { item: \"paper\" }, { $set: { \"size.uom\": \"cm\", status: \"P\" }, $currentDate: { lastModified: true } } ) 其中第一个dict是过滤器\u003cfilter\u003e, 即需要更新的文档是满足item键的值为paper的第一个文档 第二个dict为\u003cupdate/replacment\u003e, 使用$set操作符更新key为\"size.uom\"的值为\"cm\", 更新key为status的值为\"p\", 使用$currentDate更新key为lastModified的值为当前时间, 如果没有这个key就创建一个key 删除文档 db.collection.remove( \u003cquery\u003e, \u003cjustOne\u003e ) 第一个参数就是用来搜索符合条件的文档的, 例如{x: 1}会删除所有符合{x: 1}的文档, 第二个参数可选, 是设定只删除第一个true还是删除所有符合条件的文档的false 删除collection使用db.\u003ccollectionName\u003e.drop()指令 删除数据库使用db.dropDatabase();(前提是在该数据库下, 即先使用use \u003cdbName\u003e;) 或者直接在终端使用mongo \u003cdbname\u003e --eval \"db.dropDatabase()\" ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:2:4","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"pymongo简介 pymongo是使用python操作mongodb的包, 官方文档 ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:0","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"1. 建立数据库和collection from pymongo import MongoClient client = MongoClient('localhost', 27017) 连接数据库, 省略参数之后会使用默认host和port, 如例子所示 db = client.test_database db = client['test-database'] 以上两句是等价的, 创建了一个名为test_database的数据库 collection = db.test_collection collection = db['test-collection'] 以上两句是等价的, 在test_database中创建了一个叫test_collection的collection. collection是数据库中的一组文档. 文档(document)是mongodb中数据的最小单位, 类似于python中的dict, 或者JSON格式 ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:1","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"2. 插入一个document import datetime post = {\"author\": \"Mike\", \"text\": \"My first blog post!\", \"tags\": [\"mongodb\", \"python\", \"pymongo\"], \"date\": datetime.datetime.utcnow()} posts = db.posts post_id = posts.insert_one(post).inserted_id 以上语句封装了一个JSON格式的post, 并使用insert_one()方法插入了名为posts的collection. 其中post中的datetime.datetime.utcnow()会被自动转换成BSON格式 插入成功之后会返回一个inserted_id, 对于每个document是独一无二的. 对于每个插入的document, mongodb会自动为其添加一个名为_id的key, 其value也为inserted_id ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:2","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"3. 使用find_one()查询文档 \u003e\u003e\u003e posts.find_one() \u003e\u003e\u003e {u'_id': ObjectId('...'), ... u'author': u'Mike', ... u'date': datetime.datetime(...), ... u'tags': [u'mongodb', u'python', u'pymongo'], ... u'text': u'My first blog post!'} 直接调用find_one()会返回posts的第一个文档 posts.find_one({\"author\": \"Mike\"}) posts.find_one({\"_id\": post_id}) 定制搜索条件. 其中如果按照_id搜索的话, 必须传入ObjectId类而不是str类, 比如例子中post_id为第2节中返回的ObjectId() 另外, 如果想要自己初始化一个_id可以这么做: from bson.objectid import ObjectId document = client.db.collection.find_one({'_id': ObjectId(post_id)}) ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:3","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"4. 使用find()查询多个文档 for post in posts.find(): pprint.pprint(post) find()返回一个Cursor实例, 它是一个可遍历实例, 可以像list一样进行遍历 参数使用和find_one()类似, 可以定制搜索条件 ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:4","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"5. 编码问题 MongoDB stores data in BSON format. BSON strings are UTF-8 encoded so PyMongo must ensure that any strings it stores contain only valid UTF-8 data. Regular strings (\u003ctype ‘str’\u003e) are validated and stored unaltered. Unicode strings (\u003ctype ‘unicode’\u003e) are encoded UTF-8 first. The reason our example string is represented in the Python shell as u’Mike’ instead of ‘Mike’ is that PyMongo decodes each BSON string to a Python unicode string, not a regular str. 简单说就是mongodb会把Unicode自动转换成UTF-8存储, 对于已经encode的str则不作处理 ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:5","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"6. 使用count()计数 \u003e\u003e\u003e posts.count() 3 \u003e\u003e\u003e posts.find({\"author\": \"Mike\"}).count() 2 ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:6","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"7. 高级搜索 d = datetime.datetime(2009, 11, 12, 12) for post in posts.find({\"date\": {\"$lt\": d}}).sort(\"author\"): pprint.pprint(post) 此处使用$lt搜索出了所有在d之前的所有posts, 并按照\"author\"关键词升序排列 ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:7","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"8. 创建自己的编号(index) \u003e\u003e\u003e result = db.profiles.create_index([('user_id', pymongo.ASCENDING)], ... unique=True) \u003e\u003e\u003e sorted(list(db.profiles.index_information())) [u'_id_', u'user_id_1'] 用index_information()查询一下, 发现现在我们有了两个index, 一个是插入文档时自创的_id, 还有一个是刚刚创建的user_id \u003e\u003e\u003e user_profiles = [ ... {'user_id': 211, 'name': 'Luke'}, ... {'user_id': 212, 'name': 'Ziltoid'}] \u003e\u003e\u003e result = db.profiles.insert_many(user_profiles) 使用insert_many()方法插入两个文档之后,再试试插入拥有相同user_id的文档试试 \u003e\u003e\u003e new_profile = {'user_id': 213, 'name': 'Drew'} \u003e\u003e\u003e duplicate_profile = {'user_id': 212, 'name': 'Tommy'} \u003e\u003e\u003e result = db.profiles.insert_one(new_profile) # 插入编号213, ok \u003e\u003e\u003e result = db.profiles.insert_one(duplicate_profile) # 插入编号212, 已经存在, 抛出DuplicateKeyError Traceback (most recent call last): DuplicateKeyError: E11000 duplicate key error index: test_database.profiles.$user_id_1 dup key: { : 212 } Refer: https://docs.mongodb.com/manual/mongo/ https://docs.mongodb.com/manual/crud/ https://api.mongodb.com/python/current/tutorial.html ","date":"2017-02-17","objectID":"/code/2017-02-17-mongodb/:3:8","tags":["MongoDB"],"title":"MongoDB+pymongo简介(持续更新中)","uri":"/code/2017-02-17-mongodb/"},{"categories":null,"content":"这是个课程总结. ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:0","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"1. 分治法 适用条件 原问题可以分解为若干个与原问题性质相类似的子问题 问题的规模缩小到一定程度后可方便求出解 子问题的解可以合并得到原问题的解 分解出的各个子问题应相互独立，即不包含重叠子问题 基本步骤 分解问题(divide)：把原问题分解为若干个与原问题性质相类似的子问题 求解子问题(conquer)：不断分解子问题直到可方便求出子问题的解为止 合并子问题的解(combine)：合并子问题的解得到原问题的解 相关算法 归并排序 最好情况: $O(nlogn)$ 最坏情况: $O(nlogn)$ 快速排序 最好情况: $O(nlogn)$ 最坏情况: $O(n^2)$ (相同元素, 或者已经有序) ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:1","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"2. 动态规划 必要前提 最优子结构性质: 一个问题的最优解中所包含的所有子问题的解都是最优的 重叠子问题: 不是必要前提, 但存在重叠子问题可以提高算法效率 基本步骤 确定问题的最优子结构 递归定义最优解值 自底向上计算最优解值 从已计算得到的最优解值信息中构造最优解 相关算法 矩阵链乘法 $O(n^3)$ 最长公共子序列问题 $Θ(mn)$, m,n为两序列长度 最优二叉查找树 $O(n^3)$ ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:2","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"3. 贪心算法 必要前提 最优子结构性质: 局部最优+一系列贪心选择产生原问题的一个最优解 贪心选择性质: 所求问题的最优解可以通过一系列局部最优的贪心选择而得到。即：局部最优-\u003e全局最优. 贪心选择性质是区别与动态规划方法的一个重要特征, 因为贪心法在作出选择时并不依赖于将来的情况, 只需根据当前的情况即可作出选择。 基本步骤 确定问题的最优子结构 将优化问题转化为作出一种选择，即贪心选择 贪心选择后应只留下一个子问题，其它子问题均为空 证明贪心选择的正确性，即本次贪心选择与剩余子问题的最优解可以构成原问题的最优解 贪心算法与动态规划的比较 贪心法效率更高，表现在子问题不需要都计算，而选择数为1。 动态规划方法的应用面更广，问题只需具有最优子结构即可，而贪心法不仅需要具有最优子结构性质，还需要具有贪心选择性质 相关算法 活动选择问题(Recursive-Activity-Selector) $O(n)$ Huffman编码: 采用二叉堆管理序列, $O(nlgn)$ 胚的概念 胚$(S, I)$, $S$是有限集, $I$是$S$的独立子集构成的集合族: 独立性: 每个独立集的子集也是独立的. 即任何${A’} \\subseteq A \\subseteq S$, 如果 $A \\in I$, 则 ${A’} \\in I$ 交换性: 如果$A$和$B$是$I$的两个独立集，$A$比$B$有更多的元素, 则在$A$中存在一个元素，当其加入$B$时得到一个比$B$更大独立集. 最大独立子集: 若$A$是$S$的独立子集即$A \\in I$, 如果存在一个元素$x \\notin A$, 且 $A \\cup \\lbrace x \\rbrace \\in I$, 则称为$A$的一个扩展. 如果A不存在扩展就称为最大独立子集 加权胚: 为$S$中每个元素$x$赋予一个权重$w(x)$, 对于集合$A$, 其中每个元素乘以各自的权重得到的就是$A$的权值, 即: $$w(A) = \\sum\\limits_{x \\in A} w(x)$$ 最优子集: 使$w(A)$权值达到最大的独立子集A称为最优子集. 若$A$是$S$的独立子集, 由于任何元素$x \\in S$的权重$\\omega(x)$都是正数, 故最大独立子集具有最大权重, 最优子集 = 最大独立子集 ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:3","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"4. 渐进记号 $f(n) = O(g(n))$: $g(n)$是$f(n)$的上界 $f(n) = \\Omega(g(n))$: 下界 $f(n) = \\Theta(g(n))$: 紧确界 $f(n) = \\omicron(g(n))$: 不紧确上界 $f(n) = \\omega(g(n))$: 不紧确下界 ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:4","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"5. 主方法 $T(n) = a T(n/b) + f(n)$ 比较$n^{log_b a}$和$f(n)$: 情况1: $f(n) = O(n^{log_b a})$, $T(n) = \\Theta(n^{log_b a})$ 情况2: $f(n) = \\Theta(n^{log_b a})$, $T(n) = \\Theta(n^{log_b a} lgn)$ 情况3: $f(n) = \\Omega(n^{log_b a})$, 且对某个常数c\u003c1存在足够大的n使得$a f(n/b) \\leq cf(n)$, 则$T(n) = \\Theta(f(n))$ ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:5","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"6. 红黑树 二叉查找树 假设n为节点数, h为树高, 则h最低为$lg(n)$, 最高为n 静态操作: 查找结点、找前驱、后继等时间为O(h) 动态操作: 插入和删除结点等时间为O(h) 红黑树 查找、插入、删除: O(h) 插入最多旋转2次, 删除最多旋转3次 序统计树 定义: 所谓序统计就是在一系列数中找出最大、最小值，某个数的序值等操作 找第i个最小值操作: O(lgn) 求x的序值操作: O(lgn) 插入或删除操作: O(lgn) 区间树 i与i’重叠可描述为: $low[i]≤high[i’], low[i’] ≤high[i]$ 插入和删除操作: O(lgn) 数据结构扩张的步骤 挑选一个合适的基本数据结构 决定在基本数据结构上应增加的信息 修改基本数据结构上的操作并维持原有的性能 修改或设计新的操作 ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:6","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"7. 平摊分析 ####势函数方法 先证明势函数恒为正, 再计算平摊代价 栈操作 二进制计数器 动态表扩张/收缩 ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:7","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"8. 二项堆 性质 最小堆性质: 节点关键字大于父节点关键字 没有度数相同的根. 使用二进制表示每个根, 那么每一位的大小就是结点个数, 因此n个结点最多有lgn+1颗二项树 根表: 单链表, 根的度数严格递增 算法性能: |算法|性能(最坏)|备注| |:-:|:-:| |建堆|O(1)| |寻找最小关键字|$\\Omega(lgn)$|沿着根表寻找最小值| |合并两个二项堆|$\\Omega(lgn)$|维护两个指针, 连接两个度数相同的根,跳过三个度数相同的根里的第一个| |插入节点|$\\Omega(lgn)$|先构造一个节点的二项堆,再合并| |抽取最小节点|$\\Theta(lgn)$|删除根, 将子树逆序排列, 再与剩下的堆合并| |减少关键字的值|$\\Theta(lgn)$|如果比父节点小就交换,直到比父节点大| |删除节点|$\\Theta(lgn)$|先减少到最小, 再抽取最小节点| 9. 斐波那契堆 性质 根表: 环形双链表, 指向最小值 算法性能 操作 性能(平摊) 备注 抽取最小节点 O(lgn) 合并度数相同的根 删除节点 O(lgn) 减值 O(1) 先检查是不是根节点, 不是若违背最小堆性质就删掉再把子树合并 其他操作都是O(1) ","date":"2017-01-11","objectID":"/2017-01-11-algo-sum/:0:8","tags":null,"title":"算法总结","uri":"/2017-01-11-algo-sum/"},{"categories":null,"content":"这是个课程报告 1. 课程贡献: pull requests(2次) #更新项目1手写体识别的README #221 sklearn改进 将数据集更换为train2.csv(数据集容量提升到了8000), 使用相同脚本预测性别时准确率从70%提升到了80%, 说明在2000以上的样本容量对准确率依然有影响 数据预处理, 使用平均值填充缺失值 def load_data(): # 数据集已合并, 去掉了标签行, sex预处理为数字 df = pd.DataFrame(pd.read_csv('train2.csv', names=class_names_train2)) # 转化为字符串 df = df.convert_objects(convert_numeric=True) # 使用平均值填充缺失值 df = df.fillna(df.mean()) 按照年龄将数据集分为不同年龄段分段进行训练 def split_data(df, low, high): \"\"\" :param df: 输入的dataframe :param low: 截取区间的低阈值 :param high: 截取区间的高阈值(不包含) :return: 截取的dataframe \"\"\" df_lowcut = df[df['age'] \u003e= low] df_cut = df_lowcut[df_lowcut['age'] \u003c high] selected_names = [x for x in class_names_train2 if (x != 'age' and x != 'sex')] x_data = df_cut[selected_names].as_matrix() y_data = df_cut['age'].as_matrix() # 用平均值填充nan def fill_nan(np_array): col_mean = np.nanmean(np_array, axis=0) nan_ids = np.where(np.isnan(np_array)) np_array[nan_ids] = np.take(col_mean, nan_ids[1]) return np_array x_data = fill_nan(x_data) print 'x有没有nan值:', np.any(np.isnan(x_data)) print 'y有没有nan值:', np.any(np.isnan(y_data)) return x_data, y_data 区分年龄段之后, 预测结果如下 0-25岁 训练集准确率为: 0.983532934132 测试集准确率为: 0.906040268456 25-60岁 训练集准确率为: 0.843848874 测试集准确率为: 0.373534338358 60-80岁 训练集准确率为: 0.977473065622 测试集准确率为: 0.649122807018 由此可见, 区分不同年龄段之后预测准确率有了极大提升, 尤其是低年龄段, 说明不同年龄段的分布不是一样的. 2. 课程贡献: Presentation(2次) ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:0:0","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"2.1 Adaboost理论简介 ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:0","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"强可学习和弱可学习 在概率近似正确(PAC)学习框架中, 一个类如果存在: 一个多项式复杂度的学习算法,正确率略大于随机猜测(例如二分类问题中大于1/2),称弱可学习的类 一个多项式复杂度的学习算法,并且正确率很高,称强可学习的类 Kearns和Valiant证明了强可学习和弱可学习是等价的 Adaboost算法就是将弱学习器组成强学习器的算法 ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:1","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"组合弱分类器 测试集: $$T = \\left\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(N)}, y^{(N)})\\right\\rbrace, \\quad x \\in \\chi \\subseteq R^N, y \\in \\lbrace +1, -1 \\rbrace \\\\ \\text{with weights: } D_i = (w_{i1}, w_{i2}, \\cdots, w_{iN})$$ 分类器 $$G_m(x): \\chi \\to \\lbrace +1, -1\\rbrace, \\quad G(x) = \\sum\\limits^M_{i=1} \\alpha_m G_m(x) \\\\ \\text{with weights: } A = (\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{M}) $$ ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:2","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"AdaBoost 算法 输入: 训练集 $T$, m个弱分类器 $G_m(x)$ 输出: 集成分类器 $G(x)$ 初始化权重: $$ D_1 = (w_{11}, w_{12}, \\cdots, w_{1N}), \\quad w_{1i} = \\frac{1}{N} $$ For $m = 1,2, \\cdots, M$: (a) 对具有权重分布 $D_m = (w_{m1}, w_{m2}, \\cdots, w_{mN})$ 的训练集 $T$ 训练出弱分类器 $G_m(x)$ (b) 计算弱分类器 $G_m(x)$ 在 $T$ 上的误差率: $$ e_m = P(G_m(x^{(i)}) \\neq y^{(i)}) = \\sum\\limits^N_{i=1} w_{mi}I(G_m(x^{(i)}) \\neq y^{(i)}) \\tag{1}$$ (c) 计算 $G_m(x)$ 的系数 $\\alpha_m$: $$ \\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m} \\tag{2}$$ ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:3","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"AdaBoost 算法 (d) 更新训练集的权重分布 $D_{m+1}$: $$ D_{m+1} = (w_{m+1, 1}, w_{m+1, 2}, \\cdots, w_{m+1, N}) $$ $$w_{m+1, i} =\\begin{cases} \\frac{w_{mi}}{Z_m} e^{-\\alpha_m}, \u0026 G_m(x^{(i)}) = y^{(i)} \\\\ \\\\ \\frac{w_{mi}}{Z_m}e^{\\alpha_m}, \u0026 G_m(x^{(i)}) \\neq y^{(i)} \\end{cases} = \\frac {w_{mi}e^{-\\alpha_m y^{(i)} G_m(x^{(i)})}} {Z_m} \\tag{3}$$ $$Z_m = \\sum\\limits^N_{i=1}w_{mi}e^{-\\alpha_m y^{(i)} G_m(x^{(i)})} $$ 这里 $Z_m$ 是规范化因子, 使得 $D_{m+1}$ 成为一个概率分布, 保证了 $\\sum\\limits^N_{i=1}w_{m+1, i} = 1$ 最后,组合所有弱分类器 $G_m(x)$: $$ f(x) = \\sum\\limits^M_{i=1} \\alpha_m G_m(x), \\quad G(x) = sign(f(x))$$ ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:4","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"实现细节 注意到每次迭代测试集中的样本都具有不同的权重, 实现方法有: 在每个弱分类器计算损失函数的时候, 对相应样本的loss乘以权重 缺点: 需要修改弱分类器, 在loss中引入权重 不需要修改弱分类器的方案: 直接修改训练集 每次迭代都使用 $D_i$ 作为概率分布从原始数据集中生成新的数据集进行训练 ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:5","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"直观解释 对模型来说, 这里显示了弱分类器 $G_m(x)$的权重, $ \\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m} $ 与误差 $e_m$, 变化的关系. 显然更精确的弱分类器具有更大的权重 相反, 对训练集数据来说, 从 (3) 可知被误分类的样本的权重会增加, 被正确分类的样本权重会降低, 增加/降低的速度都是指数级别的. ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:6","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"加法模型与前向分步算法 考虑一个与adaboost相似的累加模型 (additive model) : $$ f(x) = \\sum\\limits_{m=1}^M \\beta_m b(x; \\gamma_m) $$ in which $b(x; \\gamma_m)$ 是基分类器, $\\gamma_m$ 是基分类器的参数, $\\beta_m$ 是基分类器的权重. 为了最小化损失函数, 提出前向分步算法(forward stagewise algorithm), 每一步只学习一个基函数及其系数, 即每一步中把其他所有模型看成常数, 只优化一个模型. 这是前向分步算法的核心概念. ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:7","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"前向分步算法 输入: 训练集 $T = \\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(N)}, y^{(N)}) \\rbrace$, 损失函数$L(y, f(x))$, 基分类器 $b(x; \\gamma)$ 输出: 累加模型 $f(x)$, 初始化 $f_0(x) = 0$ For $m = 1,2,\\cdots,M$: 最小化损失函数: $$ (\\beta_m, \\gamma_m) = arg\\min\\limits_{\\beta, \\gamma} \\sum\\limits_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + \\beta b(x^{(i)};\\gamma)) $$ 更新 $f(x)$: $$ f_m(x) = f_{m-1}(x) + \\beta_m b(x^{(i)};\\gamma_m)$$ 生成累加模型: $$ f(x) = \\sum\\limits_{m=1}^M \\beta_m b(x; \\gamma_m) $$ ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:8","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"Adaboost与前向分步算法 以下证明, adaboost算法实际上就是一个使用累加模型, 且损失函数为指数函数的模型 假设损失函数为指数损失函数(exponential loss function): $$ L(y, f(x)) = exp(-yf(x)) $$ 在第$m$次迭代中: $$ f_m(x) = f_{m-1}(x) + \\alpha_mG_m(x), \\\\ \\text{其中 } f_{m-1}(x) = \\sum\\limits_{m=1}^{M-1} \\alpha_mG_m(x) $$ 为了最小化 $$\\begin{align} (\\alpha_m, G_m) \u0026 = arg\\min\\limits_{\\alpha, G} \\sum\\limits_{i=1}^N exp \\left\\lbrace -y^{(i)}(f_{m-1}(x^{(i)}) + \\alpha_m G_m(x^{(i)}))\\right\\rbrace \\\\ \u0026 = arg\\min\\limits_{\\alpha, G} \\sum\\limits_{i=1}^N w_{mi} exp \\left\\lbrace -y^{(i)} \\alpha_m G_m(x^{(i)}) \\right\\rbrace \\end{align} $$ 其中 $w_{mi} = exp(-y^{(i)} f_{m-1}(x^{(i)}))$, 则损失函数仅依赖于$\\alpha$ 和 $G$. 为了证明算法中的$\\alpha_m^*, G_m^*$ 与adaboost中的 $\\alpha_m, G_m$ 等价: 对任意 $\\alpha \u003e 0$, 使损失函数最小的 $G_m^*(x)$ 由下式得到: $$ G_m^*(x) = arg\\min\\limits_{G} \\sum\\limits_{i=1}^N w_{mi} I(y^{(i)} \\neq G(x^{(i)})) $$ 此分类器即为adaboost算法中的基分类器, 即 $G_m^*(x) = G_m(x)$ 求 $\\alpha_m$ $$\\begin{align} L(\\alpha, G_m) \u0026 = \\sum\\limits_{i=1}^N w_{mi} exp \\left\\lbrace -y^{(i)} \\alpha G_m(x^{(i)}) \\right\\rbrace \\\\ \u0026 = (e^{\\alpha} - e^{-\\alpha}) \\sum\\limits_{\\neq} w_{mi} + e^{-\\alpha} \\sum\\limits w_{mi} \\end{align} $$ 其中$\\sum\\limits_{\\neq}$ 是$\\sum\\limits_{y^{(i)} \\neq G(x^{(i)})}$的缩写, $\\sum$ 是$\\sum\\limits_{i=1}^N$ 的缩写 对$\\alpha$求导并使导数为0, 即得到使损失函数最小的$\\alpha$ $$\\frac {\\partial L(\\alpha, G_m)} {\\partial \\alpha} = (e^{\\alpha} + e^{-\\alpha}) \\sum\\limits_{\\neq} w_{mi} - e^{-\\alpha} \\sum\\limits w_{mi} = 0$$ s.t. $$(e^{\\alpha} + e^{-\\alpha}) \\frac{\\sum\\limits_{\\neq} w_{mi}}{\\sum\\limits w_{mi}} - e^{-\\alpha} = 0$$ s.t. $$(e^{\\alpha} + e^{-\\alpha}) \\epsilon_m - e^{-\\alpha} = 0$$ s.t. $$ \\alpha^* = \\frac{1}{2}ln \\frac{1-\\epsilon_m}{\\epsilon_m} $$ 其中 $\\epsilon_m = \\frac{\\sum\\limits_{\\neq} w_{mi}}{\\sum\\limits w_{mi}}$, 即为 $G_m^*(x)$ 的损失函数 最后我们得到 $(\\alpha_m, G_m)$ 是和adaboost中的一致的 ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:1:9","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"2.2 Adaboost Code Review ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:0","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"数据集介绍 Horse Colic Data Set 马是否得了疝气的预测 数据集大小: 68 特征数目: 24 数据集是否完整: 有缺失数据, 用0补全 ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:1","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"项目结构 基分类器使用决策树桩 基分类器的数据结构: 采用字典结构 基分类器的训练: 损失函数为0-1损失函数, 找到最好阈值 adaboost算法 集成分类器的数据结构: 采用类保存权重list和基分类器list 算法训练 保存/载入模型的功能: 读取/保存为json文件 模型评估: 准确率和ROC曲线 ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:2","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"基分类器:决策树桩 import numpy as np def stump_classifier(data_matrix, feature_index, threshold, rule='lt'): \"\"\" :param: data_matrix: 测试集, 样本按行排列 :param: feature_index: 用来分类的特征序号 :param: threshold: 阈值 :param: rule: 规则, 默认情况下是小于阈值被分类为-1.0 决策树桩 输入: 测试集 输出: 预测结果(以1,-1标注) \"\"\" results = np.ones((data_matrix.shape[0], 1)) feature_values = data_matrix[:, feature_index] if rule == 'lt': results[np.where(feature_values \u003c= threshold)[0]] = -1.0 elif rule == 'gt': results[np.where(feature_values \u003e threshold)[0]] = -1.0 else: print('ERROR: rule not recognized, use default as lt.') results[np.where(feature_values \u003c= threshold)[0]] = -1.0 return results ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:3","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"基分类器训练过程: 寻找最小损失的阈值 import numpy as np def create_stump(data_matrix, labels, data_weights, step_number=10): \"\"\" :param: data_matrix: 测试集, 样本按行排列 :param: labels: 标注 :param: data_weights: 训练集样本权重 :param: step_number: 迭代次数, 亦即设置阈值每一步的步长 :return: stump: 决策树桩, 用dict实现 :return: return_prediction: 预测的标注值 :return: min_error: 最小损失函数值 决策树桩训练函数 输入: 训练集, 训练集权重, 迭代次数 输出: 决策树桩, 输出值, 最小损失 \"\"\" m, n = np.shape(data_matrix) stump = {} return_prediction = np.zeros((m, 1)) min_error = np.inf # 遍历特征 for i in range(n): min_value = data_matrix[:, i].min() max_value = data_matrix[:, i].max() step_size = (max_value - min_value) / float(step_number) # 按步长设定阈值 for j in range(-1, step_number + 1): for rule in ['lt', 'gt']: threshold = min_value + j * step_size prediction = stump_classifier(data_matrix, i, threshold, rule) # is_error用来存放是否错误的标记, 即I(prediction = labels) is_error = np.ones((m, 1)) is_error[prediction == labels] = 0 # 损失乘以归一化的权重 weighted_error = np.dot(data_weights.T, is_error) if weighted_error \u003c min_error: min_error = weighted_error return_prediction = prediction.copy() stump['feature_index'] = i stump['threshold'] = threshold stump['rule'] = rule return stump, return_prediction, min_error ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:4","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"集成分类器数据结构 import numpy as np class Model: \"\"\" :return: model_weights: np数组,弱分类器权重 :return: model_list: weak model list, 其中每个弱分类器用dict实现 \"\"\" def __init__(self, size=10): self.model_weights = np.zeros((size, 1)) self.model_list = [] ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:5","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"Adaboost训练集成分类器 import numpy as np def adaboost_train(data_matrix, labels, iteration=40): \"\"\" :param: data_matrix: (m,n) np数组, 训练集, 样本按行排列 :param: labels: (m,1) np数组 标注 :param: iteration: int 弱分类器个数 输入训练集和弱分类器个数, 输出模型 \"\"\" number = data_matrix.shape[0] data_weights = np.ones((number, 1)) / number # 初始化训练集权重为1/number m = Model(iteration) # 初始化模型权重为0 for i in range(iteration): stump, predictions, weighted_error = st.create_stump(data_matrix, labels, data_weights) m.model_list.append(stump) m.model_weights[i] = 0.5 * math.log((1.0 - weighted_error) / max(weighted_error, 1e-16)) data_weights = data_weights * np.exp(-1.0 * m.model_weights[i] * labels * predictions) data_weights = data_weights / np.sum(data_weights) return m ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:6","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"集成分类器 import numpy as np def adaboost_classify(input_matrix, m): \"\"\" :param: data_matrix: (m,n) np数组,测试集, 样本按行排列 :param: m: 模型 :return: models_output: (m,1) np数组,强分类器输出值 ensemble model, 输入训练集, 返回输出结果 \"\"\" models_output = np.zeros((input_matrix.shape[0], 1)) for i in range(len(m.model_list)): model_prediction = st.stump_classifier(input_matrix, m.model_list[i]['feature_index'], m.model_list[i]['threshold'], m.model_list[i]['rule']) models_output += m.model_weights[i] * model_prediction return models_output ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:7","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"分类器测试函数 import numpy as np def adaboost_test(data_matrix, labels, m): \"\"\" :param: data_matrix: 测试集, 样本按行排列 :param: labels: 标注 输入测试集和模型, 输出模型参数, 输出结果和正确率, 返回输出结果 \"\"\" models_output = adaboost_classify(data_matrix, m) i_vec = (np.sign(models_output) == labels).astype(int) error_rate = 1 - np.count_nonzero(i_vec) / float(data_matrix.shape[0]) print 'model weights:'+'\\n', m.model_weights, '\\n'+'models_output:'+'\\n', models_output, '\\n'+'error rate: ', error_rate return models_output ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:8","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"保存模型参数到json文件 import json def save_model(m): weights_json = json.dumps(m.model_weights.tolist(), indent=4, separators=(',', ': ')) models_json = json.dumps(m.model_list, indent=4, separators=(',', ': ')) with open(\"model/model_weights.json\", 'w') as fp: fp.write(weights_json) with open(\"model/model_list.json\", 'w') as fp: fp.write(models_json) def load_model(): with open(\"model/model_weights.json\") as fp: weights_json = json.loads(fp.read()) weights_list = np.array(weights_json) size = weights_list.shape[0] model = ab.Model(size) model.model_weights = weights_list with open(\"model/model_list.json\") as fp: model_list = json.loads(fp.read()) model.model_list = model_list return model ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:9","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"测试用例 import numpy as np import adaboost as ab # adaboost训练脚本 import test_toolkit as tt # 测试用工具包, 主要用于数据预处理(未展示) import plot_roc as plot # 绘制ROC曲线, 评估模型优劣(未展示) # 载入数据 data_matrix = tt.load_data('test_data/horseColicTest2.txt') # 预处理, 测试集/训练集分为8:2 train_matrix, cv_matrix, test_matrix = tt.split_data(data_matrix, (0.8, 0.0)) train_matrix_x, class_vector = tt.separate_x_y(train_matrix) 1. 自己训练- m = ab.adaboost_train(train_matrix_x, class_vector) --2. 载入预训练- # m = load_model() -测试模型泛化 test_matrix_x, test_labels = tt.separate_x_y(test_matrix) results = ab.adaboost_test(test_matrix_x, test_labels, m) # 绘制ROC曲线 plot.plot_roc(results, test_labels) # 保存模型 save_model(m) 训练结果 models_output: [[ 1. -1. 1. 1. -1. -1. 1. 1. 1. 1. 1. 1. 1. 1.]] labels: [[ 1. -1. 1. 1. -1. -1. 1. 1. 1. 1. 1. 1. 1. 1.]] error rate: 0.0 AUC曲线完美, 准确率也是100%.在如此数据缺失, 且特征多的情况下还能达到这么高的效果, 显示了adaboost的强大 ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:2:10","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"2.2 caffe简介 Caffe，全称Convolution Architecture For Feature Extractioncaffe是一个清晰，可读性高，快速的深度学习框架。 使用MNIST手写字体数据集训练Lenet cd $CAFFE_ROOT ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh 该脚本会下载好训练集和测试集并在./examples/mnist/下生成两个lmdb文件. caffe生成的神经网络是通过prototxt文件定义的, 其中涉及每一层的参数, caffe可以通过这些参数自动生成对应的层并组合成卷积神经网络. 其中Lenet的prototxt的卷积层定义如下: layer { name: \"conv1\" type: \"Convolution\" param { lr_mult: 1 } param { lr_mult: 2 } convolution_param { num_output: 20 kernel_size: 5 stride: 1 weight_filler { type: \"xavier\" } bias_filler { type: \"constant\" } } bottom: \"data\" top: \"conv1\" } 简单介绍下重要参数含义: num_output: 输出层维度 kernel_size: 卷积核大小 stride: 卷积层步长 其他训练通用参数定义在lenet_solver.prototxt中: # The train/test net protocol buffer definition net: \"examples/mnist/lenet_train_test.prototxt\" test_iter: 100 # Carry out testing every 500 training iterations. test_interval: 500 # The base learning rate, momentum and the weight decay of the network. base_lr: 0.01 momentum: 0.9 weight_decay: 0.0005 # 学习率 lr_policy: \"inv\" gamma: 0.0001 power: 0.75 # Display every 100 iterations display: 100 # 最大迭代次数 max_iter: 5000 # 5000次迭代之后保存模型快照 snapshot: 5000 snapshot_prefix: \"examples/mnist/lenet\" # 使用CPU模式 solver_mode: CPU 运行完毕之后显示 I0126 17:32:32.171516 18290 solver.cpp:246] Iteration 10000, loss = 0.00453533 I0126 17:32:32.171550 18290 solver.cpp:264] Iteration 10000, Testing net (#0) I0126 17:32:40.498195 18290 solver.cpp:315] Test net output #0: accuracy = 0.9903 I0126 17:32:40.498236 18290 solver.cpp:315] Test net output #1: loss = 0.0309918 (* 1 = 0.0309918 loss) I0126 17:32:40.498245 18290 solver.cpp:251] Optimization Done. I0126 17:32:40.498249 18290 caffe.cpp:121] Optimization Done. 说明准确率达到了99.03%, 损失函数下降到了0.00453533. 使用Faster R-CNN进行物体识别 使用了在VOC2006数据集上预训练的模型, 运行demo如下: 3. Demo展示 完成一篇署名博客，博客内容至少包括课程学习心得总结、通过commit/pr等说明自己的功劳和苦劳、提供自己的版本库URL并附上安装运行方法和Demo过程截图、其他重要事项等。 项目版本库 为了将sklearn集成到BloodTestReportOCR中, 还将训练好的模型保存为age.pkl和gender.pkl并在view.py中调用 保存模型 # 保存预训练模型 from sklearn.externals import joblib joblib.dump(clf, 'gender.pkl') # 其他代码 joblib.dump(clf, 'age.pkl') 调用保存的预训练模型预测 # 在view.py中调用预训练的模型 from sklearn.externals import joblib import numpy as np # 载入模型 age_clf = joblib.load('age.pkl') gender_clf = joblib.load('gender.pkl') # 预测函数 def predict(arr): # 根据预训练的模型分别从原始数据中选取不同的特征 age_vector = [arr[0][i] for i in [2, 0, 14, 9, 18, 17]] gender_vector = [arr[0][i] for i in [13, 11, 17]] # 按照选取的特征预测 gender = gender_clf.predict(gender_vector) age = age_clf.predict(age_vector) return gender[0], age[0] ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:3:0","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"3.1 安装 # 安装numpy, sudo apt-get install python-numpy # http://www.numpy.org/ # 安装opencv sudo apt-get install python-opencv # http://opencv.org/ ##安装OCR和预处理相关依赖 sudo apt-get install tesseract-ocr sudo pip install pytesseract sudo apt-get install python-tk sudo pip install pillow # 安装Flask框架、mongo sudo pip install Flask sudo apt-get install mongodb # 如果找不到可以先sudo apt-get update sudo service mongodb started sudo pip install pymongo # 用sklearn进行预测需要的框架 sudo apt-get install cython python-scipy pip install -U scikit-learn pip install pandas ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:4:0","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"3.2 运行 cd BloodTestReportOCR python view.py # upload图像,在浏览器打开http://yourip:8080 提交图片: 生成报告: 预测性别年龄: 4. 课程学习心得总结 本次课程学习了以下知识 ​课程项目A1a：神经网络实现手写字符识别系统 学习了基本前后端输入图片进行检测的原理 学习了具有一个隐含层的最简单神经网络的构造: 前向传播, 反向传播的原理 课程项目A2：血常规检验报告的图像OCR识别 学习了使用github合作项目的流程: 提交pull request, 从源版本库fetch最新版本, merge 不同分支 课程项目A3：根据血常规检验的各项数据预测年龄和性别 学习了使用sklearn进行预测 学习了最基本的数据清洗: 归一化, 标准化, 缺失值填充 学习了pandas数据处理包的使用 学习了numpy的使用 Presentation 学习了如何使用python编写adaboost, 如何评估模型 学习了如何调用caffe, 订制神经网络 本课程学习到了很多知识,对我自己而言, 以python和机器学习的理论知识为主. 孟宁老师也是一个很有风度的老师, 在课堂上和同学们交流, 观点鲜明, 善于抓住每个同学分享知识的重点, 理清楚来龙去脉之后再与其他同学分享, 效果拔群. 感谢孟宁老师给我们带来有如此丰富内容的课程. ","date":"2017-01-05","objectID":"/2017-01-05-np2016/:5:0","tags":null,"title":"2016秋 网络程序设计 课程学习心得总结","uri":"/2017-01-05-np2016/"},{"categories":null,"content":"YOLO做目标检测很快 ","date":"2016-12-26","objectID":"/code/2016-12-26-yolo-darknet/:0:0","tags":["deep learning"],"title":"Blazing Fast! 使用YOLO+Darknet进行目标检测","uri":"/code/2016-12-26-yolo-darknet/"},{"categories":null,"content":"系统配置 系统: Ubuntu 16.04 x64 显卡: gtx960m + i5核显 内存: 8G CPU: i5-6300HQ ","date":"2016-12-26","objectID":"/code/2016-12-26-yolo-darknet/:1:0","tags":["deep learning"],"title":"Blazing Fast! 使用YOLO+Darknet进行目标检测","uri":"/code/2016-12-26-yolo-darknet/"},{"categories":null,"content":"使用独立显卡 安装驱动和切换工具 sudo apt-get install nvidia-364 nvidia-prime 驱动版本可以到软件更新-\u003e其他驱动里查看 安装过程中可能需要关闭图形界面, 按Ctrl + Alt + F1即可, 输入sudo service lightdm stop 安装完毕之后再sudo service lightdm start 安装完毕之后重启, 启动NVIDIA X Server Settings, 可以在nvidia prime下看到切换显卡的功能 ","date":"2016-12-26","objectID":"/code/2016-12-26-yolo-darknet/:2:0","tags":["deep learning"],"title":"Blazing Fast! 使用YOLO+Darknet进行目标检测","uri":"/code/2016-12-26-yolo-darknet/"},{"categories":null,"content":"安装CUDA 8 去官网下载安装包并按照指令安装, 注意使用runfile方案 注意其中有一个选项是问是否需要安装自带的驱动(似乎是361), 选择no, 因为第一步已经安装过合适的驱动了 安装完毕之后将目录加入环境变量. 往~\\.bashrc下添加以下语句 export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 使用nvidia-smi指令测试一下, 应该能看到独立显卡的信息 ","date":"2016-12-26","objectID":"/code/2016-12-26-yolo-darknet/:3:0","tags":["deep learning"],"title":"Blazing Fast! 使用YOLO+Darknet进行目标检测","uri":"/code/2016-12-26-yolo-darknet/"},{"categories":null,"content":"安装opencv 3.1 参考caffe的教程 参考官方文档 安装依赖 sudo apt-get install --assume-yes build-essential cmake git sudo apt-get install --assume-yes build-essential pkg-config unzip ffmpeg qtbase5-dev python-dev python3-dev python-numpy python3-numpy sudo apt-get install --assume-yes libopencv-dev libgtk-3-dev libdc1394-22 libdc1394-22-dev libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev sudo apt-get install --assume-yes libavcodec-dev libavformat-dev libswscale-dev libxine2-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev sudo apt-get install --assume-yes libv4l-dev libtbb-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev sudo apt-get install --assume-yes libvorbis-dev libxvidcore-dev v4l-utils 下载并编译opencv 去官方git repo下载最新的opencv 3.1源码 解压, 进入目录, 输入以下指令编译 mkdir build cd build/ cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local .. make -j $(($(nproc) + 1)) 输入以下指令安装 sudo make install sudo /bin/bash -c 'echo \"/usr/local/lib\" \u003e /etc/ld.so.conf.d/opencv.conf' sudo ldconfig sudo apt-get update 重启系统 错误解决 编译时可能会出现/usr/bin/ld: 找不到 -lippicv(ubuntu 16.04 LTS错误, 是因为CUDA8.0和opencv的兼容问题 进入 /home/ds/opencv-3.1.0/3rdparty/ippicv/unpack/ippicv_lnx/lib/intel64目录, cp liboppicv.a /usr/local/lib 即可. 参考这里 ","date":"2016-12-26","objectID":"/code/2016-12-26-yolo-darknet/:4:0","tags":["deep learning"],"title":"Blazing Fast! 使用YOLO+Darknet进行目标检测","uri":"/code/2016-12-26-yolo-darknet/"},{"categories":null,"content":"安装YOLO 下载并编译darknet git clone https://github.com/pjreddie/darknet cd darknet make 下载预训练模型 wget http://pjreddie.com/media/files/yolo.weights 检测 ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg ","date":"2016-12-26","objectID":"/code/2016-12-26-yolo-darknet/:5:0","tags":["deep learning"],"title":"Blazing Fast! 使用YOLO+Darknet进行目标检测","uri":"/code/2016-12-26-yolo-darknet/"},{"categories":null,"content":"使用YOLO进行实时目标检测 编辑Makefile, 置GPU=1, make后再置OPENCV=1, make, 编译完成 运行以下指令: ./darknet detector demo cfg/coco.data cfg/yolo.cfg yolo.weights 将会打开摄像头进行实时目标检测 ","date":"2016-12-26","objectID":"/code/2016-12-26-yolo-darknet/:6:0","tags":["deep learning"],"title":"Blazing Fast! 使用YOLO+Darknet进行目标检测","uri":"/code/2016-12-26-yolo-darknet/"},{"categories":null,"content":"简单介绍一下本站的建立过程 ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:0:0","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"部署Hexo 系统: Ubuntu 16.04 LTS ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:1:0","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"0. 安装Git(略) ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:1:1","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"1. 安装 Node.js 最好使用nvm来安装Node.js 方法一: cURL: $ curl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh 方法二: Wget: $ wget -qO- https://raw.githubusercontent.com/creationix/nvm/master/install.sh | sh 安装完毕之后重启终端, 执行: $ nvm install stable ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:1:2","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"2. 安装Hexo并建站 安装hexo. 官方文档 $ npm install -g hexo-cli 初始化blog目录, 目录名为\u003cfolder\u003e, 可以任选 $ hexo init \u003cfolder\u003e $ cd \u003cfolder\u003e $ npm install 这样建站部分就完毕了, 可以看到如下的目录结构 . ├── _config.yml ├── package.json ├── scaffolds ├── source | ├── _drafts | └── _posts └── themes 简单介绍一下主要部分的作用 _config.yml: 站点配置文件, 所有主要配置都需要在这里说明 source/_posts: 所有写好的markdown文件都会保存在这里 themes: blog主题存放地址, 一般使用git来对主题进行版本控制 ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:1:3","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"安装next主题 项目主页 安装: 首先确保在blog的根目录\u003cfolder\u003e下, 执行: $ git clone https://github.com/iissnan/hexo-theme-next themes/next 使用: 打开站点配置文件_config.yml, 更改theme关键字的值为next, 修改后显示为: theme: next 配置: 在themes/next目录下也有一个叫_config.yml的文件, 为了与站点配置文件区分开来, 称之为主题配置文件. 有关主题配置的详细信息见项目主页 ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:2:0","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"部署到github上 除了部署到github, hexo还支持Heroku, Rsync, OpenShift等平台 详见官方文档, 本文只介绍怎样部署到github上 为了将hexo生成的静态网页及时部署到github上, 需要以下配置: 安装hexo-deployer-git: 首先确保在blog的根目录\u003cfolder\u003e下, 执行: $ npm install hexo-deployer-git --save 修改站点配置文件的以下关键词: deploy: type: git repo: \u003crepository url\u003e branch: [branch] message: [message] repo是github上的repo地址, 一般使用\u003c你的github用户名\u003e.github.io这样的格式 branch是分支, 一般选择master message 是每次commit的时候默认填写的信息,可以注释掉他, 使用默认格式 ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:3:0","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"使用 首先确保在blog的根目录\u003cfolder\u003e下 写文章: $ hexo new helloworld, 此时会在source/_posts/下生成一个文件名为helloworld.md的文档, 直接编辑该文档即可 渲染: $ hexo g, 此时会生成渲染好的文章, 在public目录下 测试: $ hexo s, 此时打开http://localhost:4000 可以预览效果 部署: $ hexo d, 会将文章部署到github上 绑定域名(只需要执行一次): 在repo的setting下启用GitHub Pages即可 ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:4:0","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"Tricks 未完待续 ","date":"2016-12-22","objectID":"/code/2016-12-22-hexo-blog-based-on-next-theme/:5:0","tags":["blog","hexo"],"title":"Github Page + Hexo + next主题 blog网站的搭建","uri":"/code/2016-12-22-hexo-blog-based-on-next-theme/"},{"categories":null,"content":"jupyter可以看作是一款强化版的python terminal, 实时运行命令脚本, 实时反映结果, 非常适用于科学计算和数据分析. 除此之外通过安装kernel也可以用其他语言工作 ","date":"2016-12-22","objectID":"/code/2016-12-22-jupyter-notebooks/:0:0","tags":["python"],"title":"Jupyter Notebooks 简介","uri":"/code/2016-12-22-jupyter-notebooks/"},{"categories":null,"content":"安装(参考quickstart) 装anaconda, 见官方主页 anaconda自带了jupyter, 使用jupyter notebook会在http://localhost:8888启动 生成配置文件: jupyter notebook --generate-config, 配置文件会生成在~/.jupyter下 ","date":"2016-12-22","objectID":"/code/2016-12-22-jupyter-notebooks/:0:1","tags":["python"],"title":"Jupyter Notebooks 简介","uri":"/code/2016-12-22-jupyter-notebooks/"},{"categories":null,"content":"快捷键参考 基本操作 Enter: 进入编辑模式, cell标签会变成绿色 Esc: 进入选择模式, cell标签会变成蓝色 上下左右: 在cell之间移动 Shift-­Enter: 运行cell, 并进入下一个cell 编辑cell的操作 X: 剪切选择的cell C: 复制选择的cell V: 在下一行粘贴所在的cell A/B: 在光标所在位置上面/下面插入cell D,D:连续按两下D, 删除所在cell Shift-M: 合并所在cell和下面的cell Ctrl-S­hif­t-S­ubtract: 在光标所在位置分裂cell ","date":"2016-12-22","objectID":"/code/2016-12-22-jupyter-notebooks/:0:2","tags":["python"],"title":"Jupyter Notebooks 简介","uri":"/code/2016-12-22-jupyter-notebooks/"},{"categories":null,"content":"详见我的repo ","date":"2016-12-10","objectID":"/code/2016-12-10-build-fast-rcnn-under-docker/:0:0","tags":["docker","deep learning"],"title":"基于Docker的Faster R-CNN, CPU-Only环境配置","uri":"/code/2016-12-10-build-fast-rcnn-under-docker/"},{"categories":null,"content":"1. 系统配置 系统: VMware下的Ubuntu 14.04 LTS x64, 需要分配至少4GB内存 坑: 内存过小(\u003c1G)编译时会出错: g++: internal compiler error: Killed (program cc1plus) 预测时(\u003c2G)也会出错: *ptr host allocation of size 1382400000 failed ","date":"2016-12-10","objectID":"/code/2016-12-10-build-fast-rcnn-under-docker/:1:0","tags":["docker","deep learning"],"title":"基于Docker的Faster R-CNN, CPU-Only环境配置","uri":"/code/2016-12-10-build-fast-rcnn-under-docker/"},{"categories":null,"content":"2. 安装Docker 参考官方指南 配置用户组, ${USER}为用户名 sudo groupadd docker sudo gpasswd -a ${USER} docker sudo service docker restart ","date":"2016-12-10","objectID":"/code/2016-12-10-build-fast-rcnn-under-docker/:2:0","tags":["docker","deep learning"],"title":"基于Docker的Faster R-CNN, CPU-Only环境配置","uri":"/code/2016-12-10-build-fast-rcnn-under-docker/"},{"categories":null,"content":"3. 下载镜像并安装额外依赖 先下载镜像 docker pull tleyden5iwx/caffe-cpu-master 启动镜像, 创建容器 docker run -i -t tleyden5iwx/caffe-cpu-master /bin/bash 在容器中安装额外依赖. docker镜像中已经安装了cython sudo apt-get install python-opencv sudo pip install easydict 配置github git config --global user.name \"yourname\" git config --global user.email yourname@example.com ","date":"2016-12-10","objectID":"/code/2016-12-10-build-fast-rcnn-under-docker/:3:0","tags":["docker","deep learning"],"title":"基于Docker的Faster R-CNN, CPU-Only环境配置","uri":"/code/2016-12-10-build-fast-rcnn-under-docker/"},{"categories":null,"content":"4. 下载源码并运行demo 下载源码(这里使用的是我的源码) git clone --recursive https://github.com/shawnau/py-faster-rcnn-cpu.git 编译(假设根目录为$FRCN_ROOT) 重命名$FRCN_ROOT/caffe-fast-rcnn下的Makefile.config.example 为 Makefile.config cd $FRCN_ROOT/lib make cd $FRCN_ROOT/caffe-fast-rcnn make -j8 \u0026\u0026 make pycaffe 下载预训练模型(注:可以直接用迅雷拖下来解压至data/) cd $FRCN_ROOT ./data/scripts/fetch_faster_rcnn_models.sh 运行demo cd $FRCN_ROOT ./tools/demo.py --cpu --net zf 其中 --cpu 代表使用caffe的CPU模式, –net zf 代表使用zf网络 默认的vgg16网络在本机上会被killed在载入模型阶段, 应该为内存不足所致 速度: 在i5下4秒一张图, 差了大约1.5个数量级 结果都以tag_原图名称.jpg保存在data/demo/下 ","date":"2016-12-10","objectID":"/code/2016-12-10-build-fast-rcnn-under-docker/:4:0","tags":["docker","deep learning"],"title":"基于Docker的Faster R-CNN, CPU-Only环境配置","uri":"/code/2016-12-10-build-fast-rcnn-under-docker/"},{"categories":null,"content":"5. 主要修改 和CPU/GPU相关的: lib/fast_rcnn/config.py lib/fast_rcnn/nms_wrapper.py lib/setup.py tools/test_net.py, train_net.py caffe-fast-rcnn/Makefile.config 和输出相关的: tools/demo.py 修改输出保存为图片 ","date":"2016-12-10","objectID":"/code/2016-12-10-build-fast-rcnn-under-docker/:5:0","tags":["docker","deep learning"],"title":"基于Docker的Faster R-CNN, CPU-Only环境配置","uri":"/code/2016-12-10-build-fast-rcnn-under-docker/"},{"categories":null,"content":"Refer How to setup with CPU ONLY mode I got the error when running demo.py Caffe failed with py-faster-rcnn demo.py on TX1 ","date":"2016-12-10","objectID":"/code/2016-12-10-build-fast-rcnn-under-docker/:6:0","tags":["docker","deep learning"],"title":"基于Docker的Faster R-CNN, CPU-Only环境配置","uri":"/code/2016-12-10-build-fast-rcnn-under-docker/"},{"categories":null,"content":"docker是一款很牛逼的虚拟化工具 1. 安装 要求 64位系统 kernel版本为3.10或以上 以下使用的是Ubuntu 14.04 LTS 64位 参考官方文档 2. 使用(需添加用户组避免每次都sudo) 查看当前镜像列表 docker images 启动镜像: 创建容器并进入容器内的shell docker run -i -t \u003cdockername\u003e /bin/bash docker run - 运行一个容器 -t - 分配一个（伪）tty (link is external) -i - 交互模式 (so we can interact with it) - 使用 dockername 镜像 /bin/bash - 运行命令 bash shell 离开容器 关闭容器: Ctrl+C 用户通过 exit 命令或 Ctrl+d 来退出终端时，所创建的容器立刻终止。 离开容器, 让容器继续运行: Ctrl+P, Ctrl+Q 查看当前运行的容器 sudo docker ps -a -a为查看所有的容器，包括已经停止的。 重新连接容器 docker attach \u003ccontainerID\u003e, 其中containerID是容器名或ID, 可以使用缩写 从停止的容器恢复 docker start \u003ccontainerID\u003e 从容器创建Docker镜像 docker commit -m \"commit\" -a \"author\" 0b2616b0e5a8 ouruser/sinatra:v2 其中0b2616b0e5a8为容器ID, ouruser/sinatra为用户/镜像名, :v2为版本号 3. 其他 容器-宿主互传文件 docker cp foo.txt mycontainer:/foo.txt docker cp mycontainer:/foo.txt foo.txt 启动镜像时挂载host的文件夹 docker run -v /host/directory:/container/directory \u003c镜像名\u003e 通过导出导入的原理缩减镜像大小Reference # export the container to a tarball docker export \u003cCONTAINER ID\u003e \u003e /home/export.tar # import it back cat /home/export.tar | docker import - some-name:latest to be continued… ","date":"2016-12-08","objectID":"/code/2016-12-06-docker-abc/:0:0","tags":["docker"],"title":"Docker速查","uri":"/code/2016-12-06-docker-abc/"},{"categories":null,"content":"Landslide 是个可以把markdown文件变成幻灯片直接在浏览器里播放的工具. 页面之间直接使用---间隔即可 环境配置 pip install -U markdown pip install -U docutils pip install Jinja2 pip install Pygments pip install -U landslide Mathjax的使用 使用-m指令支持mathjax渲染, 可以直接使用$$和$作为line和inline渲染, 和本blog无缝衔接. (注: 似乎和markdown语法仍有冲突, 其中\\\\需要改成\\\\\\, _需要改成\\_, 未测试不改的效果) landslide -m README.md 使用 左右键控制播放即可 ","date":"2016-11-25","objectID":"/code/2016-11-25-landslide/:0:0","tags":["markdown"],"title":"Landslide - Markdown to Slide","uri":"/code/2016-11-25-landslide/"},{"categories":null,"content":"RT 假设白井黑子从[misaki/railgun.git]处fork了炮姐的项目到自己的项目[kuroko/railgun.git] 过了段时间, 炮姐已经更新了好几个版本黑子才想起来, 于是她想把自己fork的项目也同步更新, 简单的解决办法: # 先把自己的项目clone到本地 git clone kuroko/railgun.git # 再创建一个upstream分支, 指向炮姐的项目 git remote add upstream misaki/railgun.git # 从炮姐那取得项目更新 git fetch upstream # 和本地的master分支合并 git merge upstream/master Refer: https://help.github.com/articles/fork-a-repo/ ","date":"2016-11-22","objectID":"/code/2016-11-22-git-update-forked/:0:0","tags":["git"],"title":"git 从fork的项目更新自己的项目的简易手段","uri":"/code/2016-11-22-git-update-forked/"},{"categories":null,"content":"RT 1. 配置 Mac OS Sierra 10.12.1 2. 安装 安装CUDA8 (略) brew install -vd snappy leveldb gflags glog szip lmdb brew tap homebrew/science brew install hdf5 opencv brew edit opencv, 改变如下两行: -DPYTHON_LIBRARY=#{py_prefix}/lib/libpython2.7.dylib -DPYTHON_INCLUDE_DIR=#{py_prefix}/include/python2.7 安装protobuf和boost-python brew install --build-from-source --with-python -vd protobuf brew install --build-from-source -vd boost boost-python 下载NVIDIA Mac drivers 下载caffe cd ~/Documents git clone https://github.com/BVLC/caffe.git 打开Makefile.config, 配置BLAS BLAS_INCLUDE := /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/Headers BLAS_LIB := /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A 去掉CPU_ONLY的注释 去掉USE_LEVELDB := 0的注释 注:以下这两步似乎已经解决 下载XCode Command Line Tools for 7.3, 因为NVIDIA 暂时还不支持Xcode 8.0的. 切换到7.3的tools: sudo xcode-select --switch /Library/Developer/CommandLineTools 待会还要删掉它并使用以下语句切回来, 不然你会用不了brew sudo xcode-select -s /Applications/Xcode.app/Contents/Developer 3. 编译 make all make test make runtest make pycaffe make pytest 除了一些warning, 似乎并没问题. 但最后一步make pytest出现了如下问题: /bin/sh: line 1: 67818 Segmentation fault: 11 python -m unittest discover -s caffe/test make: *** [pytest] Error 139 似乎是anaconda的protobuf版本问题, 这里需要一个有效hack, - (详情见这里)[https://github.com/BVLC/caffe/issues/2092#issuecomment-153986008] 注释掉/python/caffe/proto/caffe_pb2.py 所有 “systax = proto \" 的行, 问题解决 4. 测试样例 # 下载测试集 cd data/ilsvrc12/ ./get_ilsvrc_aux.sh # 下载模型 cd ../../models/bvlc_reference_caffenet/ curl http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel \u003e bvlc_reference_caffenet.caffemodel 这200多m的文件我们还是用迅雷吧.然后运行下. 回到caffe根目录,执行 ./build/examples/cpp_classification/classification.bin models/bvlc_reference_caffenet/deploy.prototxt models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel data/ilsvrc12/imagenet_mean.binaryproto data/ilsvrc12/synset_words.txt examples/images/cat.jpg 参考资料 https://gist.github.com/doctorpangloss/f8463bddce2a91b949639522ea1dcbe4 http://caffe.berkeleyvision.org/install_osx.html http://caffe.berkeleyvision.org/install_osx.html https://github.com/BVLC/caffe/issues/3608 Written with StackEdit. ","date":"2016-11-12","objectID":"/code/2016-11-12-install-caffe-under-macos/:0:0","tags":["caffe","deep learning"],"title":"在mac os 10.12 Sierra下安装caffe","uri":"/code/2016-11-12-install-caffe-under-macos/"},{"categories":null,"content":"Expectation Maximization Algorithm ","date":"2016-08-26","objectID":"/2016-08-26-expectation-maximization/:0:0","tags":["machine learning","EM算法","统计学习方法"],"title":"EM算法","uri":"/2016-08-26-expectation-maximization/"},{"categories":null,"content":"1. 三硬币模型 假设有三枚硬币A, B, C, 这些硬币正面出现的概率是$p_a$, $p_b$, $p_c$. 进行如下掷硬币试验: 掷硬币A 若A为正面则选择B, 否则选择C 掷选出的硬币, 记录结果, 正面记作1, 反面记作0 独立重复n次1~3 假设只能观测到结果, 不能观察A的情况,如何估计$p_a$, $p_b$, $p_c$? 三硬币模型中, 每一次掷硬币的过程可以写作 $$ \\begin{align} P(y | \\theta) \u0026 = \\sum\\limits_z P(y, z | \\theta) = \\sum\\limits_z P(y|z,\\theta) P(z |\\theta) \\\\ \u0026 = p_a p_b^y (1 - p_b)^{1-y} + (1 - p_a) p_c^y (1-p_c)^{1 - y} \\end{align} $$ 其中 $\\theta = (p_a, p_b, p_c)$, y为观测结果, z为掷硬币A的结果(即隐变量). 推广到掷n次硬币, 观测序列为$Y = (y_1, y_2, \\cdots y_n)^T$的似然函数(likelihood function)为 $$ \\begin{align} P(Y | \\theta) \u0026 = \\prod_{j=1}^n P(y_j | \\theta) = \\prod_{j=1}^n\\sum\\limits_z P(y_j, z | \\theta)\\\\ \u0026= \\prod_{j=1}^n \\left\\lbrace P(y_j, z=1 | \\theta) + P(y_j, z=0 | \\theta) \\right\\rbrace \\\\ \u0026 = \\prod_{j=1}^n [p_a p_b^{y_j} (1 - p_b)^{1-y_j} + (1 - p_a) p_c^{y_j} (1-p_c)^{1 - y_j}] \\end{align} $$ 使用极大似然估计(MLE)来估计参数: $$ \\hat \\theta = arg\\max\\limits_{\\theta} logP(Y | \\theta) $$ 该问题没有解析解, 只能通过迭代求解, 而EM算法就是用于迭代求解该问题的方法之一. 因此EM算法就是含有隐变量的概率模型参数的极大似然估计 ","date":"2016-08-26","objectID":"/2016-08-26-expectation-maximization/:0:1","tags":["machine learning","EM算法","统计学习方法"],"title":"EM算法","uri":"/2016-08-26-expectation-maximization/"},{"categories":null,"content":"2. EM算法 2.1 模型推广 将未观测到的硬币A序列推广为任意隐变量序列$Z = (z_1, z_2, \\cdots z_n)^T$, 其中$z$为隐变量. 目标是极大化观测序列$Y = (y_1, y_2, \\cdots y_n)^T$关于参数$\\theta$的对数似然函数, 即极大化 $$ \\begin{align} L(\\theta) \u0026= log P(Y | \\theta) = log \\sum\\limits_Z P(Y, Z | \\theta) \\\\ \u0026= log \\sum\\limits_Z [ P(Y|Z,\\theta) P(Z |\\theta) ] \\end{align} \\tag{2.1} $$ 注意到式中需要对隐变量序列的所有可能求和, 但是隐变量往往是未知的, 且求和之后的函数难以处理, 因此直接极大化会遇到困难. 2.2 Q函数的推导 假设使用迭代算法估计 $\\theta$ , 其中第 $i$ 次迭代得到的参数估计值为$\\theta_i$, 使用其估计得到的似然函数必然小于 $L(\\theta)$, 设法取其下界: 利用Jenson不等式, 对于譬如log一类的凹函数(concave function)有: $$ f(\\sum\\limits_i a_i x_i) \\ge \\sum\\limits_i a_i f(x_i) \\text{ ,in which} \\sum\\limits_i a_i = 1 $$ 故 $$ \\begin{align} L(\\theta) - L(\\theta_i) \u0026= log \\sum\\limits_z [ P(Y|Z,\\theta) P(Z |\\theta) ] - log P(Y | \\theta_i) \\\\ \u0026= log \\sum\\limits_Z P(Z|Y,\\theta_i ) \\frac {P(Y|Z,\\theta) P(Z |\\theta)}{P(Z|Y,\\theta_i)} - log P(Y | \\theta_i) \\\\ \u0026\\ge \\sum\\limits_Z P(Z|Y,\\theta_i ) log\\frac {P(Y|Z,\\theta) P(Z |\\theta)}{P(Z|Y,\\theta_i)} - log P(Y | \\theta_i) \\\\ \u0026= \\sum\\limits_Z P(Z|Y,\\theta_i ) log \\frac {P(Y|Z,\\theta) P(Z |\\theta)}{P(Z|Y,\\theta_i) P(Y|\\theta_i)} \\end{align} $$ 其中第二行推导利用了$\\sum\\limits_Z P(Z|Y,\\theta_i ) = 1$ 的性质, 而第四行把$log P(Y | \\theta_i)$ 化为 $\\left\\lbrace \\sum\\limits_Z P(Z|Y,\\theta_i )\\right\\rbrace log P(Y | \\theta_i)$ 以便和前一项合并. 由此可得 $L(\\theta)$ 的下界: $$ B(\\theta, \\theta_i) = L(\\theta_i) + \\sum\\limits_Z P(Z|Y,\\theta_i ) log \\frac {P(Y|Z,\\theta) P(Z |\\theta)}{P(Z|Y,\\theta_i) P(Y|\\theta_i)} \\tag{2.2} $$ 易知 $B(\\theta_i, \\theta_i) = L(\\theta_i)$, 对任意$\\theta_{i+1}$, 若有$B(\\theta_{i+1}, \\theta_i) \u003e B(\\theta_i, \\theta_i)$, 则: $$ L(\\theta_{i+1}) \\ge B(\\theta_{i+1}, \\theta_i) \u003e B(\\theta_i, \\theta_i) = L(\\theta_i) \\tag{2.3} $$ 由(2.3)可知, 为了取尽量大的$L(\\theta_{i+1})$, 就需要取其尽量大的下界$B(\\theta_{i+1}, \\theta_i)$. 省略和 $\\theta$ 无关的变量, 有: $$ \\begin{align} \\theta_{i+1} \u0026= arg\\max\\limits_\\theta B(\\theta, \\theta_i) \\\\ \u0026= arg\\max\\limits_\\theta \\sum\\limits_Z P(Z|Y,\\theta_i ) log P(Y, Z | \\theta) \\\\ \u0026= arg\\max\\limits_\\theta Q(\\theta, \\theta_i) \\end{align} $$ 由此可知, EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大值的算法, 但并不能求得全局极大值. 2.3 Q函数的定义 $$ \\begin{align} Q(\\theta, \\theta_i) \u0026= E_Z [log P(Y, Z | \\theta) | Y, \\theta_i]\\\\ \u0026= \\sum\\limits_Z P(Z|Y,\\theta_i ) log P(Y, Z | \\theta) \\end{align} \\tag{2.4} $$ 完全观测数据的对数似然函数 $log P(Y, Z | \\theta)$ 在给定观测数据 $Y$ 和当前估计参数 $\\theta_i$ 下, 对隐变量序列 $Z$ 的条件概率分布 $P(Z|Y,\\theta_i )$ 的期望称为Q函数. 2.4 EM算法 输入: 观测变量$Y$, 隐变量取值范围$Z$, 条件分布$P(Z|Y,\\theta_i )$, 联合分布 $P(Y, Z | \\theta)$ 输出: 模型参数$\\theta$ (1) 初始化$\\theta_0$ (2) E步: 在第 $i$ 步计算出的参数为 $\\theta_i$, 第 $i+1$ 步, 计算 $$ \\begin{align} Q(\\theta, \\theta_i) \u0026= E_Z [log P(Y, Z | \\theta) | Y, \\theta_i]\\\\ \u0026= \\sum\\limits_Z P(Z|Y,\\theta_i ) log P(Y, Z | \\theta) \\end{align} $$ (3) M步: 最大化$Q(\\theta, \\theta_i)$, 得到 $\\theta_{i+1} = arg\\max\\limits_\\theta Q(\\theta, \\theta_i)$ (4) 重复(2), (3)两步, 直到 $\\theta$ 收敛 简而言之, EM算法的E步: 从观测序列 $Y$ 和估计参数 $\\theta_i$ 计算出隐变量序列的概率分布 $P(Z|Y,\\theta_i )$ ,对每个观测序列 $Z$, 求出 $log P(Y, Z | \\theta)$ 的期望, 此时这是个仅含有 $\\theta$ 的函数. M步: 最大化$Q(\\theta, \\theta_i)$, 得到 $\\theta_{i+1} = arg\\max\\limits_\\theta Q(\\theta, \\theta_i)$ ","date":"2016-08-26","objectID":"/2016-08-26-expectation-maximization/:0:2","tags":["machine learning","EM算法","统计学习方法"],"title":"EM算法","uri":"/2016-08-26-expectation-maximization/"},{"categories":null,"content":"3. 利用EM算法求解三硬币模型 计算需要的参数: $Y = (1, 1, 0, 1, 0, 0, 1, 0, 1, 1)^T$ $Z = \\lbrace 1, 0 \\rbrace$ $\\theta = (p_a, p_b, p_c)$ 3.1 E步 对于单步实验: $$ \\begin{align} P(z=1 | y_j,\\theta_i ) \u0026= \\frac {P(y_j, z=1 | \\theta_i)} {P(y_j | \\theta_i)} \\text{ (Bayes公式) } \\\\ \u0026= \\frac {P(y_j, z=1 | \\theta_i)} {P(y_j, z=1 | \\theta_i) + P(y_j, z=0 | \\theta_i)} \\\\ \u0026= \\frac {p_a p_b^{y_j} (1 - p_b)^{1-y_j}} {p_a p_b^{y_j} (1 - p_b)^{1-y_j} + (1 - p_a) p_c^{y_j} (1-p_c)^{1 - y_j}} \\\\ \u0026= \\mu_j^{(i)} \\end{align} $$ $P(z=0 | y_j,\\theta_i ) = 1 - P(z=1 | y_j,\\theta_i ) = 1 - \\mu_j^{(i)}$ $$ \\begin{align} \u0026Q(\\theta, \\theta_i) \\\\ \u0026= E_Z [log P(Y, Z | \\theta) | Y, \\theta_i] \\\\ \u0026= \\sum\\limits_j E_z [logP(y_j, z|\\theta) | y_j, \\theta_i] \\\\ \u0026= \\sum\\limits_j \\sum\\limits_z P(z|y_j,\\theta_i ) log P(y_i, z | \\theta) \\\\ \u0026= \\sum\\limits_j [\\mu_j^{(i)} logP(y_j, z_j=1 | \\theta) + (1 - \\mu_j^{(i)})logP(y_j, z_j=0 | \\theta)] \\\\ \u0026= \\sum\\limits_j \\left\\lbrace \\mu_j^{(i)} log [p_a p_b^{y_j} (1 - p_b)^{1-y_j}] + (1 - \\mu_j^{(i)})log[(1 - p_a) p_c^{y_j} (1-p_c)^{1 - y_j}] \\right\\rbrace \\end{align} $$ 注: 对于以上推导的1-2步, 由于Q函数是概率的期望值, 对于单步重复试验, 整个序列的期望值可以由单步实验的期望值相加取得. 3.2 M步 其中 $\\mu_j^{(i)}$ 中包含了第 $i$ 次迭代得到的估计参数, 最优化 $Q(\\theta, \\theta_i)$ 之后得到新的参数, 即 $\\theta_{i+1}$ . 优化手段直接求导并令导数等于零(过程略): $$ p_a^{(i+1)} = \\frac {\\partial Q} {\\partial p_a} = \\frac {1}{n} \\sum\\limits_{j=1}^n \\mu_j^{(i)} \\\\ p_b^{(i+1)} = \\frac {\\partial Q} {\\partial p_b} = \\frac{\\sum\\limits_{j=1}^n \\mu_j^{(i)} y_j} {\\sum\\limits_{j=1}^n \\mu_j^{(i)}} \\\\ p_c^{(i+1)} = \\frac {\\partial Q} {\\partial p_c} = \\frac{\\sum\\limits_{j=1}^n(1 - \\mu_j^{(i)}) y_j} {\\sum\\limits_{j=1}^n (1 - \\mu_j^{(i)})} $$ 依次使用以上三式迭代, 直到收敛即可 ","date":"2016-08-26","objectID":"/2016-08-26-expectation-maximization/:0:3","tags":["machine learning","EM算法","统计学习方法"],"title":"EM算法","uri":"/2016-08-26-expectation-maximization/"},{"categories":null,"content":"4. 小结 EM算法是对极大似然估计的自然推广. 对于没有隐变量的参数估计, 直接使用最大似然估计即可. 对含有隐变量的参数估计: 首先先验地估计一组参数$\\theta_i$ 因为不知道隐变量的分布情况, 所以先根据$\\theta_i$, 使用Bayes公式求出隐变量的概率分布$P(z|y,\\theta_i )$ 根据该分布求出完全数据的对数似然函数$log P(y, z | \\theta)$的期望, 该期望即为Q函数. 最大化Q函数, 等价于最大化似然函数的下界. 获得新的 $\\theta_{i+1}$ 并进行迭代, 相当于提升了似然函数的值 ","date":"2016-08-26","objectID":"/2016-08-26-expectation-maximization/:0:4","tags":["machine learning","EM算法","统计学习方法"],"title":"EM算法","uri":"/2016-08-26-expectation-maximization/"},{"categories":null,"content":"5. Reference What is the expectation maximization algorithm? 统计学习方法 Written with StackEdit. ","date":"2016-08-26","objectID":"/2016-08-26-expectation-maximization/:0:5","tags":["machine learning","EM算法","统计学习方法"],"title":"EM算法","uri":"/2016-08-26-expectation-maximization/"},{"categories":null,"content":"主要涉及到Selenium的使用 源代码: https://github.com/shawnau/bilibili_scraper 本文中的所有技术在上一篇文章中均有介绍, 因此本文只涉及到针对bilibili主页结构的分析, 省略了技术细节 ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:0:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"0. 抓取番剧弹幕的基本步骤 b站的每个视频页面源代码中, 负责启动播放器的script标签下保存了弹幕编号(cid)以及视频编号(aid), 例如上图中的视频, 其script内容为 EmbedPlayer('player', \"http://static.hdslb.com/play.swf\", \"cid=8628660\u0026aid=5244087\u0026pre_ad=0\"); 找到了cid=8628660后, 前往comment.bilibili.tv下取得xml格式的弹幕, 例如本视频的弹幕地址为: http://comment.bilibili.tv/8628660.xml 幸运的是, b站看视频并不需要登录, 且cid都可以通过直接读取网页源代码并通过正则表达式匹配得到, 不幸的是分p视频的分p链接那一栏大部分需要点击展开按钮才能获得, 因此我们的selenium派上了用场. 综上所述, 我们需要实现以下功能: 获得网页上分p一栏所有视频的链接 打开每个链接, 获得cid, 标题等页面信息 利用cid下载弹幕 除此之外, 爬虫需要一定的容错率, 对找不到cid/难以下载弹幕的视频, 爬虫需要在重试一定次数之后主动跳过 ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:1:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"1. 旧番站点结构 打开分p在播放器上方的视频页面, 使用xpath找到id为plist的div标签, 可以发现标签下有三部分: span标签, 代表当前页面 带有超链接的a标签, 代表其他p的链接和副标题 带有class名为v-part-toggle的a标签, 代表展开按钮 由此可得我们需要的操作: span = driver.find_elements_by_xpath(p.xPaths[\"plistButton\"]) if len(span) == 0: raise Exception('No other page found') 利用plist下的span标签检查是否有分p, 如果没有则扔异常, 在异常处理中只将本页面链接添加进链接表中. 对于只有一个链接的视频也同样处理即可 # 检查是否有展开按钮, 有就按一下 unfold_button = driver.find_elements_by_class_name(p.togglename) if len(unfold_button) == 1: unfold_button[0].click() # 截取展开后的所有链接, 提取源代码 plist = driver.find_element_by_xpath(p.xPaths[\"plist\"]) list_source = plist.get_attribute('innerHTML') 首先有展开按钮就先点击展开, 然后再取得链接, 发送给beautifulsoup分析. (当然也可以直接用正则表达式分析) link_list = [] # 切割完整域名 cut_current_list = re.search(\"/video/av[0-9]+/\", page_url).group(0) # bs4识别所有链接 bsobj = BeautifulSoup(list_source, \"html.parser\") a_list = bsobj.findAll(\"a\", {\"href\": re.compile(\"/video/[a-z0-9/_.]+\")}) # 保存包括本页面的所有链接 link_list.append(cut_current_list) for a in a_list: link_list.append(str(a[\"href\"])) return link_list 将链接列表保存至 link_list , 注意匹配到的是除了本页面以外的链接, 且格式是子域名(/video/av…./), 为了保持格式一致性, 本页面的链接也应该先切割后再保存进列表. ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:2:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"2. 新番站点结构 新番站点结构相对简单, 直接使用xpath搜索到链接表(v_bgm_list_data)并提取源代码, 用beautifulsoup分析后保存即可, 其他步骤和前面类似 ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:3:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"3. 提取页面cid, 标题等信息 打开每个链接, 直接提取源代码, 使用正则表达式匹配cid和title, 注意标题需要使用utf-8编码, 否则在使用它作为文件名保存的时候会报错 driver.get(page_url) source_code = driver.page_source # 源代码中可以找到cid和title, 对应弹幕文件和标题 title_reg = re.compile(\"(?\u003c=\u003ctitle\u003e).*(?=\u003c/title\u003e)\") cid_reg = re.compile(\"(?\u003c=cid=)[0-9]+\") p.title = re.search(title_reg, source_code).group(0).encode('utf-8') p.cid = str(re.search(cid_reg, source_code).group(0)) ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:4:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"4. 利用cid下载弹幕并保存 获得了cid之后, 拼接出完整地址, 直接保存页面源代码为xml文件即可 # 拼接网址 comment_url = \"http://comment.bilibili.tv/\" + p.cid + \".xml\" # 保存源码, 设置30秒超时 driver.set_page_load_timeout(30) driver.get(comment_url) source = driver.page_source # 保存时注意使用utf-8编码 with open(\"./data/\" + p.title + \".xml\", 'w') as fd: fd.write(source.encode('utf-8')) ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:5:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"5. 提高容错率 主程序使用对link_list的循环依次打开视频并保存. 取得页面信息和保存弹幕的函数都可以根据自身成功与否返回True或者False, 主程序根据取回情况决定是否重新执行, 执行3次依然失败的话即跳过该链接. 例子如下: for link in link_list: page_url = \"http://www.bilibili.com\" + str(link) # 爬取页面失败重试3次, 再失败则跳过 cid_flag = get_page_info(driver, page_url, p) if not cid_flag: for i in range(3): cid_flag = get_page_info(driver, page_url, p) if cid_flag: break print(\"Get cid failed, retrying...\") if not cid_flag: print(\"Retry times out, skip\") continue ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:6:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"6. 小结 通过分析, 我们获得了提取b站弹幕的基本策略, 并使用了简单的容错机制. 还可以完善的地方有: 是否可以下载视频? 如何处理副标题? 如何处理视频链接? 在保存的弹幕中是否需要体现链接? 是否可以并行下载以提升速度? 感谢阅读! Written with StackEdit. ","date":"2016-07-17","objectID":"/code/2016-07-17-bilibili-scraper/:7:0","tags":["python","爬虫"],"title":"bilibili新番/旧番弹幕抓取","uri":"/code/2016-07-17-bilibili-scraper/"},{"categories":null,"content":"源代码: https://github.com/shawnau/ustcsse_scraper ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:0:0","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"0. 准备工作 首先介绍下需要安装的组件： selenium, 自动化测试工具, 本文会通过它操纵phantomJS, 使得爬虫能做出模仿普通用户的操作, 基于python的selenium指南请参考官方文档, 本文将会较重度地使用selenium. phantomJS, 简单说这就是个可以静默运行的浏览器模拟器, 具体安装事项请看官方文档, 这篇文章并不会过多地使用它 beautifulsoup, 做过静态网页爬虫的朋友应该都很熟悉了. 同样, 这篇文章并不会过多地使用它, 因为需要爬的是动态网页, 只把它作为简单的html分析器使用 Firefox, 主要是使用FirePath和HttpFox插件来分析网页, 关于xPath, 详见xPath教程, 本文并不要求掌握. 除此之外, 本文默认读者具备最基础的python(2.7)知识和正则表达式知识.有关python和正则表达式的知识详见: The Python Tutorial, 正则表达式30分钟入门教程, Python Regular Expressions, re module的官方文档 ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:1:0","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"1. 模拟登录并保存cookies ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:2:0","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"1.1 使用selenium打开网页 首先看一下我们需要爬取的网站: http://mis.sse.ustc.edu.cn/ (恩..教务网..a little private for my 1st web scraper lol) 观察源代码 万幸的是没有验证码. 不幸的是登录界面是javascript渲染出来的, 连input标签都找不到, 传统的直接通过requests包进行post操作登录将会遇到困难, 因此我们使用selenium来操作phantomJS登录. 首先通过selenium启动phantomJS: from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities # set phantomJS's agent to Firefox dcap = dict(DesiredCapabilities.PHANTOMJS) dcap[\"phantomjs.page.settings.userAgent\"] = \\ \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0 \" # system path you need to config by yourself phantomjsPath = \"/Users/Shawn/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs\" driver = webdriver.PhantomJS(executable_path=phantomjsPath, desired_capabilities=dcap) 第一部分对dcap的配置是为了改变user-agent, “伪装\"成网页需要的浏览器访问. 这里通过desired_capabilities告诉selenium, 模拟的是OS X10.9下的Firefox 25.0, 第二部分是指出了PhantomJS的可执行文件位置, 根据系统的不同需要自行找到phantomJS的文件地址, 并通过executable_path告诉selenium. 最后一行执行完毕之后driver就启动了, 这时driver相当于启动了一个浏览器窗口. 接着我们用这个浏览器登录, 这里使用了get()方法 login_url = \"http://mis.sse.ustc.edu.cn/\" driver.get(login_url) 此时相当于往浏览器输入地址, 打开登录界面. ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:2:1","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"1.2 使用xpath找到需要的元素 接下来我们需要让selenium找到填写账户和密码的文本框, 填写完毕之后再点击登录. 所有这些元素都可以通过强大的xPath找到. 首先使用火狐的firePath找到元素对应的xPath. 右键登录的文本框, 点击inspect in firepath: 复制找到的xPath, 同理找到其他两个元素, 保存下来: idXpath = \".//*[@id='winLogin_sfLogin_txtUserLoginID']\" pwXpath = \".//*[@id='winLogin_sfLogin_txtPassword']\" loginXpath = \".//*[@id='ext-gen5']\" 下面就是填写表单登录了. 但是由于js渲染需要时间, 若脚本一打开就填写有可能还没渲染出来, selenium找不到, 因此需要使用selenium为我们准备的WebDriverWait方法 wait = WebDriverWait(driver, 10) wait.until(EC.presence_of_element_located((By.XPATH, idXpath))) wait.until(EC.presence_of_element_located((By.XPATH, pwXpath))) wait.until(EC.element_to_be_clickable((By.XPATH, loginXpath))) 以上三条的意思是让我们的driver一直等到找到了填写账号和密码的文本框, 并且登录按钮可以点击的时候. 如果等10秒还找不到的话, 就raise exception, 报异常了. 接下来就可以填写账户密码, 并点击登录按钮了 driver.find_element_by_xpath(idXpath).send_keys(username) driver.find_element_by_xpath(pwXpath).send_keys(password) driver.find_element_by_xpath(loginXpath).click() print(\"Login Success!\") 首先通过find_element_by_xpath方法找到对应的元素, 再通过send_keys()和 click()方法填写表单和点击按钮. ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:2:2","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"1.3 登陆完毕后保存cookies备用 登录完毕之后, 可以看见网页已被重定向到了http://mis.sse.ustc.edu.cn/homepage/StuDefault.aspx, 那么怎样确认网页已经被重定向了呢? 我们故技重施, 在这个页面随意找一个之前登录页面没有的元素, 让selenium等到他出现就是了 redirectXpath = \".//*[@id='RegionPanel1_UpRegion_ContentPanel1_content']/table/tbody/tr/td[2]\" wait.until(EC.presence_of_element_located((By.XPATH, redirectXpath))) 完成这些之后, 当爬虫下次登录的时候可以用这次的cookies, 做到免输入直接登录. 这就要求我们把本次登录的cookies保存下来. 虽然selenium对取得cookies有极为方便的方法get_cookies(), 但遗憾的是phantomJS对cookies的格式支持不是那么完善, 直接取用会报错. 因此我们需要定义函数手动整理好cookies的格式并将其保存在一个js文件里, 当下次driver启动的时候可以通过execute_script() 方法直接调用: def save_cookies(driver, file_path): # The format could vary LINE = \"document.cookie = '{name}={value}; path={path}; domain={domain}';\\n\" with open(file_path, 'w') as fd: for cookie in driver.get_cookies(): fd.write(LINE.format(**cookie)) def load_cookies(driver, file_path): with open(file_path, 'r') as fd: driver.execute_script(fd.read()) 通过以上方法, 辅以完善的报错机制, 我们可以把以上登录步骤封装成一个函数, 其完成登录之后就把cookies保存到文件之中, 以便其后的爬虫直接通过cookies登录. def login(login_url, username, password): driver = webdriver.PhantomJS(executable_path=phantomjsPath, desired_capabilities=dcap) try: driver.get(login_url) wait = WebDriverWait(driver, 10) wait.until(EC.presence_of_element_located((By.XPATH, idXpath))) wait.until(EC.presence_of_element_located((By.XPATH, pwXpath))) wait.until(EC.element_to_be_clickable((By.XPATH, loginXpath))) driver.find_element_by_xpath(idXpath).send_keys(username) driver.find_element_by_xpath(pwXpath).send_keys(password) driver.find_element_by_xpath(loginXpath).click() print(\"Login Success!\") wait.until(EC.presence_of_element_located((By.XPATH, redirectXpath))) print(\"Current url is: \" + driver.current_url) print(\"Current cookies are: \" + str(driver.get_cookies())) save_cookies(driver, r'cookies.js') except Exception as ex: print(\"Login failed!\") print(ex) finally: driver.close() driver.close() 方法相当于关闭了浏览器, 每次打开浏览器时都要记得最后关闭它. 本文之后还会定义一个 logout() 函数用于退出, 其机制和 login() 类似, 并使用了预存的cookies, 详见源代码. Selenium除了可以操作phantomJS之外, 还可以操作其他浏览器, 比如Firefox, Chrome等等, 在debug的时候这些浏览器会自动打开并进行模拟, 因此可以获得更清晰的模拟过程. 在debug我们的phantomJS遇到问题时, 也可以用其他浏览器代替, 来观察是哪个细节出错了. 关于其他浏览器请看官方文档的WebDriver API一节, 其安装是很容易的, 使用也只需要一行来代替我们之前使用phantomJS的方法, 例如: driver.Firefox() ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:2:3","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"2. 爬取动态网页内容 ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:3:0","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"2.1 使用cookies自动登录 有了cookies, 我们的爬虫就可以畅游教务网了. 使用cookies登录的方法很简单 scrape_url = \"http://mis.sse.ustc.edu.cn/homepage/StuDefault.aspx\" driver.get(login_url) driver.delete_all_cookies() load_cookies(driver, r'cookies.js') driver.get(scrape_url) 首先登录主页, 然后使用 delete_all_cookies() 清空自己的cookies, 再载入预先保存的cookies, 然后直接访问要去的地址, 比如 scrape_url 这个子域名. 之所以先登录再访问而不是直接访问是因为cookies的域名必须和driver原先的域名一致, 不然会报错, 因此先登录主页初始化一下域名. 封装一下: def scraper(login_url, scrape_url): driver = webdriver.PhantomJS(executable_path=phantomjsPath, desired_capabilities=dcap) try: driver.get(login_url) driver.delete_all_cookies() load_cookies(driver, r'cookies.js') driver.get(scrape_url) if driver.current_url == login_url: raise NameError('Redirect failed!') print(\"Scraper login through cookie success!\") print(\"scrapping...\") # Do anything here! Scrape! except Exception as ex: print(\"Scraper login through cookie failed!\") print(ex) finally: logout(driver, login_url) 有了这个函数, 我们就可以使用它的driver任意爬取数据了. ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:3:1","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"2.2 爬取通知 找一找主页有什么可以爬取的? 从通知下手: 点击最新通知右下角的那个more, 转移到一个包含通知的新网页. 新网页似乎是嵌入在某个窗口中的. 我们用httpfox检查一下, 是不是能直接打开这个地址. 回到主页, 打开httpfox, 点击more之前按start检测, 点击完后再按stop停止: 果然, 在第一条下就发现了它的地址: http://mis.sse.ustc.edu.cn/Base/NoticeInfo/ListView.aspx 打开这个地址, 正是我们要找的窗口. 接下来就可以使用 driver.get() 打开此地址并爬取通知了. 这里的做法是先用xPath选中整个列表, 把列表信息以html的形式保存下来, 然后用正则表达式匹配其中的链接, 获得所有链接之后一页一页地爬取 import re infoXpath = \".//*[@id='ext-gen51']\" elem = driver.find_element_by_xpath(infoXpath) table_source = elem.get_attribute('innerHTML') reg = re.compile(\"/Base/NoticeInfo/ViewNotice\\.aspx\\?ID=[-a-z0-9]+\") link_list = re.findall(reg, table_source) selenium对每个找到的元素, 都可以使用 get_attribute('innerHTML') 方法来获得其完整的html代码. 接着观察一下链接的形式, 发现很容易通过正则表达式提取. 提取完毕后在debug模式下我们可以得到 link_list 的内容: /Base/NoticeInfo/ViewNotice.aspx?ID=65494f34-8f5c-4183-aa09-153c55a2e842 /Base/NoticeInfo/ViewNotice.aspx?ID=de4b8355-0466-4c0c-8cec-6b152471f0da /Base/NoticeInfo/ViewNotice.aspx?ID=223acddd-9f5a-48e5-8e3d-151d0737750b 找到链接之后就可以循环提取链接, 把它和域名拼接起来, 使用beautifulsoup保存 # 设置保存地址 savePath = \"/Users/Shawn/Documents/webscraper/data/\" for link in link_list: page_link = login_url + link # 取得链接内容的源代码 driver.get(page_link) pagesource = driver.page_source # 用beautifulsoup解析页面内容并提取标题 from bs4 import BeautifulSoup bsObj = BeautifulSoup(pagesource, \"html.parser\") title = bsObj.find(\"span\", {\"id\": \"lblNoticeName\"}).get_text().encode('utf-8') # 保存到本地, 文件名就是标题名 with open(savePath + str(title)+\".html\", 'w') as fd: fd.write(bsObj.get_text().encode('utf-8') + '\\n') 最后封装成 rule() 函数嵌入到 scraper() 的 # Do anything here! Scrape! 注释那一行中, 详见源代码. 再看一下保存的文件: 格式似乎还没有整理完毕, 文字和css分离了 ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:3:2","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"3. 小结 通过登录, 保存cookies, 爬取网页的过程, 本文介绍了怎样利用selenium+phantomJS操作动态网页. 爬虫的不足之处以及还可以进一步探索的内容有: 批量保存文件时将渲染之后的网页源代码保存 代码重用性: 在不改变框架的情况下, 如何利用该脚本爬取其他网页 如果有验证码, 如何手动登录之后获取cookies再传递给爬虫自动登录, 乃至自动识别验证码 如何提高速度: 使用多线程并发的技术同时爬取不同页面 以上内容将在未来进一步完善, 感谢阅读! Written with StackEdit. ","date":"2016-07-16","objectID":"/code/2016-07-16-webscraper/:4:0","tags":["python","爬虫"],"title":"基于selenium+phantomJS的动态网页抓取","uri":"/code/2016-07-16-webscraper/"},{"categories":null,"content":"Spark简介 This article is copied from the BerkeleyX: CS105x Introduction to Apache Spark course materials. Source: https://raw.githubusercontent.com/spark-mooc/mooc-setup/master/cs105_lab1a_spark_tutorial.dbc This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:0:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Spark Tutorial: Learning Apache Spark This tutorial will teach you how to use Apache Spark, a framework for large-scale data processing, within a notebook. Many traditional frameworks were designed to be run on a single computer. However, many datasets today are too large to be stored on a single computer, and even when a dataset can be stored on one computer (such as the datasets in this tutorial), the dataset can often be processed much more quickly using multiple computers. Spark has efficient implementations of a number of transformations and actions that can be composed together to perform data processing and analysis. Spark excels at distributing these operations across a cluster while abstracting away many of the underlying implementation details. Spark has been designed with a focus on scalability and efficiency. With Spark you can begin developing your solution on your laptop, using a small dataset, and then use that same code to process terabytes or even petabytes across a distributed cluster. During this tutorial we will cover: Part 1: Basic notebook usage and Python integration Part 2: An introduction to using Apache Spark with the PySpark SQL API running in a notebook Part 3: Using DataFrames and chaining together transformations and actions Part 4: Python Lambda functions and User Defined Functions Part 5: Additional DataFrame actions Part 6: Additional DataFrame transformations Part 7: Caching DataFrames and storage options Part 8: Debugging Spark applications and lazy evaluation The following transformations will be covered: select(), filter(), distinct(), dropDuplicates(), orderBy(), groupBy() The following actions will be covered: first(), take(), count(), collect(), show() Also covered: cache(), unpersist() Note that, for reference, you can look up the details of these methods in the Spark’s PySpark SQL API ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:1:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"**Part 1: Basic notebook usage and Python integration ** ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:2:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(1a) Notebook usage A notebook is comprised of a linear sequence of cells. These cells can contain either markdown or code, but we won’t mix both in one cell. When a markdown cell is executed it renders formatted text, images, and links just like HTML in a normal webpage. The text you are reading right now is part of a markdown cell. Python code cells allow you to execute arbitrary Python commands just like in any Python shell. Place your cursor inside the cell below, and press “Shift” + “Enter” to execute the code and advance to the next cell. You can also press “Ctrl” + “Enter” to execute the code and remain in the cell. These commands work the same in both markdown and code cells. # This is a Python cell. You can run normal Python code here... print 'The sum of 1 and 1 is {0}'.format(1+1) # Here is another Python cell, this time with a variable (x) declaration and an if statement: x = 42 if x \u003e 40: print 'The sum of 1 and 2 is {0}'.format(1+2) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:2:1","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(1b) Notebook state As you work through a notebook it is important that you run all of the code cells. The notebook is stateful, which means that variables and their values are retained until the notebook is detached (in Databricks) or the kernel is restarted (in Jupyter notebooks). If you do not run all of the code cells as you proceed through the notebook, your variables will not be properly initialized and later code might fail. You will also need to rerun any cells that you have modified in order for the changes to be available to other cells. # This cell relies on x being defined already. # If we didn't run the cells from part (1a) this code would fail. print x * 2 ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:2:2","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(1c) Library imports We can import standard Python libraries (modules) the usual way. An import statement will import the specified module. In this tutorial and future labs, we will provide any imports that are necessary. # Import the regular expression library import re m = re.search('(?\u003c=abc)def', 'abcdef') m.group(0) # Import the datetime library import datetime print 'This was last run on: {0}'.format(datetime.datetime.now()) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:2:3","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Part 2: An introduction to using Apache Spark with the PySpark SQL API running in a notebook ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:3:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Spark Context In Spark, communication occurs between a driver and executors. The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion. The results from these tasks are delivered back to the driver. In part 1, we saw that normal Python code can be executed via cells. When using Databricks this code gets executed in the Spark driver’s Java Virtual Machine (JVM) and not in an executor’s JVM, and when using an Jupyter notebook it is executed within the kernel associated with the notebook. Since no Spark functionality is actually being used, no tasks are launched on the executors. In order to use Spark and its DataFrame API we will need to use a SQLContext. When running Spark, you start a new Spark application by creating a SparkContext. You can then create a SQLContext from the SparkContext. When the SparkContext is created, it asks the master for some cores to use to do work. The master sets these cores aside just for you; they won’t be used for other applications. When using Databricks, both a SparkContext and a SQLContext are created for you automatically. sc is your SparkContext, and sqlContext is your SQLContext. ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:3:1","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(2a) Example Cluster The diagram shows an example cluster, where the slots allocated for an application are outlined in purple. (Note: We’re using the term slots here to indicate threads available to perform parallel work for Spark. Spark documentation often refers to these threads as cores, which is a confusing term, as the number of slots available on a particular machine does not necessarily have any relationship to the number of physical CPU cores on that machine.) You can view the details of your Spark application in the Spark web UI. The web UI is accessible in Databricks by going to “Clusters” and then clicking on the “Spark UI” link for your cluster. In the web UI, under the “Jobs” tab, you can see a list of jobs that have been scheduled or run. It’s likely there isn’t any thing interesting here yet because we haven’t run any jobs, but we’ll return to this page later. At a high level, every Spark application consists of a driver program that launches various parallel operations on executor Java Virtual Machines (JVMs) running either in a cluster or locally on the same machine. In Databricks, “Databricks Shell” is the driver program. When running locally, pyspark is the driver program. In all cases, this driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations (transformations \u0026 actions) to those datasets. Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster. A Spark SQL context object (sqlContext) is the main entry point for Spark DataFrame and SQL functionality. A SQLContext can be used to create DataFrames, which allows you to direct the operations on your data. Try printing out sqlContext to see its type. # Display the type of the Spark sqlContext type(sqlContext) Note that the type is HiveContext. This means we’re working with a version of Spark that has Hive support. Compiling Spark with Hive support is a good idea, even if you don’t have a Hive metastore. As the Spark Programming Guide states, a HiveContext “provides a superset of the functionality provided by the basic SQLContext. Additional features include the ability to write queries using the more complete HiveQL parser, access to Hive UDFs [user-defined functions], and the ability to read data from Hive tables. To use a HiveContext, you do not need to have an existing Hive setup, and all of the data sources available to a SQLContext are still available.” ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:3:2","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(2b) SparkContext attributes You can use Python’s dir() function to get a list of all the attributes (including methods) accessible through the sqlContext object. # List sqlContext's attributes dir(sqlContext) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:3:3","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(2c) Getting help Alternatively, you can use Python’s help() function to get an easier to read list of all the attributes, including examples, that the sqlContext object has. # Use help to obtain more detailed information help(sqlContext) Outside of pyspark or a notebook, SQLContext is created from the lower-level SparkContext, which is usually used to create Resilient Distributed Datasets (RDDs). An RDD is the way Spark actually represents data internally; DataFrames are actually implemented in terms of RDDs. While you can interact directly with RDDs, DataFrames are preferred. They’re generally faster, and they perform the same no matter what language (Python, R, Scala or Java) you use with Spark. In this course, we’ll be using DataFrames, so we won’t be interacting directly with the Spark Context object very much. However, it’s worth knowing that inside pyspark or a notebook, you already have an existing SparkContext in the sc variable. One simple thing we can do with sc is check the version of Spark we’re using: # After reading the help we've decided we want to use sc.version to see what version of Spark we are running sc.version # Help can be used on any Python object help(map) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:3:4","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Part 3: Using DataFrames and chaining together transformations and actions ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Working with your first DataFrames In Spark, we first create a base DataFrame. We can then apply one or more transformations to that base DataFrame. A DataFrame is immutable, so once it is created, it cannot be changed. As a result, each transformation creates a new DataFrame. Finally, we can apply one or more actions to the DataFrames. Note that Spark uses lazy evaluation, so transformations are not actually executed until an action occurs. We will perform several exercises to obtain a better understanding of DataFrames: Create a Python collection of 10,000 integers Create a Spark DataFrame from that collection Subtract one from each value using map Perform action collect to view results Perform action count to view counts Apply transformation filter and view results with collect Learn about lambda functions Explore how lazy evaluation works and the debugging challenges that it introduces A DataFrame consists of a series of Row objects; each Row object has a set of named columns. You can think of a DataFrame as modeling a table, though the data source being processed does not have to be a table. More formally, a DataFrame must have a schema, which means it must consist of columns, each of which has a name and a type. Some data sources have schemas built into them. Examples include RDBMS databases, Parquet files, and NoSQL databases like Cassandra. Other data sources don’t have computer-readable schemas, but you can often apply a schema programmatically. ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:1","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(3a) Create a Python collection of 10,000 people We will use a third-party Python testing library called fake-factory to create a collection of fake person records. from faker import Factory fake = Factory.create() fake.seed(4321) We’re going to use this factory to create a collection of randomly generated people records. In the next section, we’ll turn that collection into a DataFrame. We’ll use the Spark Row class, because that will help us define the Spark DataFrame schema. There are other ways to define schemas, though; see the Spark Programming Guide’s discussion of schema inference for more information. (For instance, we could also use a Python namedtuple.) # Each entry consists of last_name, first_name, ssn, job, and age (at least 1) from pyspark.sql import Row def fake_entry(): name = fake.name().split() return (name[1], name[0], fake.ssn(), fake.job(), abs(2016 - fake.date_time().year) + 1) # Create a helper function to call a function repeatedly def repeat(times, func, *args, **kwargs): for _ in xrange(times): yield func(*args, **kwargs) data = list(repeat(10000, fake_entry)) data is just a normal Python list, containing Python tuples objects. Let’s look at the first item in the list: data[0] We can check the size of the list using the Python len() function. len(data) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:2","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(3b) Distributed data and using a collection to create a DataFrame In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine. Each partition holds a unique subset of the entries in the list. Spark calls datasets that it stores “Resilient Distributed Datasets” (RDDs). Even DataFrames are ultimately represented as RDDs, with additional meta-data. One of the defining features of Spark, compared to other data analytics frameworks (e.g., Hadoop), is that it stores data in memory rather than on disk. This allows Spark applications to run much more quickly, because they are not slowed down by needing to read data from disk. The figure to the right illustrates how Spark breaks a list of data entries into partitions that are each stored in memory on a worker. To create the DataFrame, we’ll use sqlContext.createDataFrame(), and we’ll pass our array of data in as an argument to that function. Spark will create a new set of input data based on data that is passed in. A DataFrame requires a schema, which is a list of columns, where each column has a name and a type. Our list of data has elements with types (mostly strings, but one integer). We’ll supply the rest of the schema and the column names as the second argument to createDataFrame(). Let’s view the help for createDataFrame(). help(sqlContext.createDataFrame) dataDF = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age')) Let’s see what type sqlContext.createDataFrame() returned. print 'type of dataDF: {0}'.format(type(dataDF)) Let’s take a look at the DataFrame’s schema and some of its rows. dataDF.printSchema() We can register the newly created DataFrame as a named table, using the registerDataFrameAsTable() method. sqlContext.registerDataFrameAsTable(dataDF, 'dataframe') What methods can we call on this DataFrame? help(dataDF) How many partitions will the DataFrame be split into? dataDF.rdd.getNumPartitions() A note about DataFrames and queries When you use DataFrames or Spark SQL, you are building up a query plan. Each transformation you apply to a DataFrame adds some information to the query plan. When you finally call an action, which triggers execution of your Spark job, several things happen: Spark’s Catalyst optimizer analyzes the query plan (called an unoptimized logical query plan) and attempts to optimize it. Optimizations include (but aren’t limited to) rearranging and combining filter() operations for efficiency, converting Decimal operations to more efficient long integer operations, and pushing some operations down into the data source (e.g., a filter() operation might be translated to a SQL WHERE clause, if the data source is a traditional SQL RDBMS). The result of this optimization phase is an optimized logical plan. Once Catalyst has an optimized logical plan, it then constructs multiple physical plans from it. Specifically, it implements the query in terms of lower level Spark RDD operations. Catalyst chooses which physical plan to use via cost optimization. That is, it determines which physical plan is the most efficient (or least expensive), and uses that one. Finally, once the physical RDD execution plan is established, Spark actually executes the job. You can examine the query plan using the explain() function on a DataFrame. By default, explain() only shows you the final physical plan; however, if you pass it an argument of True, it will show you all phases. (If you want to take a deeper dive into how Catalyst optimizes DataFrame queries, this blog post, while a little old, is an excellent overview: Deep Dive into Spark SQL’s Catalyst Optimizer.) Let’s add a couple transformations to our DataFrame and look at the query plan on the resulting transformed DataFrame. Don’t be too concerned if it looks like gibberish. As you gain more experience with Apache Spark, you’ll begin to be able to use explain() to help you understand mo","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:3","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(3c): Subtract one from each value using select So far, we’ve created a distributed DataFrame that is split into many partitions, where each partition is stored on a single machine in our cluster. Let’s look at what happens when we do a basic operation on the dataset. Many useful data analysis operations can be specified as “do something to each item in the dataset”. These data-parallel operations are convenient because each item in the dataset can be processed individually: the operation on one entry doesn’t effect the operations on any of the other entries. Therefore, Spark can parallelize the operation. One of the most common DataFrame operations is select(), and it works more or less like a SQL SELECT statement: You can select specific columns from the DataFrame, and you can even use select() to create new columns with values that are derived from existing column values. We can use select() to create a new column that decrements the value of the existing age column. select() is a transformation. It returns a new DataFrame that captures both the previous DataFrame and the operation to add to the query (select, in this case). But it does not actually execute anything on the cluster. When transforming DataFrames, we are building up a query plan. That query plan will be optimized, implemented (in terms of RDDs), and executed by Spark only when we call an action. # Transform dataDF through a select transformation and rename the newly created '(age -1)' column to 'age' # Because select is a transformation and Spark uses lazy evaluation, no jobs, stages, # or tasks will be launched when we run this code. subDF = dataDF.select('last_name', 'first_name', 'ssn', 'occupation', (dataDF.age - 1).alias('age')) Let’s take a look at the query plan. subDF.explain(True) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:4","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(3d) Use collect to view results To see a list of elements decremented by one, we need to create a new list on the driver from the the data distributed in the executor nodes. To do this we can call the collect() method on our DataFrame. collect() is often used after transformations to ensure that we are only returning a small amount of data to the driver. This is done because the data returned to the driver must fit into the driver’s available memory. If not, the driver will crash. The collect() method is the first action operation that we have encountered. Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the values returned by the action. In our example, this means that tasks will now be launched to perform the createDataFrame, select, and collect operations. In the diagram, the dataset is broken into four partitions, so four collect() tasks are launched. Each task collects the entries in its partition and sends the result to the driver, which creates a list of the values, as shown in the figure below. Now let’s run collect() on subDF. # Let's collect the data results = subDF.collect() print results A better way to visualize the data is to use the show() method. If you don’t tell show() how many rows to display, it displays 20 rows. subDF.show() If you’d prefer that show() not truncate the data, you can tell it not to: subDF.show(n=30, truncate=False) In Databricks, there’s an even nicer way to look at the values in a DataFrame: The display() helper function. display(subDF) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:5","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(3e) Use count to get total One of the most basic jobs that we can run is the count() job which will count the number of elements in a DataFrame, using the count() action. Since select() creates a new DataFrame with the same number of elements as the starting DataFrame, we expect that applying count() to each DataFrame will return the same result. Note that because count() is an action operation, if we had not already performed an action with collect(), then Spark would now perform the transformation operations when we executed count(). Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure on the right shows what would happen if we ran count() on a small example dataset with just four partitions. print dataDF.count() print subDF.count() ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:6","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(3f) Apply transformation filter and view results with collect Next, we’ll create a new DataFrame that only contains the people whose ages are less than 10. To do this, we’ll use the filter() transformation. (You can also use where(), an alias for filter(), if you prefer something more SQL-like). The filter() method is a transformation operation that creates a new DataFrame from the input DataFrame, keeping only values that match the filter expression. The figure shows how this might work on the small four-partition dataset. To view the filtered list of elements less than 10, we need to create a new list on the driver from the distributed data on the executor nodes. We use the collect() method to return a list that contains all of the elements in this filtered DataFrame to the driver program. filteredDF = subDF.where(subDF.age \u003c 10) filteredDF.show() filteredDF.count() (These are some seriously precocious children…) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:4:7","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Part 4: Python Lambda functions and User Defined Functions Python supports the use of small one-line anonymous functions that are not bound to a name at runtime. lambda functions, borrowed from LISP, can be used wherever function objects are required. They are syntactically restricted to a single expression. Remember that lambda functions are a matter of style and using them is never required - semantically, they are just syntactic sugar for a normal function definition. You can always define a separate normal function instead, but using a lambda function is an equivalent and more compact form of coding. Ideally you should consider using lambda functions where you want to encapsulate non-reusable code without littering your code with one-line functions. Here, instead of defining a separate function for the filter() transformation, we will use an inline lambda() function and we will register that lambda as a Spark User Defined Function (UDF). A UDF is a special wrapper around a function, allowing the function to be used in a DataFrame query. from pyspark.sql.types import BooleanType less_ten = udf(lambda s: s \u003c 10, BooleanType()) lambdaDF = subDF.filter(less_ten(subDF.age)) lambdaDF.show() lambdaDF.count() # Let's collect the even values less than 10 even = udf(lambda s: s % 2 == 0, BooleanType()) evenDF = lambdaDF.filter(even(lambdaDF.age)) evenDF.show() evenDF.count() ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:5:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Part 5: Additional DataFrame actions Let’s investigate some additional actions: first() take() One useful thing to do when we have a new dataset is to look at the first few entries to obtain a rough idea of what information is available. In Spark, we can do that using actions like first(), take(), and show(). Note that for the first() and take() actions, the elements that are returned depend on how the DataFrame is partitioned. Instead of using the collect() action, we can use the take(n) action to return the first n elements of the DataFrame. The first() action returns the first element of a DataFrame, and is equivalent to take(1)[0]. print \"first: {0}\\n\".format(filteredDF.first()) print \"Four of them: {0}\\n\".format(filteredDF.take(4)) This looks better: display(filteredDF.take(4)) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:6:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Part 6: Additional DataFrame transformations ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:7:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(6a) orderBy orderBy() allows you to sort a DataFrame by one or more columns, producing a new DataFrame. For example, let’s get the first five oldest people in the original (unfiltered) DataFrame. We can use the orderBy() transformation. orderBy takes one or more columns, either as names (strings) or as Column objects. To get a Column object, we use one of two notations on the DataFrame: Pandas-style notation: filteredDF.age Subscript notation: filteredDF['age'] Both of those syntaxes return a Column, which has additional methods like desc() (for sorting in descending order) or asc() (for sorting in ascending order, which is the default). Here are some examples: dataDF.orderBy(dataDF['age']) # sort by age in ascending order; returns a new DataFrame dataDF.orderBy(dataDF.last_name.desc()) # sort by last name in descending order # Get the five oldest people in the list. To do that, sort by age in descending order. display(dataDF.orderBy(dataDF.age.desc()).take(5)) Let’s reverse the sort order. Since ascending sort is the default, we can actually use a Column object expression or a simple string, in this case. The desc() and asc() methods are only defined on Column. Something like orderBy('age'.desc()) would not work, because there’s no desc() method on Python string objects. That’s why we needed the column expression. But if we’re just using the defaults, we can pass a string column name into orderBy(). This is sometimes easier to read. display(dataDF.orderBy('age').take(5)) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:7:1","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(6b) distinct and dropDuplicates distinct() filters out duplicate rows, and it considers all columns. Since our data is completely randomly generated (by fake-factory), it’s extremely unlikely that there are any duplicate rows: print dataDF.count() print dataDF.distinct().count() To demonstrate distinct(), let’s create a quick throwaway dataset. tempDF = sqlContext.createDataFrame([(\"Joe\", 1), (\"Joe\", 1), (\"Anna\", 15), (\"Anna\", 12), (\"Ravi\", 5)], ('name', 'score')) tempDF.show() tempDF.distinct().show() Note that one of the (“Joe”, 1) rows was deleted, but both rows with name “Anna” were kept, because all columns in a row must match another row for it to be considered a duplicate. dropDuplicates() is like distinct(), except that it allows us to specify the columns to compare. For instance, we can use it to drop all rows where the first name and last name duplicates (ignoring the occupation and age columns). print dataDF.count() print dataDF.dropDuplicates(['first_name', 'last_name']).count() ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:7:2","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(6c) drop drop() is like the opposite of select(): Instead of selecting specific columns from a DataFrame, it drops a specifed column from a DataFrame. Here’s a simple use case: Suppose you’re reading from a 1,000-column CSV file, and you have to get rid of five of the columns. Instead of selecting 995 of the columns, it’s easier just to drop the five you don’t want. dataDF.drop('occupation').drop('age').show() ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:7:3","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(6d) groupBy [groupBy()]((http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) is one of the most powerful transformations. It allows you to perform aggregations on a DataFrame. Unlike other DataFrame transformations, groupBy() does not return a DataFrame. Instead, it returns a special GroupedData object that contains various aggregation functions. The most commonly used aggregation function is count(), but there are others (like sum(), max(), and avg(). These aggregation functions typically create a new column and return a new DataFrame. dataDF.groupBy('occupation').count().show(truncate=False) We can also use groupBy() to do aother useful aggregations: print \"Maximum age: {0}\".format(dataDF.groupBy().max('age').first()[0]) print \"Minimum age: {0}\".format(dataDF.groupBy().min('age').first()[0]) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:7:4","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(6e) sample (optional) When analyzing data, the sample() transformation is often quite useful. It returns a new DataFrame with a random sample of elements from the dataset. It takes in a withReplacement argument, which specifies whether it is okay to randomly pick the same item multiple times from the parent DataFrame (so when withReplacement=True, you can get the same item back multiple times). It takes in a fraction parameter, which specifies the fraction elements in the dataset you want to return. (So a fraction value of 0.20 returns 20% of the elements in the DataFrame.) It also takes an optional seed parameter that allows you to specify a seed value for the random number generator, so that reproducible results can be obtained. sampledDF = dataDF.sample(withReplacement=False, fraction=0.10) print sampledDF.count() sampledDF.show() print dataDF.sample(withReplacement=False, fraction=0.05).count() ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:7:5","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"Part 7: Caching DataFrames and storage options ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:8:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(7a) Caching DataFrames For efficiency Spark keeps your DataFrames in memory. (More formally, it keeps the RDDs that implement your DataFrames in memory.) By keeping the contents in memory, Spark can quickly access the data. However, memory is limited, so if you try to keep too many partitions in memory, Spark will automatically delete partitions from memory to make space for new ones. If you later refer to one of the deleted partitions, Spark will automatically recreate it for you, but that takes time. So, if you plan to use a DataFrame more than once, then you should tell Spark to cache it. You can use the cache() operation to keep the DataFrame in memory. However, you must still trigger an action on the DataFrame, such as collect() or count() before the caching will occur. In other words, cache() is lazy: It merely tells Spark that the DataFrame should be cached when the data is materialized. You have to run an action to materialize the data; the DataFrame will be cached as a side effect. The next time you use the DataFrame, Spark will use the cached data, rather than recomputing the DataFrame from the original data. You can see your cached DataFrame in the “Storage” section of the Spark web UI. If you click on the name value, you can see more information about where the the DataFrame is stored. # Cache the DataFrame filteredDF.cache() # Trigger an action print filteredDF.count() # Check if it is cached print filteredDF.is_cached ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:8:1","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(7b) Unpersist and storage options Spark automatically manages the partitions cached in memory. If it has more partitions than available memory, by default, it will evict older partitions to make room for new ones. For efficiency, once you are finished using cached DataFrame, you can optionally tell Spark to stop caching it in memory by using the DataFrame’s unpersist() method to inform Spark that you no longer need the cached data. ** Advanced: ** Spark provides many more options for managing how DataFrames cached. For instance, you can tell Spark to spill cached partitions to disk when it runs out of memory, instead of simply throwing old ones away. You can explore the API for DataFrame’s persist() operation using Python’s help() command. The persist() operation, optionally, takes a pySpark StorageLevel object. # If we are done with the DataFrame we can unpersist it so that its memory can be reclaimed filteredDF.unpersist() # Check if it is cached print filteredDF.is_cached ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:8:2","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"** Part 8: Debugging Spark applications and lazy evaluation ** ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:9:0","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"How Python is Executed in Spark Internally, Spark executes using a Java Virtual Machine (JVM). pySpark runs Python code in a JVM using Py4J. Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects. Because pySpark uses Py4J, coding errors often result in a complicated, confusing stack trace that can be difficult to understand. In the following section, we’ll explore how to understand stack traces. ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:9:1","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(8a) Challenges with lazy evaluation using transformations and actions Spark’s use of lazy evaluation can make debugging more difficult because code is not always executed immediately. To see an example of how this can happen, let’s first define a broken filter function. Next we perform a filter() operation using the broken filtering function. No error will occur at this point due to Spark’s use of lazy evaluation. The filter() method will not be executed until an action operation is invoked on the DataFrame. We will perform an action by using the count() method to return a list that contains all of the elements in this DataFrame. def brokenTen(value): \"\"\"Incorrect implementation of the ten function. Note: The `if` statement checks an undefined variable `val` instead of `value`. Args: value (int): A number. Returns: bool: Whether `value` is less than ten. Raises: NameError: The function references `val`, which is not available in the local or global namespace, so a `NameError` is raised. \"\"\" if (val \u003c 10): return True else: return False btUDF = udf(brokenTen) brokenDF = subDF.filter(btUDF(subDF.age) == True) # Now we'll see the error # Click on the `+` button to expand the error and scroll through the message. brokenDF.count() ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:9:2","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(8b) Finding the bug When the filter() method is executed, Spark calls the UDF. Since our UDF has an error in the underlying filtering function brokenTen(), an error occurs. Scroll through the output “Py4JJavaError Traceback (most recent call last)” part of the cell and first you will see that the line that generated the error is the count() method line. There is nothing wrong with this line. However, it is an action and that caused other methods to be executed. Continue scrolling through the Traceback and you will see the following error line: NameError: global name 'val' is not defined Looking at this error line, we can see that we used the wrong variable name in our filtering function brokenTen(). ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:9:3","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(8c) Moving toward expert style As you are learning Spark, I recommend that you write your code in the form: df2 = df1.transformation1() df2.action1() df3 = df2.transformation2() df3.action2() Using this style will make debugging your code much easier as it makes errors easier to localize - errors in your transformations will occur when the next action is executed. Once you become more experienced with Spark, you can write your code with the form: df.transformation1().transformation2().action() We can also use lambda() functions instead of separately defined functions when their use improves readability and conciseness. # Cleaner code through lambda use myUDF = udf(lambda v: v \u003c 10) subDF.filter(myUDF(subDF.age) == True) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:9:4","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"(8d) Readability and code style To make the expert coding style more readable, enclose the statement in parentheses and put each method, transformation, or action on a separate line. # Final version from pyspark.sql.functions import * (dataDF .filter(dataDF.age \u003e 20) .select(concat(dataDF.first_name, lit(' '), dataDF.last_name), dataDF.occupation) .show(truncate=False) ) ","date":"2016-06-28","objectID":"/code/2016-06-28-spark-tutorial/:9:5","tags":["spark","CS105x"],"title":"Spark Tutorial - Learning Apache Spark","uri":"/code/2016-06-28-spark-tutorial/"},{"categories":null,"content":"梯度提升算法简介 ","date":"2016-06-23","objectID":"/2016-06-24-gradient-boosting/:0:0","tags":["machine learning","统计学习方法","boosting"],"title":"Boosting(3)- Gradient Boosting","uri":"/2016-06-24-gradient-boosting/"},{"categories":null,"content":"Boosting on different Loss Functions In the last section, we use forward stagewise on an addictive model to solve the optimization of adaboost. The exponential loss function is relatively easy to handle, but other loss functions may not. Recall from the last section, we have loss function like this: $$ L(y_i, f(x_i)) $$ Consider optimizing an arbitary loss function using forward stagewise: $$ (\\beta_m, \\gamma_m) = arg\\min\\limits_{\\beta, \\gamma} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i;\\gamma)) \\tag{1}$$ Now since $\\beta b(x_i;\\gamma)$ is relatively small compared to $f_{m-1}(x_i)$, using Taylor Series we could unfold it around $f_{m-1}(x_i)$: $$ L \\approx \\frac{1}{N} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i)) + \\beta \\sum\\limits_{i=1}^N \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} b(x_i;\\gamma) \\tag{2}$$ Add regularzation to (2) to avoid $b(x_i;\\gamma)$ being too big, since we already have $\\beta$ to adjust the term’s magnitude: $$\\begin{align} L \u0026 \\approx \\frac{1}{N} \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i)) + \\frac{\\beta}{2} \\sum\\limits_{i=1}^N \\left. { 2 \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} b(x_i;\\gamma) + b^2(x_i;\\gamma) \\\\ \\text{(Strip the constants)} \u0026 = \\beta \\sum\\limits_{i=1}^N 2 \\frac{\\partial L}{\\partial s} b(x_i;\\gamma) + b^2(x_i;\\gamma) \\\\ \u0026 = \\beta \\sum\\limits_{i=1}^N (b(x_i;\\gamma) + \\frac{\\partial L}{\\partial s})^2 - (\\frac{\\partial L}{\\partial s} )^2 \\end{align} \\tag{3}$$ ","date":"2016-06-23","objectID":"/2016-06-24-gradient-boosting/:0:1","tags":["machine learning","统计学习方法","boosting"],"title":"Boosting(3)- Gradient Boosting","uri":"/2016-06-24-gradient-boosting/"},{"categories":null,"content":"Gradient Boosting Now we could minimize the loss function using the last section’s results: Optimize $b(x_i;\\gamma)$. From (3) we get: $$\\gamma_m = arg\\min\\limits_\\gamma \\beta \\sum\\limits_{i=1}^N \\left(b(x_i;\\gamma) + \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)} \\right)^2$$ That is, just train the base model $b(x_i;\\gamma_m)$ with the loss function’s gradient $-\\frac{\\partial L(y_i, s)}{\\partial s}$ in every step $m$, that’s why it calld gradient boosting: $$ \\text{fit } b(x_i;\\gamma_m) = - \\left. { \\frac{\\partial L(y_i, s)}{\\partial s} } \\right |_{s=f_{m-1}(x_i)}$$ Optimize $\\beta$ $$\\beta_m = arg\\min\\limits_\\beta \\sum\\limits_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i;\\gamma_m))$$ Since we already have $y_i$, $f_{m-1}(x_i)$ and $b(x_i;\\gamma_m)$, this is just a one-dimensional optimization problem, which is easy to handle. The full algorithm is similar, see the reference of wiki. The base model used most frequently is decision trees. Reference: 统计学习方法 机器学习技法 Wikipedia ","date":"2016-06-23","objectID":"/2016-06-24-gradient-boosting/:0:2","tags":["machine learning","统计学习方法","boosting"],"title":"Boosting(3)- Gradient Boosting","uri":"/2016-06-24-gradient-boosting/"},{"categories":null,"content":"Boosting理论基础: 和前向分步算法的等价性 ","date":"2016-06-22","objectID":"/2016-06-22-forward-stagewise-algorithm/:0:0","tags":["machine learning","统计学习方法","boosting"],"title":"Boosting(2) - Adaboost and Forward Stagewise","uri":"/2016-06-22-forward-stagewise-algorithm/"},{"categories":null,"content":"Forward stagewise Consider an additive model like adaboost: $$ f(x) = \\sum\\limits_{m=1}^M \\beta_m b(x; \\gamma_m) $$ in which $b(x; \\gamma_m)$ is the base model, $\\gamma_m$ is the model’s parameters, $\\beta_m$ is it’s coefficient. To minimim the loss function, we have the forward stagewise algorithm, which minimize one model \u0026 coefficient at a time, that is, take the models already trained as constant, that’s the core concept of this algorithm: Input: Training data $T = \\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(N)}, y^{(N)}) \\rbrace$, lost function$L(y, f(x))$ and base model $b(x; \\gamma)$ Output: additive model $f(x)$ Initialize $f_0(x) = 0$ For $m = 1,2,\\cdots,M$: (1) Minimize loss function: $$ (\\beta_m, \\gamma_m) = arg\\min\\limits_{\\beta, \\gamma} \\sum\\limits_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + \\beta b(x^{(i)};\\gamma)) $$ (2) Update $f(x)$: $$ f_m(x) = f_{m-1}(x) + \\beta_m b(x^{(i)};\\gamma_m)$$ Get additive model: $$ f(x) = \\sum\\limits_{m=1}^M \\beta_m b(x; \\gamma_m) $$ ","date":"2016-06-22","objectID":"/2016-06-22-forward-stagewise-algorithm/:0:1","tags":["machine learning","统计学习方法","boosting"],"title":"Boosting(2) - Adaboost and Forward Stagewise","uri":"/2016-06-22-forward-stagewise-algorithm/"},{"categories":null,"content":"Equivalence of adaboost and Forward stagewise When using additive model \u0026 exponential loss function, forward stagewise algorithm is equivalent to adaboost. Here is the proof: Assume the exponential loss function is: $$ L(y, f(x)) = exp(-yf(x)) $$ In the $m$th iteration, we have: $$ f_m(x) = f_{m-1}(x) + \\alpha_mG_m(x), \\\\ \\text{in which } f_{m-1}(x) = \\sum\\limits_{m=1}^{M-1} \\alpha_mG_m(x) $$ To minimize $$\\begin{align} (\\alpha_m, G_m) \u0026 = arg\\min\\limits_{\\alpha, G} \\sum\\limits_{i=1}^N exp \\left\\lbrace -y^{(i)}(f_{m-1}(x^{(i)}) + \\alpha_m G_m(x^{(i)}))\\right\\rbrace \\\\ \u0026 = arg\\min\\limits_{\\alpha, G} \\sum\\limits_{i=1}^N w_{mi} exp \\left\\lbrace -y^{(i)} \\alpha_m G_m(x^{(i)}) \\right\\rbrace \\end{align} $$ in which $w_{mi} = exp(-y^{(i)} f_{m-1}(x^{(i)}))$, it depends on neither $\\alpha$ nor $G$. To prove the $\\alpha_m^*, G_m^*$ achieved here is exatly the $\\alpha_m, G_m$ in adaboost: For any $\\alpha \u003e 0$, $G_m^*(x)$ which maximize the loss function is the one who has the minimum error rate: $$ G_m^*(x) = arg\\min\\limits_{G} \\sum\\limits_{i=1}^N w_{mi} I(y^{(i)} \\neq G(x^{(i)})) $$ That’s exatly what we train the base model to do, so $G_m^*(x) = G_m(x)$ Then calculate $\\alpha_m$ $$\\begin{align} L(\\alpha, G_m) \u0026 = \\sum\\limits_{i=1}^N w_{mi} exp \\left\\lbrace -y^{(i)} \\alpha G_m(x^{(i)}) \\right\\rbrace \\\\ \u0026 = e^{-\\alpha} \\sum\\limits_{=} w_{mi} + e^{\\alpha} \\sum\\limits_{\\neq} w_{mi} \\\\ \u0026 = e^{-\\alpha} \\sum\\limits w_{mi} - e^{-\\alpha} \\sum\\limits_{\\neq} w_{mi} + e^{\\alpha} \\sum\\limits_{\\neq} w_{mi} \\\\ \u0026 = (e^{\\alpha} - e^{-\\alpha}) \\sum\\limits_{\\neq} w_{mi} + e^{-\\alpha} \\sum\\limits w_{mi} \\end{align} $$ in which $\\sum\\limits_{\\neq}$ is abbreviation for $\\sum\\limits_{y^{(i)} \\neq G(x^{(i)})}$, $\\sum$ is abbreviation for $\\sum\\limits_{i=1}^N$ Let the partial derivative=0, we have: $$\\frac {\\partial L(\\alpha, G_m)} {\\partial \\alpha} = (e^{\\alpha} + e^{-\\alpha}) \\sum\\limits_{\\neq} w_{mi} - e^{-\\alpha} \\sum\\limits w_{mi} = 0$$ s.t. $$(e^{\\alpha} + e^{-\\alpha}) \\frac{\\sum\\limits_{\\neq} w_{mi}}{\\sum\\limits w_{mi}} - e^{-\\alpha} = 0$$ s.t. $$(e^{\\alpha} + e^{-\\alpha}) \\epsilon_m - e^{-\\alpha} = 0$$ s.t. $$ \\alpha^* = \\frac{1}{2}ln \\frac{1-\\epsilon_m}{\\epsilon_m} $$ in which $\\epsilon_m = \\frac{\\sum\\limits_{\\neq} w_{mi}}{\\sum\\limits w_{mi}}$, which is the error rate of $G_m^*(x)$ Finally we could see that the $(\\alpha_m, G_m)$ we get here is exatly the same as in adaboost. ","date":"2016-06-22","objectID":"/2016-06-22-forward-stagewise-algorithm/:0:2","tags":["machine learning","统计学习方法","boosting"],"title":"Boosting(2) - Adaboost and Forward Stagewise","uri":"/2016-06-22-forward-stagewise-algorithm/"},{"categories":null,"content":"All the resources from codecademy ","date":"2016-06-19","objectID":"/code/2016-06-20-sql-abc/:0:0","tags":["SQL"],"title":"SQL 速查","uri":"/code/2016-06-20-sql-abc/"},{"categories":null,"content":"Basic Operations celebs: $$ \\begin{array}{c|c|c|c} \\text{id} \u0026 \\text{name} \u0026 \\text{age} \u0026 \\text{twitter_handle} \\\\ \\hline 1 \u0026 \\text{Justin Bieber} \u0026 22 \u0026 \\\\ 2 \u0026 \\text{Beyonce Knowles} \u0026 33 \u0026 \\\\ 3 \u0026 \\text{Jeremy Lin} \u0026 26 \u0026 \\\\ \\end{array} $$ Table Creation CREATE TABLE celebs (id INTEGER, name TEXT, age INTEGER, twitter_handle TEXT); Changing values UPDATE celebs SET age = 22 WHERE id = 1; Inserting Insert a row: INSERT INTO celebs (id, name, age) VALUES (1, 'Justin Bieber', 21); Insert a column: ALTER TABLE celebs ADD COLUMN twitter_handle TEXT; Deletion DELETE FROM celebs WHERE twitter_handle IS NULL; ","date":"2016-06-19","objectID":"/code/2016-06-20-sql-abc/:0:1","tags":["SQL"],"title":"SQL 速查","uri":"/code/2016-06-20-sql-abc/"},{"categories":null,"content":"Queries movies: $$ \\begin{array}{c|c|c|c|c} \\text{id} \u0026 \\text{name} \u0026 \\text{genre} \u0026 \\text{year} \u0026 \\text{imdb_rating} \\\\ \\hline 1 \u0026 \\text{Avatar} \u0026 \\text{action} \u0026 2009 \u0026 7.9 \\\\ 2 \u0026 \\text{Jurassic World} \u0026 \\text{action} \u0026 2015 \u0026 7.3 \\\\ 3 \u0026 \\text{The Avengers} \u0026 \\text{action} \u0026 2012 \u0026 8.1 \\\\ \\end{array} $$ Select one or more column: SELECT * FROM movies; SELECT name, imdb_rating FROM movies; Select distinct elements SELECT DISTINCT genre FROM movies; Select values specifying rules: Sinple rules: SELECT * FROM movies WHERE imdb_rating \u003e 8; Regular expression-like rules: SELECT * FROM movies WHERE name LIKE 'Se_en'; SELECT * FROM movies WHERE name LIKE 'A%'; SELECT * FROM movies WHERE name BETWEEN 'A' AND 'J'; Combine different rules: SELECT * FROM movies WHERE genre = 'comedy' OR year \u003c 1980; Ordering: SELECT * FROM movies ORDER BY imdb_rating DESC; DESC is a keyword in SQL that is used with ORDER BY to sort the results in descending order, ASC works similarly SELECT * FROM movies ORDER BY imdb_rating ASC LIMIT 3; LIMIT specifies that the result table returned can not have more than three rows ","date":"2016-06-19","objectID":"/code/2016-06-20-sql-abc/:0:2","tags":["SQL"],"title":"SQL 速查","uri":"/code/2016-06-20-sql-abc/"},{"categories":null,"content":"Aggregate Functions fake_apps: $$ \\begin{array}{c|c|c|c|c} \\text{id} \u0026 \\text{name} \u0026 \\text{category} \u0026 \\text{downloads} \u0026 \\text{price} \\\\ \\hline 1 \u0026 \\text{siliconphase} \u0026 \\text{Productivity } \u0026 17193 \u0026 0.0 \\\\ 2 \u0026 \\text{Donzolab} \u0026 \\text{Education} \u0026 4259 \u0026 0.99 \\\\ 3 \u0026 \\text{Ittechi} \u0026 \\text{Reference} \u0026 3874 \u0026 0.0 \\\\ \\end{array} $$ Counting SELECT COUNT(*) FROM fake_apps; SELECT price, COUNT(*) FROM fake_apps GROUP BY price; SELECT price, COUNT(*) FROM fake_apps WHERE downloads \u003e 20000 GROUP BY price; COUNT() is a function that takes the name of a column as an argument and counts the number of rows where the column is not NULL. Here, we want to count every row so we pass * as an argument. SUM, MAX, AVG, ROUND Return the sum of downloads under each category: SELECT category, SUM(downloads) FROM fake_apps GROUP BY category; Return the name and category of the most frequently downloaded apps in each category: SELECT name, category, MAX(downloads) FROM fake_apps GROUP BY category; Calculate the average number of downloads at each price SELECT price, AVG(downloads) FROM fake_apps GROUP BY price; Rounding the average number of downloads to two decimal(default is 0): SELECT price, ROUND(AVG(downloads), 2) FROM fake_apps GROUP BY price; ","date":"2016-06-19","objectID":"/code/2016-06-20-sql-abc/:0:3","tags":["SQL"],"title":"SQL 速查","uri":"/code/2016-06-20-sql-abc/"},{"categories":null,"content":"Multiple Tables albums: $$ \\begin{array}{c|c|c|c} \\text{id} \u0026 \\text{name} \u0026 \\text{artist_id} \u0026 \\text{year} \\\\ \\hline 1 \u0026 \\text{A Hard Days Night} \u0026 1 \u0026 1964 \\\\ 2 \u0026 \\text{Elvis Presley} \u0026 2 \u0026 1956 \\\\ 3 \u0026 \\text{Unorthodox Jukebox} \u0026 \u0026 2012 \\\\ \\end{array} $$ artists: $$ \\begin{array}{c|c} \\text{id} \u0026 \\text{name} \\\\ \\hline 1 \u0026 \\text{The Beatles} \\\\ 2 \u0026 \\text{Elvis Presley} \\\\ 3 \u0026 \\text{Unorthodox Jukebox} \\\\ \\end{array} $$ Primary Key CREATE TABLE artists(id INTEGER PRIMARY KEY, name TEXT) A primary key serves as a unique identifier for each row. By specifying that the id column is the PRIMARY KEY, SQL makes sure that: None of the values in this column are NULL Each value in this column is unique Selection from multiple tables: SELECT albums.name, albums.year, artists.name FROM albums, artists; Unfortunately, this operation would return a table which combines every row of the artists table with every row of the albums table. Join 2 different tables through rule: SELECT * FROM albums JOIN artists ON albums.artist_id = artists.id; Here, artist_id is a foreign key in the albums table. A foreign key is a column that contains the primary key of another table in the database. SELECT * FROM albums LEFT JOIN artists ON albums.artist_id = artists.id; Every row in the left table is returned in the result set, and if the join condition is not met, then NULL values are used to fill. The left table is simply the first table that appears in the statement. Set alias SELECT albums.name AS 'Album', albums.year, artists.name AS 'Artist' FROM albums JOIN artists ON albums.artist_id = artists.id WHERE albums.year \u003e 1980; AS is a keyword in SQL that allows you to use an alias. The aliases only appear in the result set, columns have not been renamed Written with StackEdit. ","date":"2016-06-19","objectID":"/code/2016-06-20-sql-abc/:0:4","tags":["SQL"],"title":"SQL 速查","uri":"/code/2016-06-20-sql-abc/"},{"categories":null,"content":"Adaptive Boosting 算法 ","date":"2016-06-06","objectID":"/2016-06-06-adaboost/:0:0","tags":["machine learning","boosting","统计学习方法"],"title":"Boosting(1) - AdaBoost","uri":"/2016-06-06-adaboost/"},{"categories":null,"content":"Combine different classifiers \u0026 weights Test data: $$T = \\left\\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(N)}, y^{(N)})\\right\\rbrace, \\quad x \\in \\chi \\subseteq R^N, y \\in \\lbrace +1, -1 \\rbrace \\\\ \\text{with weights: } D_i = (w_{i1}, w_{i2}, \\cdots, w_{iN})$$ Classifier: $$G_m(x): \\chi \\to \\lbrace +1, -1\\rbrace, \\quad G(x) = \\sum\\limits^M_{i=1} \\alpha_m G_m(x) \\\\ \\text{with weights: } A = (\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{M}) $$ ","date":"2016-06-06","objectID":"/2016-06-06-adaboost/:0:1","tags":["machine learning","boosting","统计学习方法"],"title":"Boosting(1) - AdaBoost","uri":"/2016-06-06-adaboost/"},{"categories":null,"content":"AdaBoost Algorithm Input: Training data $T$, classifier $G_m(x)$ Output: Ensemble $G(x)$ Initialize data weights: $$ D_1 = (w_{11}, w_{12}, \\cdots, w_{1N}), \\quad w_{1i} = \\frac{1}{N} $$ For $m = 1,2, \\cdots, M$: (a) Train data with $D_m = (w_{m1}, w_{m2}, \\cdots, w_{mN})$ weights on $T$, generate $G_m(x)$ (b) Calculate error rate of $G_m(x)$ on $T$: $$ e_m = P(G_m(x^{(i)}) \\neq y^{(i)}) = \\sum\\limits^N_{i=1} w_{mi}I(G_m(x^{(i)}) \\neq y^{(i)}) \\tag{1}$$ (c) Calculate $G_m(x)$’s weight $\\alpha_m$: $$ \\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m} \\tag{2}$$ (d) Update $D_{m+1}$: $$ D_{m+1} = (w_{m+1, 1}, w_{m+1, 2}, \\cdots, w_{m+1, N}) $$ $$w_{m+1, i} =\\begin{cases} \\frac{w_{mi}}{Z_m} e^{-\\alpha_m}, \u0026 G_m(x^{(i)}) = y^{(i)} \\\\ \\\\ \\frac{w_{mi}}{Z_m}e^{\\alpha_m}, \u0026 G_m(x^{(i)}) \\neq y^{(i)} \\end{cases} = \\frac {w_{mi}e^{-\\alpha_m y^{(i)} G_m(x^{(i)})}} {Z_m} \\tag{3}$$ $$Z_m = \\sum\\limits^N_{i=1}w_{mi}e^{-\\alpha_m y^{(i)} G_m(x^{(i)})} $$ In which $Z_m$ is normalization factor, which makes $D_{m+1}$ a probability distribution, keeping $\\sum\\limits^N_{i=1}w_{m+1, i} = 1$ Combine all the $G_m(x)$: $$ f(x) = \\sum\\limits^M_{i=1} \\alpha_m G_m(x), \\quad G(x) = sign(f(x))$$ Note on implementation: The weights on datasets could be implemented with sampling the original dataset with a probability distribution of $D_i$ ","date":"2016-06-06","objectID":"/2016-06-06-adaboost/:0:2","tags":["machine learning","boosting","统计学习方法"],"title":"Boosting(1) - AdaBoost","uri":"/2016-06-06-adaboost/"},{"categories":null,"content":"Intuitive Explanation Here is how $G_m(x)$’s coefficient, $ \\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m} $ varies through $e_m$, it’s easy to see that the more precise model will get a larger weight. Contrarily, from (3) it’s easy to tell that the samples get classified wrong will increase their weights, while the weights of the samples get classified correct will decrease, both in an exponential rate. ","date":"2016-06-06","objectID":"/2016-06-06-adaboost/:0:3","tags":["machine learning","boosting","统计学习方法"],"title":"Boosting(1) - AdaBoost","uri":"/2016-06-06-adaboost/"},{"categories":null,"content":"决策树 ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:0:0","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"CN ver. ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:1:0","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"1. 熵 熵的定义和变量的概率分布有关, 和变量本身的值无关, 定义如下: $$ H(X) = - \\sum\\limits_{i=1}^{n} p_i log p_i \\\\ \\text{其中} P(X=x_i) = p_i, i= 1,2,\\cdots, n$$ $H(X)$的一个主要特点是$x$的分布越平均, 熵越大, 因此熵代表了分布的混乱程度 另外, 对数底一般取2或e, 其单位为bit或nat ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:1:1","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"2. 条件熵 假设两个随机变量$(X, Y)$的联合概率分布为$$P(X=x_i, Y=y_j) = p_{ij} \\\\ |X| = n, |Y| = m$$ 根据贝叶斯公式, $Y$对于$X$的条件概率为$$ P(Y = y_j | X=x_i) = \\frac{P(X=x_i, Y=y_j)}{P(X=x_i)} = \\frac{p_{ij}}{p_i}$$ 那么在给定$X= x_i$的条件下$Y$的条件概率分布的熵为 $$ \\begin{align} H(Y | X=x_i) \u0026 = - \\sum\\limits_{j=1}^{m} P(Y = y_j | X=x_i) log P(Y = y_j | X=x_i) \\\\ \u0026 = - \\sum\\limits_{j=1}^{m} \\frac{p_{ij}}{p_i}log \\frac{p_{ij}}{p_i} \\end{align} $$ 最后, $H(Y | X=x_i)$对于$X$的数学期望就是$Y$对于$X$的条件熵: $$ H(Y | X) = \\sum\\limits_{i=1}^{n} p_i H(Y | X=x_i)$$ ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:1:2","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"3. 一个例子 条件熵的概念, 可以通过例子理解 假设$X$是某位长者前去魔都的概率分布, 去是1, 不去是0 $$P(X=1) = 0.5, P(X=0) = 0.5$$ 假设$Y$是魔都天气的概率分布, 1是晴, 0是雨 $$P(Y=1) = 0.5, P(X=0) = 0.5$$ 也就是说, 去不去看心情, 天气好不好看老天. 然而 $$ P(Y=1 |X=1 ) = 1 \\\\ P(Y=1 |X=0 ) = 0 \\\\ P(Y=0 |X=1 ) = 0 \\\\ P(Y=0 |X=0 ) = 1$$ 也就是说主席一来, 天气晴朗, 主席一走, 立马下雨 那么求一下$Y$的条件概率分布的熵 $$H(Y | X=1) = - (\\frac{P_{11}}{P_{1}} log \\frac{P_{11}}{P_{1}} + \\frac{P_{01}}{P_{1}} log \\frac{P_{01}}{P_{1}}) = -2 \\\\ H(Y | X=不去) = - (\\frac{P_{10}}{P_{0}} log \\frac{P_{10}}{P_{0}} + \\frac{P_{00}}{P_{0}} log \\frac{P_{00}}{P_{0}}) = -2 $$ 最后得到条件熵 $$ H(Y | X) =p_1 H(Y | X=1) + p_0H(Y | X=0) = -2$$ 总结一下, $H(X) = H(Y) = 1$, 然而$H(Y | X) = -2$, 也就是在主席到来的约束下, $Y$的条件熵下降了, 也就是天气分布的不确定性下降了, 可见主席来不来对天气好坏是有很大影响的 ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:1:3","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"4. 信息增益 信息增益表示得知特征$X$的信息而使得类Y的信息的不确定性减少的程度. 拿之前的例子来说, 选择主席是否来这个特征对天气数据集的信息增益为 $$ g(Y, X) = H(Y) - H(Y|X) = 1 - (-2) = 3$$ 决策树的特征选择基础就是建立在选择最大信息增益的特征上的. ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:1:4","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"5. 信息增益比 信息增益的一个缺点是决策树会偏向于选择取值较多的特征. 例如假设每个$X$的取值对应于独一无二的值(item_id类型的数据)有: $$ \\begin{align} H(Y | X=x_i) \u0026 = - \\sum\\limits_{j=1}^{m} P(Y = y_j | X=x_i) log P(Y = y_j | X=x_i) \\\\ \u0026 = - \\sum\\limits_{j=1}^{m} \\frac{p_{ij}}{p_i}log \\frac{p_{ij}}{p_i} \\\\ \u0026 = - \\frac{1/N}{1/N}log \\frac{1/N}{1/N} = 0 \\end{align} $$ $$ H(Y | X) = \\sum\\limits_{i=1}^{n} p_i H(Y | X=x_i) = 0 $$, 条件熵降到了0, 显然信息增益是最大的. 但这样的分类毫无意义, 属于极端过拟合.因此引入信息增益比. 首先由于$X$的划分行为, 导致数据集label$Y$的熵增加了(每一类内部的熵减少了) $$ H_X(Y) = - \\sum\\limits^{n}_{i=1} P(X = x_i) log P(X = x_i) $$ $$ g_R(Y, X) = \\frac{H(Y) - H(Y|X)}{H_X(Y)} $$ 这样一来使用之前的例子, $ H_X(Y) = - n \\times \\frac{1}{n} log \\frac{1}{n} = - log \\frac{1}{n} $, $n$越大, 分得越细, 信息增益的分母越大, 信息增益比越小. ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:1:5","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"EN ver. ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:2:0","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"1. Sample Partition $$ \\begin{array}{c|c} \\text{A|C} \u0026 c_{1}, \\ c_{2}, \\ \\cdots, \\ c_{K} \\\\ \\hline a_1 \u0026 s_{11}, s_{12}, \\cdots, s_{1K} \\\\ a_2 \u0026 s_{21}, s_{22}, \\cdots, s_{2K} \\\\ a_3 \u0026 s_{31}, s_{32}, \\cdots, s_{3K} \\\\ \\vdots \u0026 \\vdots \\\\ a_N \u0026 s_{N1}, s_{N2}, \\cdots, s_{NK} \\end{array} $$ Assume a set of samples have $K$ different labels named $c_{1}, \\ c_{2}, \\ \\cdots, \\ c_{K}$, a feature $A$ has $N$ different values named $a_1, a_2, \\cdots, a_N$, for any sample $s_{ij}$, $s_{ij} \\in A_i \\cap C_j$ ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:2:1","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"2. Entropy, Conditional Entropy and Information Gain Using Maximum Likelihood Estimation, the empirical entropy \u0026 empirical conditional entropy could be represented as: $$ H(S) = - \\sum\\limits_{j=1}^{K} P(s \\in C_j) log_2 P(s \\in C_j) = - \\sum\\limits_{j=1}^{K} \\frac{|C_j|}{|S|} log_2(\\frac{|C_j|}{|S|}), \\quad |S| = \\sum\\limits_{j=1}^{K} |C_j| \\tag{1} $$ $$ \\begin{align} H(S|A) \u0026 = \\sum\\limits_{i=1}^{N} P(s \\in A_i) H(S|s \\in A_i) \\\\ \u0026 = \\sum\\limits_{i=1}^{N} P(s \\in A_i) \\left [ - \\sum\\limits_{j=1}^{K} P(s_{ij} \\in A_i \\cap C_j)log_2 P(s_{ij} \\in A_i \\cap C_j) \\right ] \\\\ \u0026 = \\sum\\limits_{i=1}^{N} \\frac{|s_{i\\cdot}|}{|S|} \\left [ - \\sum\\limits_{j=1}^{K} \\frac{|s_{ij}|}{|s_{i\\cdot}|}log_2 \\frac{|s_{ij}|}{|s_{i\\cdot}|}\\right ] \\end{align} \\tag{2} $$ Using the two formulas above, the information gain from $A$ in sample $S$ is: $$g(S, A) = H(S) - H(S|A) \\tag{3}$$ Using information gain to choose the feature has one disadvantage: it tends to select the feature which has the largest amount of values. To fix this, using information gain ratio instead: $$ g_R(S, A) = \\frac{g(S, A)}{H_A(S)}, \\quad H_A(S) = - \\sum\\limits_{i=1}^{N} \\frac{|s_{i\\cdot}|}{|S|}log_2\\frac{|s_{i\\cdot}|}{|S|} \\tag{4} $$ ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:2:2","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"3. Tree Building ID3 Input: Train set $S$, feature set $A$, tolerance $\\epsilon$ Output: Decision Tree $T$ (1) If $s_i \\in C_j$ for all $i$: $label = c_k$, return T (2) If $A = \\varnothing$: $label=c_j: \\max(|c_j|)$, return T (3) Else: calculate $g(S, A_i)$ for all $i$, choose $A_g$ in which $g = argmax(g(S, A_i))$ (4) If $g(S, A_g) \u003c \\epsilon$: $label= c_j: \\max(|c_j|)$, return T (5) Else: Split $S$ into $S_i$ for each $a_i$ in $A_g$, $label_i= c_j: \\max(|c_j|), c_j \\in S_i$, return T (6) For each $a_i$ as a node, train set = $S_i$, feature set = $A - A_g$, run (1) ~ (5) recursively, return $T_i$ C4.5 Using $g_R(S, A)$ instead of $g(S, A)$ in ID3 step (3) and (4) Notice: Using information gain ratio tends to choose the feature which have less values, so C4.5 uses a heuristic method: first choose the ones have information gain above the average, then choose the one with the larger information gain ratio ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:2:3","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"4. Pruning Assume tree $T$ has $|T|$ leaf nodes $t$, each $t$ has $N_t$ samples, in which $N_{tk}$ samples $\\in C_k$, for each $t$, sum all the empirical entropy to get the cost function: $$ C_\\alpha(T) = \\sum\\limits_{t=1}^{|T|} N_t H_t(T) + \\alpha|T| \\\\ H_t(T) = -\\sum\\limits_k \\frac{N_{tk}}{N_t} log_2 \\frac{N_{tk}}{N_t} \\tag{5} $$ The pruning algorithm recursively calculates $C_\\alpha(T)$ to find the minimum cost. Another prunning method called pre-prunning using the test set and train set, if the current splitting causes worse performance on the test set, then cancel the prunning ","date":"2016-05-22","objectID":"/2016-05-22-decision-tree/:2:4","tags":["machine learning","decision tree","统计学习方法"],"title":"决策树 (Decision Tree)","uri":"/2016-05-22-decision-tree/"},{"categories":null,"content":"SMO算法的python实现 Full implementation here ","date":"2016-05-16","objectID":"/2016-05-16-smo-implementation/:0:0","tags":["machine learning","svm","machine-learning-in-action"],"title":"Support Vector Machine(4)-SMO implementation","uri":"/2016-05-16-smo-implementation/"},{"categories":null,"content":"SMO.1: Calculating $L$, $H$ and $E_i$ if p.y[i] != p.y[j]: L = max(0, p.a[j] - p.a[i]) H = min(p.c, p.a[j] - p.a[i] + p.c) else: L = max(0, p.a[j] + p.a[i] - p.c) H = min(p.c, p.a[j] + p.a[i]) def calc_ei(p, i): f_xi = float(np.dot((p.a * p.y).T, np.dot(p.x, p.x[i].T)) + p.b) ei = f_xi - float(p.y[i]) return ei ","date":"2016-05-16","objectID":"/2016-05-16-smo-implementation/:0:1","tags":["machine learning","svm","machine-learning-in-action"],"title":"Support Vector Machine(4)-SMO implementation","uri":"/2016-05-16-smo-implementation/"},{"categories":null,"content":"SMO.2: Calculating $\\alpha_2^{i+1, unc}$ eta = np.dot(p.x[i], p.x[i].T) + np.dot(p.x[j], p.x[j].T) - 2 * np.dot(p.x[i], p.x[j].T) p.a[j] += p.y[j] * (ei - ej) / eta ","date":"2016-05-16","objectID":"/2016-05-16-smo-implementation/:0:2","tags":["machine learning","svm","machine-learning-in-action"],"title":"Support Vector Machine(4)-SMO implementation","uri":"/2016-05-16-smo-implementation/"},{"categories":null,"content":"SMO.3: Clipping $\\alpha_2^{i+1, unc}$ def clip_a(ai, H, L): if ai \u003e H: return H elif ai \u003c L: return L return ai p.a[j] = clip_a(p.a[j], H, L) ","date":"2016-05-16","objectID":"/2016-05-16-smo-implementation/:0:3","tags":["machine learning","svm","machine-learning-in-action"],"title":"Support Vector Machine(4)-SMO implementation","uri":"/2016-05-16-smo-implementation/"},{"categories":null,"content":"SMO.4: Calculating $\\alpha_1^{i+1}$ p.a[i] = p.a[i] + p.y[i] * p.y[j] * (a_j_old - p.a[j]) ","date":"2016-05-16","objectID":"/2016-05-16-smo-implementation/:0:4","tags":["machine learning","svm","machine-learning-in-action"],"title":"Support Vector Machine(4)-SMO implementation","uri":"/2016-05-16-smo-implementation/"},{"categories":null,"content":"SMO.5: Selecting $\\alpha_1$, $\\alpha_2$ Selecting $\\alpha_1$ that breaks the KKT conditions within precision tol if ((p.a[i] \u003c p.c) and (p.y[i] * ei \u003c -p.tol)) or \\ ((p.a[i] \u003e 0) and (p.y[i] * ei \u003e p.tol)): j, ej = select_aj(i, p, ei) Selecting $\\alpha_2$ which gives the largest $\\vert E^i_1 - E^i_2 \\vert$. In order to save time for calculation, save all the $E_i$ in e_cache advance. If there is none, using random_select_aj() randomly select one instead. def select_aj(i, p, ei): max_index = -1; ej = 0; max_delta_e = 0 p.e_cache[i] = [1, ei] valid_indexes = np.transpose(np.nonzero(p.e_cache[:, 0])) if len(valid_indexes) \u003e 1: for k in valid_indexes: if k == i: continue ek = calc_ei(p, k) delta_e = abs(ei - ek) if delta_e \u003e max_delta_e: max_index = k; max_delta_e = delta_e; ej = ek return max_index, ej else: j = random_select_aj(i, p.m) ej = calc_ei(p, j) return j, ej ","date":"2016-05-16","objectID":"/2016-05-16-smo-implementation/:0:5","tags":["machine learning","svm","machine-learning-in-action"],"title":"Support Vector Machine(4)-SMO implementation","uri":"/2016-05-16-smo-implementation/"},{"categories":null,"content":"SMO.6: Calculating $b$ bi = p.b - (ei + p.y[i] * np.dot(p.x[i], p.x[i].T) * (p.a[i] - a_i_old) + p.y[j] * np.dot(p.x[j], p.x[i].T) * (p.a[j] - a_j_old)) bj = p.b - (ej + p.y[i] * np.dot(p.x[i], p.x[j].T) * (p.a[i] - a_i_old) + p.y[j] * np.dot(p.x[j], p.x[j].T) * (p.a[j] - a_j_old)) if (p.a[i] \u003e 0) and (p.a[i] \u003c p.c): p.b = bi elif (p.a[j] \u003e 0) and (p.a[j] \u003c p.c): p.b = bj else: p.b = (bi + bj) / 2.0 ","date":"2016-05-16","objectID":"/2016-05-16-smo-implementation/:0:6","tags":["machine learning","svm","machine-learning-in-action"],"title":"Support Vector Machine(4)-SMO implementation","uri":"/2016-05-16-smo-implementation/"},{"categories":null,"content":"序列最小化算法 Recall from the last section: $ \\begin{align} \u0026 \\min\\limits_{\\alpha} \\quad \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} K(x^{(i)}, x^{(j)}) - \\sum\\limits_{i=1}^{N} \\alpha_i \\end{align} \\tag{3.2}$ $ s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\quad 0 \\le \\alpha_i \\le C \\tag{3.4}$ To solve the optimization problem above, we introduce the Sequantial Minimal Optimization(SMO) method. ","date":"2016-05-14","objectID":"/2016-05-14-smo/:0:0","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(3)-SMO","uri":"/2016-05-14-smo/"},{"categories":null,"content":"Solve the 2-variable Quadratic Programming Assume from all of the $\\alpha_i$, set only $\\alpha_1, \\alpha_2$ to be variables, all the other $\\alpha_i(i \\neq 1, 2)$ are constant, set $\\alpha_2^{i}$ to be the $i$th iterate of $\\alpha_2$, first we define some things for convenience: $$ L = \\begin{cases} max\\lbrace 0, \\alpha_2^i - \\alpha_1^i \\rbrace, \u0026 y^{(1)} \\neq y^{(2)} \\\\ max\\lbrace 0, \\alpha_2^i + \\alpha_1^i - C \\rbrace, \u0026 y^{(1)} = y^{(2)} \\end{cases} $$ $$ H = \\begin{cases} min \\lbrace C, \\alpha_2^i - \\alpha_1^i + C \\rbrace, \u0026 y^{(1)} \\neq y^{(2)} \\\\ min \\lbrace C, \\alpha_2^i + \\alpha_1^i \\rbrace, \u0026 y^{(1)} = y^{(2)} \\end{cases} $$ $$E_i = g(x^{(i)}) - y^{(i)} = \\left( \\sum\\limits_{j=1}^{N} \\alpha_j y^{(j)} K(x^{(i)}, x^{(j)})+b \\right)- y^{(i)} \\tag{SMO.1}$$ Then using $H$ and $L$ to cut the $\\alpha_2$, in the $(i+1)$th iterate, we have: $$ \\alpha_2^{i+1, unc} = \\alpha_2^{i} + \\frac { y^{(2)}(E_1 - E_2)}{K_{11}+K_{22}-2K_{12}} \\tag{SMO.2}$$ $$ \\alpha_2^{i+1} = \\begin{cases} H, \u0026 \\alpha_2^{i+1, unc} \u003e H \\\\ \\alpha_2^{i+1, unc}, \u0026 L \\le \\alpha_2^{i+1, unc} \\le H \\\\ L, \u0026 \\alpha_2^{i+1, unc} \u003c L \\end{cases} \\tag{SMO.3} $$ $$ \\alpha_1^{i+1} = \\alpha_1^{i} + y^{(1)}y^{(2)}(\\alpha_2^{i} - \\alpha_2^{i+1}) \\tag{SMO.4} $$ ","date":"2016-05-14","objectID":"/2016-05-14-smo/:0:1","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(3)-SMO","uri":"/2016-05-14-smo/"},{"categories":null,"content":"The Choosing of $\\alpha_1, \\alpha_2$ The KKT conditions for each $\\alpha_i$ are: $$ \\begin{align} \\alpha_i = 0 \\quad \u0026 \\Leftrightarrow \\quad y^{(i)} g(x^{(i)}) \\ge 1 \\quad \\text{(Out of the border)} \\\\ 0 \u003c \\alpha_i \u003c C \\quad \u0026 \\Leftrightarrow \\quad y^{(i)} g(x^{(i)}) = 1 \\quad \\text{(On the border)} \\\\ \\alpha_i = C \\quad \u0026 \\Leftrightarrow \\quad y^{(i)} g(x^{(i)}) \\le 1 \\quad \\text{(Inside the border)} \\end{align} \\tag{SMO.5} $$ Select $\\alpha_1$: Iterate through the points on the border(which is most likely to be the support vector), then all the other points, for the points that breaks KKT conditions Select $\\alpha_2$: Find $(x^{(i)}, y^{(i)})$ which gives the largest $\\vert E^i_1 - E^i_2 \\vert$. In order to save time for calculation, save all the $E_i$ in advance ","date":"2016-05-14","objectID":"/2016-05-14-smo/:0:2","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(3)-SMO","uri":"/2016-05-14-smo/"},{"categories":null,"content":"Calculate new $b$ and $E_i$ After Calculate $b$ $$ b_1^{i+1} = b^i -[E_1^i + y^{(1)}K_{11}(\\alpha_1^{i+1} - \\alpha_1^{i}) + y^{(2)}K_{21}(\\alpha_2^{i+1} - \\alpha_2^{i}) ] \\\\ b_2^{i+1} = b^i -[E_2^i + y^{(1)}K_{12}(\\alpha_1^{i+1} - \\alpha_1^{i}) + y^{(2)}K_{22}(\\alpha_2^{i+1} - \\alpha_2^{i}) ] \\tag{SMO.6} $$ $$b^{i+1} = \\begin{cases} b_{1}^{i+1} = b_{2}^{i+1}, \u0026 0 \u003c \\alpha_1^{i+1}, \\alpha_2^{i+1} \u003c C \\\\ \\frac {1}{2} (b_{1}^{i+1} + b_{2}^{i+1}), \u0026 \\alpha_1^{i+1}, \\alpha_2^{i+1} = 0 ; or ; C \\end{cases} $$ 2. Calculate $E_i$ using $\\alpha_1^{i+1}, \\alpha_2^{i+1}, b^{i+1}$ $$ E_i = \\sum\\limits_{j \\in S} \\alpha_j y^{(j)} K(x^{(i)}, x^{(j)})+b^{i+1} - y^{(i)} $$ ","date":"2016-05-14","objectID":"/2016-05-14-smo/:0:3","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(3)-SMO","uri":"/2016-05-14-smo/"},{"categories":null,"content":"SVM之软间隔最大化 Recall from the 1st section, we have: $$ \\begin{align} \u0026 \\min\\limits_{w, \\xi} \\quad \\frac{1}{2} {\\Vert w \\Vert}^2 + C \\sum\\limits_{i=1}^{N}\\xi_i \\\\ s.t. \\quad \u0026 y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) + \\xi_i - 1\\ge 0 \\ \u0026 \\xi_i \\ge 0\\tag{1.10} \\end{align}$$ For the constraint condition and target function in (1.10), using the method of Lagrange multipliers, (1.10) could be represented as: $$ \\begin{align} \u0026 L(w,b,\\xi,\\alpha,\\mu) = \\frac{1}{2} {\\Vert w \\Vert}^2 + C \\sum\\limits_{i=1}^{N}\\xi_i + \\sum\\limits_{i=1}^{N} \\alpha_i (1 -\\xi_i - y^{(i)}(w \\cdot x^{(i)} + b)) + \\sum\\limits_{i=1}^{N} \\mu_i(-\\xi_i) \\tag{3.1} \\end{align}$$ ","date":"2016-05-12","objectID":"/2016-05-12-svm2_2/:0:0","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.2)-Solving dual problem for soft margin maximization","uri":"/2016-05-12-svm2_2/"},{"categories":null,"content":"Solving $\\min\\limits_{w,b} L(w,b,\\xi,\\alpha,\\mu)$ $$ \\nabla_w L(w,b,\\xi,\\alpha,\\mu) = w - \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} x^{(i)} = 0 \\\\ \\nabla_b L(w,b,\\xi,\\alpha,\\mu) = \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0 \\\\ \\nabla_{\\xi_i} L(w,b,\\xi,\\alpha,\\mu) = C-\\alpha_i - \\mu_i = 0 \\tag{3.2}$$ Luckily it has the same form as (2.4) and (2.5), see the next section below. ","date":"2016-05-12","objectID":"/2016-05-12-svm2_2/:0:1","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.2)-Solving dual problem for soft margin maximization","uri":"/2016-05-12-svm2_2/"},{"categories":null,"content":"Solving $\\alpha$ for $\\max\\limits_{\\alpha} \\ \\min\\limits_{w,b,\\xi} L(w,b,\\xi,\\alpha,\\mu) $ The dual problem for soft margin maximization is: $ \\begin{align} \u0026 \\min\\limits_{\\alpha} \\quad \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} (x^{(i)} \\cdot x^{(j)}) - \\sum\\limits_{i=1}^{N} \\alpha_i \\end{align} \\tag{3.2}$ $ s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\quad \\alpha_i \\ge 0, \\quad \\mu_i \\ge 0, \\quad C- \\alpha_i-\\mu_i =0 \\tag{3.3} $ (3.3) could be simplified as: $$ s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\quad 0 \\le \\alpha_i \\le C \\tag{3.4}$$ ","date":"2016-05-12","objectID":"/2016-05-12-svm2_2/:0:2","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.2)-Solving dual problem for soft margin maximization","uri":"/2016-05-12-svm2_2/"},{"categories":null,"content":"Solving $w,b$ Once $\\alpha^{*}$ is solved, we have: $$ w^{*} = \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} x^{(i)} \\\\ b^{*} = y^{(j)} - \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} (x^{(i)} \\cdot x^{(j)}) \\tag{3.5}$$ in which the $(x^{(j)}, y^{(j)})$ satisfies: $ 0 \\le \\alpha_j \\le C $, which contains $ 1 - \\xi_i -y^{(j)}(w^{*} \\cdot x^{(j)} + b^{*})) = 0 $, which means it’s on the classification border ‘belt’. ","date":"2016-05-12","objectID":"/2016-05-12-svm2_2/:0:3","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.2)-Solving dual problem for soft margin maximization","uri":"/2016-05-12-svm2_2/"},{"categories":null,"content":"Hinge loss function Let $\\quad [ 1 - y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) ]_+ = \\xi_i $, (1.10) could also be represented as $$\\min\\limits_{w,b} \\quad \\sum\\limits_{i=1}^N \\xi_i + \\lambda {\\Vert w \\Vert}^2 \\tag{3.6}$$ Let $\\lambda = \\frac{1}{2C}$, then it is equivalent to (1.10), so the loss function could also be written as the hinge loss function: $$ \\sum\\limits_{i=1}^N[ 1 - y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) ]_+ + \\lambda {\\Vert w \\Vert}^2 \\tag{3.7}$$ ","date":"2016-05-12","objectID":"/2016-05-12-svm2_2/:0:4","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.2)-Solving dual problem for soft margin maximization","uri":"/2016-05-12-svm2_2/"},{"categories":null,"content":"SVM之硬间隔最大化 In the last section, (1.8) is a convex quadratic programming problem. Using the method of Lagrange multipliers, (1.8) could be represented as: $$ L(w,b,\\alpha) = \\frac{1}{2} {\\Vert w \\Vert}^2 + \\sum\\limits_{i=1}^{N} \\alpha_i (1-y^{(i)}(w \\cdot x^{(i)} + b)), \\alpha_i \\ge 0 \\tag{2.1}$$ The dual problem is: $$\\max\\limits_{\\alpha} \\ \\min\\limits_{w,b} L(w,b,\\alpha) \\tag{2.2}$$ ","date":"2016-05-12","objectID":"/2016-05-12-svm2_1/:0:0","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.1)-Solving dual problem for hard margin maximization","uri":"/2016-05-12-svm2_1/"},{"categories":null,"content":"Solving $\\min\\limits_{w,b} L(w,b,\\alpha)$ $$ \\nabla_w L(w,b,\\alpha) = w - \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} x^{(i)} = 0 \\\\ \\nabla_b L(w,b,\\alpha) = \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0 \\tag{2.3}$$ Combine (2.3) with (2.1), we have the dual problem: $$ \\begin{align} \u0026 \\max\\limits_{\\alpha} \\left\\lbrace - \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} (x^{(i)} \\cdot x^{(j)}) + \\sum\\limits_{i=1}^{N} \\alpha_i \\right\\rbrace \\\\ \u0026 s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\alpha_i \\ge 0 \\end{align} \\tag{2.4}$$ ","date":"2016-05-12","objectID":"/2016-05-12-svm2_1/:0:1","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.1)-Solving dual problem for hard margin maximization","uri":"/2016-05-12-svm2_1/"},{"categories":null,"content":"Solving $\\alpha$ for $\\max\\limits_{\\alpha} \\ \\min\\limits_{w,b} L(w,b,\\alpha) $ $$ \\begin{align} \u0026 \\min\\limits_{\\alpha} \\left\\lbrace \\frac{1}{2} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{N} \\alpha_i \\alpha_j y^{(i)} y^{(j)} (x^{(i)} \\cdot x^{(j)}) - \\sum\\limits_{i=1}^{N} \\alpha_i \\right\\rbrace \\\\ \u0026 s.t. \\quad \\sum\\limits_{i=1}^{N} \\alpha_i y^{(i)} = 0, \\alpha_i \\ge 0 \\end{align} \\tag{2.5}$$ The solved parameter $\\alpha^{*}$ should satisfy the KKT condition (2.3), (2.4) and $ \\alpha_i^{*} (1-y^{(i)}(w^{*} \\cdot x^{(i)} + b^{*})) = 0 $ ","date":"2016-05-12","objectID":"/2016-05-12-svm2_1/:0:2","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.1)-Solving dual problem for hard margin maximization","uri":"/2016-05-12-svm2_1/"},{"categories":null,"content":"Solving $w,b$ Once $\\alpha^{*}$ is solved, we have: $$ w^{*} = \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} x^{(i)} \\\\ b^{*} = y^{(j)} - \\sum\\limits_{i=1}^{N} \\alpha_i^{*} y^{(i)} (x^{(i)} \\cdot x^{(j)}) \\tag{2.6}$$ in which the $(x^{(j)}, y^{(j)})$ satisfies: $ 1-y^{(j)}(w^{*} \\cdot x^{(j)} + b^{*}) = 0 $, which means it’s on the classification border. Notice that when $\\alpha_i = 0$, $(x^{(i)}, y^{(i)})$ has no contribution in deciding the $w$ and $b$, which means that only a few points on the border decides the SVM with their $\\alpha_i \u003e 0$, and $ 1-y^{(i)}(w^{*} \\cdot x^{(i)} + b^{*}) = 0 $, they are support vectors. ","date":"2016-05-12","objectID":"/2016-05-12-svm2_1/:0:3","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(2.1)-Solving dual problem for hard margin maximization","uri":"/2016-05-12-svm2_1/"},{"categories":null,"content":"支持向量机 ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:0","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Hyperplane Consider a two-class separation problem: $$\\text{Training set: } T = \\left \\lbrace (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) …(x^{(N)}, y^{(N)}) \\right \\rbrace \\\\ \\text{in which }x^{(i)} \\in R^n, y^{(i)} \\in \\lbrace +1, -1 \\rbrace, i=1,2…N$$ Assuming all the samples in the sample space X are linearly separable, we have the hyperplane: $$ w \\cdot x + b = 0 \\text{, in which }w, b \\in R^N$$ ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:1","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Margin Recall that in the hilbert space, point-plane distance is defined as: $$ \\gamma^{(i)} = \\left \\vert \\frac{w}{\\Vert w \\Vert} \\cdot x^{(i)} + \\frac{b}{\\Vert b \\Vert} \\right \\vert \\tag{1.1}$$ Notice that if $x^{(i)}$ is in the same side the normal vector points to, the value would be naturally positive without the absolute function. So if we put the points labelled with +1 in the ‘positive’ space, then $\\gamma^{(i)}$ could also be defined as: $$ \\gamma^{(i)} = y^{(i)} \\left ( \\frac{w}{\\Vert w \\Vert} \\cdot x^{(i)} + \\frac{b}{\\Vert b \\Vert} \\right ) \\tag{1.2}$$ ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:2","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Predicting function In the other hand, if we have already find the best hyperplane $ w^{*} \\cdot x + b^{*} = 0 $ using SVM, given a point $x^{(i)}$, the predicting function would be: $$y^{(i)} = f(x^{(i)}) = sign(w^{*} \\cdot x^{(i)} + b^{*}) \\tag{1.3}$$ ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:3","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Geometric margin For a given hyperplane, geometric margin is defined as the minimum distance from the sample point to the plane: $$ \\gamma = \\min \\limits_{i = 1,2…N} \\quad \\gamma^{(i)} \\tag{1.4}$$ ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:4","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Functional margin Functional margin is defined as: $$ \\hat \\gamma^{(i)} = y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) = {\\Vert w \\Vert} \\gamma^{(i)} \\tag{1.5}$$ It could be considered as the certainty of the classification. We will soon use it to simplify the maximum problem. ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:5","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Hard margin maximization The main purpose of the SVM is to find the hyper plane with the maximum hard margin (geometric margin): $$ \\begin{align} \u0026 \\max \\limits_{w,b} \\quad \\gamma \\\\ \u0026 s.t. \\quad y^{(i)} \\left ( \\frac{w}{\\Vert w \\Vert} \\cdot x^{(i)} + \\frac{b}{\\Vert b \\Vert} \\right ) \\ge \\gamma , \\quad i=1,2,…,N \\tag{1.6} \\end{align}$$ Using functional margin the question could be re-defined as: $$ \\begin{align} \u0026 \\max \\limits_{w,b} \\quad \\frac{\\hat \\gamma}{\\Vert w \\Vert} \\\\ \u0026 s.t. \\quad y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) \\ge \\hat \\gamma , \\quad i=1,2,…,N \\end{align} \\tag{1.7}$$ Notice that for a given hyper plane, $\\Vert w \\Vert$ and $\\Vert b \\Vert$ could vary since $w$ and $b$ could stretch in the same direction arbitrarily without changing the hyper plane. To constrain this, we confine $\\hat\\gamma = 1$(this is an important step to constrain $\\Vert w \\Vert$ into a fixed scale), the problem finally becomes: $$ \\begin{align} \u0026 \\min \\limits_{w,b} \\quad \\frac{1}{2} {\\Vert w \\Vert}^2 \\\\ \u0026 s.t. \\quad y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) - 1 \\ge 0, \\quad i=1,2,…,N \\end{align} \\tag{1.8}$$ $\\min \\limits_{w,b} \\frac{1}{2} {\\Vert w \\Vert}^2$ equals $ \\max \\limits_{w,b} \\frac{1}{\\Vert w \\Vert} $, which is the original problem. ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:6","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Soft margin maximization If the samples is not linearly separable, to solve this problem, set a loose vector $\\xi \\in R^n$ for each sample, then the restriction in (1.8) becomes: $$ y^{(i)} \\left ( w \\cdot x^{(i)} + {b} \\right ) + \\xi_i - 1 \\ge 0 \\tag{1.9}$$ For each sample, pay a cost $\\xi_i$ in the target function: $$ \\frac{1}{2} {\\Vert w \\Vert}^2 + C \\sum\\limits_{i=1}^{N}\\xi_i \\tag{1.10}$$ $C$ is called the penalty parameter, the larger $C$ means a more strict SVM classifier. How to solve the margin maximization problem? (to be continued) ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:7","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Support Vector Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. ","date":"2016-05-11","objectID":"/2016-05-11-svm/:0:8","tags":["machine learning","svm","统计学习方法"],"title":"Support Vector Machine(1) - Hard margin maximization","uri":"/2016-05-11-svm/"},{"categories":null,"content":"Programming Exercise 5 Notification: This is a simplified code example, if you are attempting this class, don’t copy \u0026 submit since it won’t even work… ","date":"2016-02-27","objectID":"/2016-02-27-programming-exercise-5/:0:0","tags":["coursera"],"title":"Machine Learning Notes(13)-Programming Exercise 5","uri":"/2016-02-27-programming-exercise-5/"},{"categories":null,"content":"Regularized linear regression function [J, grad] = linearRegCostFunction(X, y, theta, lambda) % Initialize some useful values m = length(y); % number of training examples n = size(theta,1); % You need to return the following variables correctly J = 0; grad = zeros(size(theta)); % Calculate cost function J = (1/(2*m))*sum((X*theta .- y).^2) + (lambda/(2*m))*(sum(theta.^2)-theta(1)^2); % Calculate the partial derivative grad(1) = ((X')*(X*theta - y)*(1/m))(1); grad(2:n,:) = ((X')*(X*theta - y)*(1/m) + (lambda/m)*theta)(2:n,:); grad = grad(:); end $$ J(\\theta) = \\frac {1}{2m} \\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 + \\frac{\\lambda}{2m}(\\sum\\limits_{j=1}^{n}\\theta_j^2) $$ $$ \\begin{cases} \\frac{\\partial}{\\partial \\theta_0} J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}[(h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}] \u0026 \\text{for j=0} \\\\ \\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}[(h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}] + \\frac{\\lambda}{m}\\theta_j \u0026 \\text{for j\u003e0} \\end{cases} $$ ","date":"2016-02-27","objectID":"/2016-02-27-programming-exercise-5/:1:0","tags":["coursera"],"title":"Machine Learning Notes(13)-Programming Exercise 5","uri":"/2016-02-27-programming-exercise-5/"},{"categories":null,"content":"Learning curves function [error_train, error_val] = ... learningCurve(X, y, Xval, yval, lambda) % Number of training examples m = size(X, 1); n = size(Xval, 1); % You need to return these values correctly error_train = zeros(m, 1); error_val = zeros(m, 1); % Calculate error_train and error_val for each train set for i = 1:m [theta] = trainLinearReg(X(1:i,:), y(1:i), lambda); error_train(i) = (1/(2*i))*sum((X(1:i,:)*theta .- y(1:i)).^2); error_val(i) = (1/(2*n))*sum((Xval*theta .- yval).^2); end; end $$ J_{train}(\\theta) = \\frac {1}{2m} \\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2 $$ ","date":"2016-02-27","objectID":"/2016-02-27-programming-exercise-5/:2:0","tags":["coursera"],"title":"Machine Learning Notes(13)-Programming Exercise 5","uri":"/2016-02-27-programming-exercise-5/"},{"categories":null,"content":"Polynomial regression function [X_poly] = polyFeatures(X, p) % You need to return the following variables correctly. n = numel(X); X_poly = zeros(n, p); for j = 1:p for i = 1:n X_poly(i,j) = X(i)^j; end end; end Given a vector X, return a matrix X_poly where the p-th column of X contains the values of X to the p-th power ","date":"2016-02-27","objectID":"/2016-02-27-programming-exercise-5/:3:0","tags":["coursera"],"title":"Machine Learning Notes(13)-Programming Exercise 5","uri":"/2016-02-27-programming-exercise-5/"},{"categories":null,"content":"Learning Polynomial Regression function [lambda_vec, error_train, error_val] = ... validationCurve(X, y, Xval, yval) % Selected values of lambda (you should not change this) lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]'; m = size(X, 1); n = size(Xval, 1); % You need to return these variables correctly. error_train = zeros(length(lambda_vec), 1); error_val = zeros(length(lambda_vec), 1); for i = 1:length(lambda_vec) lambda = lambda_vec(i); [theta] = trainLinearReg(X, y, lambda); error_train(i) = (1/(2*m))*sum((X*theta .- y).^2); error_val(i) = (1/(2*n))*sum((Xval*theta .- yval).^2); end; end This function returns the train and validation errors (in error_train, error_val) for different values of lambda, same as linearRegCostFunction() ","date":"2016-02-27","objectID":"/2016-02-27-programming-exercise-5/:4:0","tags":["coursera"],"title":"Machine Learning Notes(13)-Programming Exercise 5","uri":"/2016-02-27-programming-exercise-5/"},{"categories":null,"content":"Advice for Applying Machine Learning ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:0:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Debugging a learning algorithm Get more training data Try smaller sets of features Getting additional features Changing fit hypothesis Changing $\\lambda$ ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:1:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Evaluate hypothesis Training/testing procedure Splitting data into training set and test set Learn parameters from training data Compute test error If $J_{train}(\\theta)$ is low while $J_{test}(\\theta)$ is high, then it might be overfitting. For logistic regression Misclassification error: $$ err(h_\\theta (x), y) = \\begin{cases} 1, \u0026 [h_\\theta (x)] \\text{ XOR } y = 0 \\\\ 0, \u0026 [h_\\theta (x)] \\text{ XOR } y = 1 \\\\ \\end{cases} \\\\ TestError = \\frac {1}{m_{test}} \\sum\\limits_{i=1}^{m_{test}} err(h_\\theta (x), y) $$ It’s an alternative option other than cost function ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:2:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Model Selection Set d = degree of polynomial as the extra parameter other than $\\theta$ Try different models, calculate cost function on training set, choose the d with the minimum cost function. Problem: May fit training set well but not generalize to all data. Solution: Split training data into 3 pieces: training set(60%), cross validation set(20%), test set(20%) Using validation set to choose the model: Min(Training error) Min(Validation error), Pick Test error shows how well the model generalizes ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:3:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Bias vs. Variance Diagnosing Bias vs. Variance Cross validation error and training error with different degree of polynomial d The left side indicates high bias while the right side indicates high variance ‘Error - training set size graph’ affected by different hypothesis The error with different training set size when the hypothesis is of high bias The error with different training set size when the hypothesis is of high variance ‘Error - training set size graph’ affected by different regularzation parameter Debugging a learning algorithm ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:4:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Building a Spam Classifier Do a dirty and quick example in order to examine whether a method like stemming might help lowering the error ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:5:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Handling Skewed Data Skewed data: A set of very unbalanced data Precision/recall High classfier threshold: High precision, low recall: Very confident, lots of overlook Low classfier threshold: High recall, low precision: Not very confident, but less overlook Choose threshold automatically F score is better than average on deciding which algo to use based on precision/recall data ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:6:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Using Large Data Sets Generally more training data will improve learning performance Having a significantly large set of data will improve learning performance: Low training error -\u003e training error ≈ test error -\u003e all errors are low ","date":"2016-02-27","objectID":"/2016-02-27-advice-for-applying-machine-learning/:7:0","tags":["machine learning","coursera"],"title":"Machine Learning Notes(12)-Advice for Applying Machine Learning","uri":"/2016-02-27-advice-for-applying-machine-learning/"},{"categories":null,"content":"Programming Exercise 4 Notification: This is a simplified code example, if you are attempting this class, don’t copy \u0026 submit since it won’t even work… ","date":"2016-02-20","objectID":"/2016-02-20-programming-exercise-4/:0:0","tags":["coursera"],"title":"Machine Learning Notes(11)-Programming Exercise 4","uri":"/2016-02-20-programming-exercise-4/"},{"categories":null,"content":"Cost function for fmincg function [J grad] = nnCostFunction(nn_params, ... input_layer_size, ... hidden_layer_size, ... num_labels, ... X, y, lambda) % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices % for our 2 layer neural network Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1)); Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1)); % Setup some useful variables m = size(X, 1); % You need to return the following variables correctly J = 0; Theta1_grad = zeros(size(Theta1)); Theta2_grad = zeros(size(Theta2)); %% --------------Feedforward propagation--------------- for i = 1:m a_1 = [1 X(i,:)]'; a_2 = [1; sigmoid(Theta1*a_1)]; a_3 = sigmoid(Theta2*a_2); % Transform y to standard nn outpout y_vector = zeros(num_labels,1); y_vector(y(i)) = 1; % Calculate costfunction J = J + (1/m)*sum(-y_vector .* log(a_3) - (1 .- y_vector) .* log(1 .- a_3)); end; % Calculate Regularzation Square1 = Theta1.^2; Square2 = Theta2.^2; reg_sum = sum(sum(Square1)) - sum(Square1(:,1)) + sum(sum(Square2)) - sum(Square2(:,1)); J = J + (lambda/(2*m))*reg_sum; %% ----------------Calculate Gradient----------------- for i = 1:m a_1 = [1 X(i,:)]'; a_2 = [1; sigmoid(Theta1*a_1)]; a_3 = sigmoid(Theta2*a_2); % Transform y to standard nn outpout y_vector = zeros(num_labels,1); y_vector(y(i)) = 1; % Backpropagation sigma_3 = a_3 - y_vector; sigma_2 = Theta2'*sigma_3 .* a_2 .* (1 .- a_2); % Vectorized partial derivative matrix Theta1_grad = Theta1_grad + sigma_2(2:end)*a_1'; Theta2_grad = Theta2_grad + sigma_3*a_2'; end; % Calculate regulazation terms Theta1_grad = (1/m)*Theta1_grad .+ (lambda/m)*[zeros(size(Theta1,1),1),Theta1(:,2:end)]; Theta2_grad = (1/m)*Theta2_grad .+ (lambda/m)*[zeros(size(Theta2,1),1),Theta2(:,2:end)]; % ------------------------------------------------------------- % Unroll gradients grad = [Theta1_grad(:) ; Theta2_grad(:)]; end ","date":"2016-02-20","objectID":"/2016-02-20-programming-exercise-4/:1:0","tags":["coursera"],"title":"Machine Learning Notes(11)-Programming Exercise 4","uri":"/2016-02-20-programming-exercise-4/"},{"categories":null,"content":"Main function %% Initialization clear ; close all; clc %% Setup the parameters you will use for this exercise input_layer_size = 400; % 20x20 Input Images of Digits hidden_layer_size = 25; % 25 hidden units num_labels = 10; % 10 labels, from 1 to 10 % (note that we have mapped \"0\" to label 10) %% =========== Part 1: Loading and Visualizing Data ============= load('ex4data1.mat'); m = size(X, 1); %% ================ Part 2: Loading Parameters ================ % Load the weights into variables Theta1 and Theta2 load('ex4weights.mat'); % Unroll parameters nn_params = [Theta1(:) ; Theta2(:)]; %% ================ Part 3: Initializing Pameters ================ initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size); initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels); % Unroll parameters initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)]; %% =============== Part 4: Implement Backpropagation =============== % value to see how more training helps. options = optimset('MaxIter', 50); % You should also try different values of lambda lambda = 1; % Create \"short hand\" for the cost function to be minimized costFunction = @(p) nnCostFunction(p, ... input_layer_size, ... hidden_layer_size, ... num_labels, X, y, lambda); % Now, costFunction is a function that takes in only one argument (the % neural network parameters) [nn_params, cost] = fmincg(costFunction, initial_nn_params, options); % Obtain Theta1 and Theta2 back from nn_params Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1)); Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1)); %% ================= Part 5: Implement Predict ================= pred = predict(Theta1, Theta2, X); fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100); ","date":"2016-02-20","objectID":"/2016-02-20-programming-exercise-4/:2:0","tags":["coursera"],"title":"Machine Learning Notes(11)-Programming Exercise 4","uri":"/2016-02-20-programming-exercise-4/"},{"categories":null,"content":"Neural Networks Learning ","date":"2016-02-20","objectID":"/2016-02-20-neural-networks-learning/:0:0","tags":["machine learning","neural network","coursera"],"title":"Machine Learning Notes(10)-Neural Networks Learning","uri":"/2016-02-20-neural-networks-learning/"},{"categories":null,"content":"Cost Fuction 1. For a neural network like the pic above, the cost function should be like: $$ J(\\theta) = \\frac {1}{m} \\sum\\limits\\_{i=1}^{m}\\sum\\limits\\_{k=1}^{K} [-y\\_k^{(i)} log(h\\_\\theta (x^{(i)})\\_k) - (1 - y\\_k^{(i)}) log(1 - h\\_\\theta (x^{(i)})\\_k)] $$ 2. In which there are m training data and the output layer has K units. The cost function will sum all the outputs from all the data. 3. Similarly, the regularization term just sum up the square of all the parameters from all the layers: $$ \\frac {\\lambda}{2m} \\sum\\limits\\_{j=1}^{s^{(l)}} \\sum\\limits\\_{k=2}^{s^{(l+1)}} \\sum\\limits\\_{l=1}^{n} [(\\Theta\\_{jk}^{(l)})^2] $$ 4. Note that you should not be regularizing the terms that correspond to the bias. For the matrices here, this corresponds to the first column of each matrix, so k should start from index 2. ","date":"2016-02-20","objectID":"/2016-02-20-neural-networks-learning/:1:0","tags":["machine learning","neural network","coursera"],"title":"Machine Learning Notes(10)-Neural Networks Learning","uri":"/2016-02-20-neural-networks-learning/"},{"categories":null,"content":"Backpropagation Algorithm 1. In order to use optimization algorithms like gradient descent, we must calculate the derivative of the cost function. First we should calculate $\\delta$ for each layer using backpropagation algorithm. From $layer\\_l$ to $layer\\_{l+1}$, we have: $$ \\delta^{(output)} = a^{(output)} - y \\\\\\ \\delta^{(l)} = [\\Theta^{(l)}]^T \\delta^{(l+1)} .* g'(z^{(l)}) $$ 2. For sigmoid function, $g'(z) = g(z)(1-g(z))$ 3. Then we have the derivative: $$ \\frac {\\partial}{\\partial \\Theta\\_{ij}^{(l)}} J(\\Theta) = a\\_j^{(l)} \\delta\\_i^{(l+1)} \\\\\\ \\Delta^{(l)} = \\delta^{(l+1)} (a^{(l)})^T $$ 4. Instead of calculate the $\\Delta$ for all the training data, we could sum up all the inputs independantly, like: $$ \\begin{align} \u0026 \\text{From i = 1 to m, do:} \\\\\\ \u0026\u0026 \\Delta^{(l)} += \\delta^{(l+1)} (a^{(l)})^T \\\\\\ \u0026 \\text{Then we have:} \\\\\\ \u0026\u0026 \\frac {\\partial}{\\partial \\Theta\\_{ij}^{(l)}} J(\\Theta) = \\frac {1}{m} \\Delta\\_{ij}^{(l)} \\end{align} $$ - (This step is the main step for mapreduce, see Large Scale Machine Learning) 5. Then add regularzation term for partial derivative, we have: $$ \\frac {\\partial}{\\partial \\Theta\\_{ij}^{(l)}} J(\\Theta) = \\frac {1}{m} \\Delta\\_{ij}^{(l)} + \\frac {\\lambda}{m} \\Theta\\_{ij}^{(l)} $$ 6. Finally, using $J(\\Theta)$ and $\\frac {\\partial}{\\partial \\Theta\\_{ij}^{(l)}} J(\\Theta)$, we could minimize the cost function using gradient descent as well as other optimization algorithms. ","date":"2016-02-20","objectID":"/2016-02-20-neural-networks-learning/:2:0","tags":["machine learning","neural network","coursera"],"title":"Machine Learning Notes(10)-Neural Networks Learning","uri":"/2016-02-20-neural-networks-learning/"},{"categories":null,"content":"Optional Section: How Backpropagation Works Using the same model as the last section: $$ \\begin{align} \\frac {\\partial J}{\\partial \\Theta_{ij}^{(1)}} \u0026 = \\frac {\\partial J}{\\partial a_i^{(2)}} \\frac {\\partial a_i^{(2)}}{\\partial z_i^{(2)}} \\frac {\\partial z_i^{(2)}}{\\partial \\Theta_{ij}^{(1)}} \\\\ \u0026 = \\frac {\\partial J}{\\partial a_i^{(2)}} g’(z_i^{(2)}) a_i^{(1)} \\end{align} $$ Then taking the total derivative with respect to $z^{(3)}$, a recursive expression for the derivative is obtained: $$ \\begin{align} \\frac {\\partial J}{\\partial a_i^{(2)}} \u0026 = \\sum\\limits_{m=1}^{s_3} \\frac {\\partial J}{\\partial z_m^{(3)}} \\frac {\\partial z_m^{(3)}}{\\partial a_i^{(2)}} \\\\ \u0026 = \\sum\\limits_{m=1}^{s_3} \\frac {\\partial J}{\\partial a_m^{(3)}} \\frac {\\partial a_m^{(3)}}{\\partial z_m^{(3)}} \\frac {\\partial z_m^{(3)}}{\\partial a_i^{(2)}} \\\\ \u0026 = \\sum\\limits_{m=1}^{s_3} \\frac {\\partial J}{\\partial a_m^{(3)}} g’(z_m^{(3)}) \\Theta_{mi}^{(2)} \\end{align} $$ Let $ \\delta_i^{(2)} = \\frac {\\partial J}{\\partial a_i^{(2)}} g’(z_i^{(2)}) $, then from the result of the first formula We have: $ \\frac {\\partial J}{\\partial \\Theta_{ij}^{(1)}} = \\delta_i^{(2)} a_i^{(1)} $, in which recursively, $ \\delta_i^{(2)} = (\\sum\\limits_{m=1}^{s_3} \\delta_m^{(3)}\\Theta_{mi}^{(2)}) g’(z_i^{(2)}) $ Referance: http://www.wikiwand.com/en/Backpropagation#Derivation ","date":"2016-02-20","objectID":"/2016-02-20-neural-networks-learning/:3:0","tags":["machine learning","neural network","coursera"],"title":"Machine Learning Notes(10)-Neural Networks Learning","uri":"/2016-02-20-neural-networks-learning/"},{"categories":null,"content":"Neural Networks Model A single neuron model: logistic unit Takes 3+1 inputs(the extra input called bias is just like $\\theta_0$ in logistic regression, not shown in picture). Both input and output could be represented as vectors, in which each unit has its own parameters $\\theta$ All the units in the same layer take the same input $\\mathbf{x}$, as the pic shows. Each unit has only one output: $sigmoid(\\theta^T x)$. Of course there’re other choices for sigmoid function. Neural Networks A neural network consists of an input Layer, an output layer and hidden layers. In the model above, layer 1 is input layer, 2 is hidden layer and 3 is the output layer. ","date":"2016-02-11","objectID":"/2016-02-11-neural-networks-representation/:1:0","tags":["machine learning","neural network","coursera"],"title":"Machine Learning Notes(8)-Neural Networks Representation","uri":"/2016-02-11-neural-networks-representation/"},{"categories":null,"content":"Calculation from one layer to the next In the picture above, we have the networks from layer j to layer j+1, in which layer j has 3(+1) units while layer j+1 has 3 layers. Let $s_j = 3$, $s_{j+1} = 3$ $\\alpha^{(j)}$ : Output of the $j_{th}$ layer. $s_j + 1$ dimension vector. $\\theta_i^{(j)}$ : Parameters in the $i_{th}$ unit of ${(j+1)}_{th}$ layer. $s_j + 1$ dimension vector. $\\mathbf{\\theta^{(j)}} = \\begin{bmatrix} \\theta_1^{(j)} \u0026 \\theta_2^{(j)} \u0026 \\cdots \u0026 \\theta_{s_(j + 1)}^{(j)} \\end{bmatrix}^T$ : All the network parameters from $j_{th}$ layer to ${(j+1)}_{th}$ layer. We have: $\\alpha^{(j+1)} = sigmoid(\\mathbf{\\theta^{(j)}}\\alpha^{(j)})$ add $\\alpha_0^{(j+1)}$ ","date":"2016-02-11","objectID":"/2016-02-11-neural-networks-representation/:2:0","tags":["machine learning","neural network","coursera"],"title":"Machine Learning Notes(8)-Neural Networks Representation","uri":"/2016-02-11-neural-networks-representation/"},{"categories":null,"content":"Programming Exercise 2 Notification: This is a simplified code example, if you are attempting this class, don’t copy \u0026 submit since it won’t even work… ","date":"2016-02-09","objectID":"/2016-02-09-programming-exercise-2/:0:0","tags":["coursera"],"title":"Machine Learning Notes(7)-Programming Exercise 2","uri":"/2016-02-09-programming-exercise-2/"},{"categories":null,"content":"Plot function function plotData(X, y) figure; hold on; pos = find(y==1); neg = find(y == 0); plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, ... 'MarkerSize', 7); plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', ... 'MarkerSize', 7); hold off; end ","date":"2016-02-09","objectID":"/2016-02-09-programming-exercise-2/:1:0","tags":["coursera"],"title":"Machine Learning Notes(7)-Programming Exercise 2","uri":"/2016-02-09-programming-exercise-2/"},{"categories":null,"content":"Sigmiod function function g = sigmoid(z) g = zeros(size(z)); % Compute the sigmoid of each value of z (z can be a matrix, % vector or scalar). for row_iter = 1:size(z,1) for col_iter = 1:size(z,2) g(row_iter, col_iter) = 1/(1+e^(-z(row_iter,col_iter))); end; end end ","date":"2016-02-09","objectID":"/2016-02-09-programming-exercise-2/:2:0","tags":["coursera"],"title":"Machine Learning Notes(7)-Programming Exercise 2","uri":"/2016-02-09-programming-exercise-2/"},{"categories":null,"content":"Cost function \u0026 gradient function [J, grad] = costFunction(theta, X, y) m = length(y); J = (1/m)*sum(-y .* log(sigmoid(X*theta)) - (ones(m,1) - y) .* log(ones(m,1) - sigmoid(X*theta))); grad = (X')*(sigmoid(X*theta) - y)*(1/m); How gradient works: $$ \\begin{aligned} (\\mathbf{X}^T) * (\\mathbf{X} \\mathbf{\\theta} - \\mathbf{y}) \u0026 = \\begin{bmatrix}\\mathbf{x_1} \\\\ \\mathbf{x_2} \\\\ \\cdots \\\\ \\mathbf{x_m} \\end{bmatrix} * \\begin{bmatrix}h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)} \\\\ h_\\theta(\\mathbf{x}^{(2)}) - y^{(2)} \\\\ \\vdots \\\\ h_\\theta(\\mathbf{x}^{(m)}) - y^{(m)} \\end{bmatrix} \\\\ \u0026 = \\begin{bmatrix} \\sum\\limits_{i=1}^{m}((h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) x_1^{(i)}) \\\\ \\sum\\limits_{i=1}^{m}((h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) x_2^{(i)}) \\\\ \\cdots \\\\ \\sum\\limits_{i=1}^{m}((h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) x_i^{(i)}) \\end{bmatrix} \\end{aligned} $$ ","date":"2016-02-09","objectID":"/2016-02-09-programming-exercise-2/:3:0","tags":["coursera"],"title":"Machine Learning Notes(7)-Programming Exercise 2","uri":"/2016-02-09-programming-exercise-2/"},{"categories":null,"content":"Cost function for regularzation function [J, grad] = costFunctionReg(theta, X, y, lambda) m = length(y); n = size(theta,1); J = (1/m)*sum(-y .* log(sigmoid(X*theta)) - (1 .- y) .* log(1 .- sigmoid(X*theta))) + (lambda/(2*m))*(theta'(2:n) * theta(2:n)); grad(1) = ((X')*(sigmoid(X*theta) - y)*(1/m))(1); grad(2:n,:) = ((X')*(sigmoid(X*theta) - y)*(1/m) + (lambda/m)*theta)(2:n,:); end Notice that the only difference from last section is the penalize term Notice we have to reinitialize $\\theta_0$ both in J and grad ","date":"2016-02-09","objectID":"/2016-02-09-programming-exercise-2/:4:0","tags":["coursera"],"title":"Machine Learning Notes(7)-Programming Exercise 2","uri":"/2016-02-09-programming-exercise-2/"},{"categories":null,"content":"Prediction using the results function p = predict(theta, X) m = size(X, 1); % Number of training examples p = sigmoid(X*theta); for iter = 1:m if (p(iter) \u003e= 0.5) p(iter) = 1; else p(iter) = 0; end; end; end Recall what X*theta is in the last section. ","date":"2016-02-09","objectID":"/2016-02-09-programming-exercise-2/:5:0","tags":["coursera"],"title":"Machine Learning Notes(7)-Programming Exercise 2","uri":"/2016-02-09-programming-exercise-2/"},{"categories":null,"content":"Programming Exercise 3 ","date":"2016-02-09","objectID":"/2016-02-13-programming-exercise-3/:0:0","tags":["coursera"],"title":"Machine Learning Notes(9)-Programming Exercise 3","uri":"/2016-02-13-programming-exercise-3/"},{"categories":null,"content":"Multi-class Classification CostFunction for fmincg function [J, grad] = lrCostFunction(theta, X, y, lambda) % Initialize some useful values m = length(y); % number of training examples n = size(theta,1); J = (1/m)*sum(-y .* log(sigmoid(X*theta)) - (1 .- y) .* log(1 .- sigmoid(X*theta))) + (lambda/(2*m))*(theta'(2:n) * theta(2:n)); grad(1) = ((X')*(sigmoid(X*theta) - y)*(1/m))(1); grad(2:n,:) = ((X')*(sigmoid(X*theta) - y)*(1/m) + (lambda/m)*theta)(2:n,:); end The same as is in Programming Exercise 2 One-vs-all Classification function [all_theta] = oneVsAll(X, y, num_labels, lambda) % Some useful variables m = size(X, 1); n = size(X, 2); % Initialize X and all_theta X = [ones(m, 1) X]; all_theta = zeros(num_labels, n + 1); % Initialize options for fmincg initial_theta = zeros(n + 1, 1); options = optimset('GradObj', 'on', 'MaxIter', 50); % Generate rows of all_theta row by row using fmincg for iter = 1:num_labels all_theta(iter,:) = (fmincg (@(t)(lrCostFunction(t, X, (y == iter), lambda)), initial_theta, options))'; end; end ONEVSALL trains multiple logistic regression classifiers and returns all the classifiers in a matrix all_theta, where the i-th row of all_theta corresponds to the classifier for label i The parameter y == iter returns a vector of the same size as y with ones at positions where the elements of y are equal to iter and zeroes where they are different. One-vs-all Prediction function p = predictOneVsAll(all_theta, X) m = size(X, 1); num_labels = size(all_theta, 1); % You need to return the following variables correctly p = zeros(size(X, 1), 1); % Add ones to the X data matrix X = [ones(m, 1) X]; [value,p] = max(X*all_theta', [], 2); p = p'; end Notice that the max element’s column-index in each row of X*all_theta' happensto be the number it represents(10 for 0) Main Function %% Initialization clear ; close all; clc %% =========== Part 1: Loading Data ============= load('ex3data1.mat'); load('ex3data1.mat'); m = size(X, 1); %% ============ Part 2: Vectorize Logistic Regression ============ lambda = 0.1; [all_theta] = oneVsAll(X, y, num_labels, lambda); %% ================ Part 3: Predict for One-Vs-All =============== pred = predictOneVsAll(all_theta, X); fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100); end ","date":"2016-02-09","objectID":"/2016-02-13-programming-exercise-3/:1:0","tags":["coursera"],"title":"Machine Learning Notes(9)-Programming Exercise 3","uri":"/2016-02-13-programming-exercise-3/"},{"categories":null,"content":"Neural Networks Feedforward Propagation and Prediction function p = predict(Theta1, Theta2, X) % Useful values m = size(X, 1); num_labels = size(Theta2, 1); % Add ones to the X data matrix X = [ones(m, 1) X]; % Feedforward Propagation a_2 = [ones(1, m);sigmoid(Theta1*X')]; a_3 = sigmoid(Theta2*a_2); [values, p] = max(a_3); p = p'; end The parameters of each layer has already been trained into Theta1 and Theta2 Each column of the output layer a_3 is the output for each row of X, which is each row-vectorized picture. The prediction part, especially [values, p] = max(a_3) works the same as One-vs-all Prediction function, except for the row-index rather than column-index. Main Function %% =========== Part 1: Loading Data ============= load('ex3data1.mat'); m = size(X, 1); %% =========== Part 2: Loading Pameters ========= load('ex3weights.mat'); %% =========== Part 3: Implement Predict ======== pred = predict(Theta1, Theta2, X); fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100); end ","date":"2016-02-09","objectID":"/2016-02-13-programming-exercise-3/:2:0","tags":["coursera"],"title":"Machine Learning Notes(9)-Programming Exercise 3","uri":"/2016-02-13-programming-exercise-3/"},{"categories":null,"content":"Logistic Regression ","date":"2016-02-06","objectID":"/2016-02-06-logistic-regression/:0:0","tags":["machine learning","logistic regression","coursera"],"title":"Machine Learning Notes(6)-Logistic Regression","uri":"/2016-02-06-logistic-regression/"},{"categories":null,"content":"Linear Regression is not suitable for classification problem overfit $h_\\theta$ can be \u003e1 or \u003c0 ","date":"2016-02-06","objectID":"/2016-02-06-logistic-regression/:1:0","tags":["machine learning","logistic regression","coursera"],"title":"Machine Learning Notes(6)-Logistic Regression","uri":"/2016-02-06-logistic-regression/"},{"categories":null,"content":"Logisitc Regression Hypothesis Representation Sigmoid/Logistic Function for linear regression: $$h_\\theta (x) = \\frac {1}{1 + e^{-\\theta^T x}}$$ Interpretation: estimated probability that y=1 on input x $P(y=0|x=\\theta) + P(y=1|x=\\theta) = 1$ Decision Boundary Linear Regression: y=1 equals to $\\theta^T x$ \u003e 0 which is decided by parameters Unlinear Regression: Polynomial Regression etc. Cost function for one variable hypothesis To let the cost function be convex for gradient descent, it should be like this: $$Cost(h_\\theta (x), y) = \\begin{cases} -log(h_\\theta (x)), \u0026 (y = 1) \\\\ -log(1 - h_\\theta (x)), \u0026 (y = 0) \\\\ \\end{cases}$$ Simplified Cost Function $Cost(h_\\theta (x), y) = y log(h_\\theta (x)) + (1 - y)log(1 - h_\\theta (x))$ For a set of training data, we have: $$ J(\\theta) = -\\frac{1}{m}\\sum\\limits_{i=1}^{m}[y^{(i)}\\log h_\\theta(x^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))] $$ Minimize $J(\\theta)$ using gradient descent $$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}[(h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}]$$ $$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}[(h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}]$$ Advanced Optimization Other optimization algorithms other than gradient descent (do not need to manually pick $\\alpha$): Conjugate descent BGFS L-BFGS Multiclass Classification: One-vs-all For more than 2 features of y, do logisitc regression for each feature separately ","date":"2016-02-06","objectID":"/2016-02-06-logistic-regression/:2:0","tags":["machine learning","logistic regression","coursera"],"title":"Machine Learning Notes(6)-Logistic Regression","uri":"/2016-02-06-logistic-regression/"},{"categories":null,"content":"Regularzation The problem of overfitting Addressing overfitting Reduce number of features (manually/model selection) Regularzation: add panalize terms to some less important parameters, or panalize all the parameters(except for $\\theta_0$) Regularized Linear Regression $$\\displaystyle \\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j$$ For normal equation, regularization would also make $X^T X$ invertible Regularized Logistic Regression Literally the same as Regularized Linear Regression except for the form of $h_\\theta (x)$ ","date":"2016-02-06","objectID":"/2016-02-06-logistic-regression/:3:0","tags":["machine learning","logistic regression","coursera"],"title":"Machine Learning Notes(6)-Logistic Regression","uri":"/2016-02-06-logistic-regression/"},{"categories":null,"content":"Programming Exercise 1: Gradient Descent for Linear regression ","date":"2016-02-01","objectID":"/2016-02-01-programming-exercise-1/:0:0","tags":["coursera"],"title":"Machine Learning Notes(5)-Programming Exercise 1","uri":"/2016-02-01-programming-exercise-1/"},{"categories":null,"content":"Gradient Descent for Linear regression Notification: This is a simplified code example, if you are attempting this class, don’t copy \u0026 submit since it won’t even work… ","date":"2016-02-01","objectID":"/2016-02-01-programming-exercise-1/:1:0","tags":["coursera"],"title":"Machine Learning Notes(5)-Programming Exercise 1","uri":"/2016-02-01-programming-exercise-1/"},{"categories":null,"content":"Step 1 - Load \u0026 Initialize Data data = load('ex1data1.txt'); X = data(:, 1); y = data(:, 2); X = [ones(m, 1), data(:,1)]; theta = zeros(2, 1); iterations = 1500; alpha = 0.01; ","date":"2016-02-01","objectID":"/2016-02-01-programming-exercise-1/:2:0","tags":["coursera"],"title":"Machine Learning Notes(5)-Programming Exercise 1","uri":"/2016-02-01-programming-exercise-1/"},{"categories":null,"content":"Step 2 - Feature Normalize This is done before adding a column to X, which representing $x_0$ function [X_norm, mu, sigma] = featureNormalize(X) X_norm = X; mu = zeros(1, size(X, 2)); sigma = zeros(1, size(X, 2)); mu = mean(X); sigma = std(X); X_norm = (X .- mu)./sigma end ","date":"2016-02-01","objectID":"/2016-02-01-programming-exercise-1/:3:0","tags":["coursera"],"title":"Machine Learning Notes(5)-Programming Exercise 1","uri":"/2016-02-01-programming-exercise-1/"},{"categories":null,"content":"Step 3 - Gradient Descent function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters) m = length(y); for iter = 1:num_iters summary = X' * (X*theta .- y); theta = theta .- alpha * (1/m) * summary; end end How gradient works: $$ \\mathbf{X} \\mathbf{\\theta} - \\mathbf{y} = \\begin{bmatrix}\\mathbf{x_1} \u0026 \\mathbf{x_2} \u0026 \\cdots \u0026 \\mathbf{x_m} \\end{bmatrix} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} - \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\begin{bmatrix}h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)} \\\\ h_\\theta(\\mathbf{x}^{(2)}) - y^{(2)} \\\\ \\vdots \\\\ h_\\theta(\\mathbf{x}^{(m)}) - y^{(m)} \\end{bmatrix}$$ $$ \\begin{align} (\\mathbf{X}^T) * (\\mathbf{X} \\mathbf{\\theta} - \\mathbf{y}) \\\\ \u0026 = \\begin{bmatrix}\\mathbf{x_1} \\\\ \\mathbf{x_2} \\\\ \\cdots \\\\ \\mathbf{x_m} \\end{bmatrix} * \\begin{bmatrix}h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)} \\\\ h_\\theta(\\mathbf{x}^{(2)}) - y^{(2)} \\\\ \\vdots \\\\ h_\\theta(\\mathbf{x}^{(m)}) - y^{(m)} \\end{bmatrix} \u0026 = \\begin{bmatrix} \\sum\\limits_{i=1}^{m}((h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) x_1^{(i)}) \\\\ \\sum\\limits_{i=1}^{m}((h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) x_2^{(i)}) \\\\ \\cdots \\\\ \\sum\\limits_{i=1}^{m}((h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) x_i^{(i)}) \\end{bmatrix} \\end{align} $$ ","date":"2016-02-01","objectID":"/2016-02-01-programming-exercise-1/:4:0","tags":["coursera"],"title":"Machine Learning Notes(5)-Programming Exercise 1","uri":"/2016-02-01-programming-exercise-1/"},{"categories":null,"content":"Too lazy to explain some of the commands…orz ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:0:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Basic PS1('sign') Change prompt to ‘sign’ Matrix Assignment v = [1 2; 3 4; 5 6] v = [1.1 1.2 1.3] = [1:\u003c0.1:\u003e1.3] (default step is 1) Matrix Generation commands ones(2,3) = [1 1 1; 1 1 1] (ones/zeros/rand/randn) eye(n): generate n by n identical matrix magic(n): generate n by n magic matrix size(M, n): return the size of the n=1:row n=2:coloum length(M): max dimention of M ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:1:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Moving data around pwd: show current path load('xxx') load \u003cfilename\u003e who: display current scope. whos: details clear \u003cvariable\u003e: clear the space of the variable M(start:end): count from start to end coloum by coloum save \u003cfilename\u003e \u003cvariable\u003e \u003cencode\u003e: save variable to the file using enconde like ascii ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:2:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Manipulate M(x,y): indexing M(r1:r2,c1:c2) sub-matrix block: coloumn1-2/row1-2. M(rn, :) = [a b c]: assign [a b c] to row n of M M([r1 r2], :): copy row1 and row2 M(M, [a b c]): append [a b c] to M M(:): put all elements of M into a single vector M = [X Y] = [X,Y]: concatenating X \u0026 Y into M(horizontally) M = [X;Y]: concatenating X \u0026 Y into M (vertically) ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:3:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Computing on data M': transpose pinv(M): inverse of M Element-wise calculation A*b, A .* B: element-wise multiply, as well as .^, ./, log(v), exp(v), abs(v) and so on M \u003c a: element-wise comparison, which returns a boolean matrix max() [value, index] = max(M) max(M,N): M and N are of the same size, do element-wise comparison, then generate a new matrix max(M) = max(M,[],1): 1-\u003ecoloum-wise; 2-\u003eper row-wise. [r, c] = find(M \u003c a): returns the index of the elements sum() sum(M): summarize all the elements sum(M,1): summarize each coloum sum(M,2): summarize each row prod(M) = prod(M,1): multiply each coloum. 1-\u003ecoloum-wise; 2-\u003eper row-wise. floor(M): rounds down ceil(M): rounds up flipud(M): flip upside-down ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:4:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Plotting data hist(v, n): plot v with n buckets histogram plot(x, y, \u003ccolor\u003e) plot(x,y1) hold; plot(x,y2) xlabel('X') ylabel('Y') legend('y1','y2') title('title') print -dpng 'filename.png' close: close figure window figure \u003cnumber\u003e; plot(x,y1);figure \u003cnumber\u003e; plot(x,y2); … subplot(x,y,index): access the index axis([xmin xmax ymin ymax]) clf: clear all figure imagesc(M), colorbar, colormap gray; ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:5:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Control statements for i=1:10, v(i) = 2^i; end; break continue while i\u003c=5, v(i)=100; i=i+1; end; if i==6, \u003cstat\u003e; elseif i==7, \u003cstat2\u003e; else \u003cstat3\u003e; end; ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:6:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Functions \u003cfunctionname.m\u003e: format of the function file Example: function [y1, y2] = squareAndCubeThisNumber(x) y1 = x^2; y2 = x^3; addpath(‘path’) ","date":"2016-02-01","objectID":"/2016-02-01-octave-abc/:7:0","tags":["coursera"],"title":"Machine Learning Notes(4)-Octave abc","uri":"/2016-02-01-octave-abc/"},{"categories":null,"content":"Multivariate Linear Regression ","date":"2016-01-30","objectID":"/2016-01-30-multivariate-linear-regression/:0:0","tags":["machine learning","linear regression","coursera"],"title":"Machine Learning Notes(3)-Multivariate Linear Regression","uri":"/2016-01-30-multivariate-linear-regression/"},{"categories":null,"content":"Gradient Descent for Multiple Variables Suppose we have n variables, set hypothesis to be: $$h_\\theta(\\mathbf{x}) = \\sum_{i=0}^n \\theta_i x_i = \\theta^T \\mathbf{x}, x_0 =1$$ in which $\\mathbf{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$, $\\mathbf{\\theta} = \\begin{bmatrix}\\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}$ Cost Function $$J(\\theta^T) = \\frac {1}{2m} \\sum_{i=1}^m (h_\\theta (\\mathbf{x}^{(i)}) - y^{(i)} )^2$$ Gredient Descent Algorithm $$\\begin{aligned} \\text{repeat until convergence: } \\lbrace \u0026 \\\\ \\theta_j := \u0026 \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}\\left((h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\\right) \\ \\rbrace\u0026\\end{aligned}$$ Feature Scalling Get every feature into approximately [-1, 1]. Just normalize all the parameters :) $x_i := \\frac {xi-\\mu}{\\sigma}$ Do not apply to $x_0$ Learning Rate Not too big(fail to converge), not too small(too slow) Polynormal Regression Use feature scalling. (Somewhat like normalizing dimension) ","date":"2016-01-30","objectID":"/2016-01-30-multivariate-linear-regression/:1:0","tags":["machine learning","linear regression","coursera"],"title":"Machine Learning Notes(3)-Multivariate Linear Regression","uri":"/2016-01-30-multivariate-linear-regression/"},{"categories":null,"content":"Normal Equation Set $\\mathbf{X} = \\begin{bmatrix}\\mathbf{x_1} \u0026 \\mathbf{x_2} \u0026 \\cdots \u0026 \\mathbf{x_m} \\end{bmatrix}$, $\\mathbf{y^T} = \\begin{bmatrix} y_1 \u0026 y_2 \u0026 \\cdots \u0026 y_m \\end{bmatrix}$ as our training data: If $\\exists \\mathbf{\\theta_h} \\forall \\mathbf{x_i} \\in \\mathbf{X} (\\theta_h^T \\mathbf{x} = y^{(i)})$, then $\\mathbf{\\theta_h}$ should be our perfect fit of the hypothesis. However, in most cases this fit doesn’t exist, so we should find a set of $\\mathbf{\\theta}$ as close as the best fit. The question above equals solving this set of linear equations: $\\mathbf{X^T} \\mathbf{\\theta} = \\mathbf{y}$, while in most cases there’s no solution. Instead we try to find a $\\mathbf{\\theta}$ let $\\mathbf{X^T} \\mathbf{\\theta}$ as close as $\\mathbf{y}$. Let $\\Delta = |\\mathbf{y} - \\mathbf{X^T} \\mathbf{\\theta}|$, we need this $\\Delta$ to be as small as possible. Use a little imagination we could get the right answer: In this case, the right $\\mathbf{\\theta}$ would let $\\mathbf{X^T} \\mathbf{\\theta}$ be the projection of $\\mathbf{y}$ to the column space of $\\mathbf{X^T}$, which means $\\Delta$ vertical to the column space of $\\mathbf{X^T}$, which means: $$\\mathbf{X} \\Delta = \\mathbf{0}$$ so $\\mathbf{X} (\\mathbf{y} - \\mathbf{X^T} \\mathbf{\\theta}) = \\mathbf{0}$, so $\\mathbf{\\theta} = {(\\mathbf{X}\\mathbf{X^T})}^{-1} \\mathbf{X}\\mathbf{y}$ Notice our $\\mathbf{X}$ is a little different from the course video, in which the $x_i$ are arranged parallel while our $x_i$ are arranged vertically. Feature scalling is not necessary here. Gradient Descent vs Normal Equation Just know Gradient Descent works well even when n is very large, while Normal Equation gets slow since it needs a lot calculation solving ${(\\mathbf{X}\\mathbf{X^T})}^{-1}$ ","date":"2016-01-30","objectID":"/2016-01-30-multivariate-linear-regression/:2:0","tags":["machine learning","linear regression","coursera"],"title":"Machine Learning Notes(3)-Multivariate Linear Regression","uri":"/2016-01-30-multivariate-linear-regression/"},{"categories":null,"content":"This is my notes for the open course Machine Learning from coursera. ","date":"2016-01-29","objectID":"/2016-01-29-model-and-cost-function/:0:0","tags":["machine learning","coursera"],"title":"ML 笔记 2 - A Linear Regression Example","uri":"/2016-01-29-model-and-cost-function/"},{"categories":null,"content":"Model and Cost Function - A Linear Regression Example Hypothesis: $$h_\\theta(x) = \\theta_0 + {\\theta_1}x$$ Cost function: $$J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)} )^2$$ Basically this function is derived from the maximum likelihood estimation of a set of $\\theta_a, \\theta_b \\sim N(0, \\sigma^2)$ $\\exists J(\\theta_a, \\theta_b) (J(\\theta_a, \\theta_b) = min \\lbrace J(\\theta_0, \\theta_1) \\rbrace ) \\to \\theta_a, \\theta_b$ is the best fit of hypothesis ","date":"2016-01-29","objectID":"/2016-01-29-model-and-cost-function/:1:0","tags":["machine learning","coursera"],"title":"ML 笔记 2 - A Linear Regression Example","uri":"/2016-01-29-model-and-cost-function/"},{"categories":null,"content":"Parameter Learning - Minimize Cost Function Gradient descent algorithm repeat until convergence:$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$$ Notifications $\\alpha$: learning rate. The larger $\\alpha$ is, the faster the algorithm will be(but rougher, or even fail to convergence) $\\theta_0, \\theta_1$ should be updated simultaneously(using multiple temp var should work!) Gradient descent can converge to a local minimum even with the learning rate fixed, as the step will automatically become smaller ","date":"2016-01-29","objectID":"/2016-01-29-model-and-cost-function/:2:0","tags":["machine learning","coursera"],"title":"ML 笔记 2 - A Linear Regression Example","uri":"/2016-01-29-model-and-cost-function/"},{"categories":null,"content":"Gradient Descent For Linear Regression $$\\begin{aligned} \\text{repeat until convergence: } \\lbrace \u0026 \\\\ \\theta_0 := \u0026 \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\ \\theta_1 := \u0026 \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}\\left((h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}\\right) \\ \\rbrace\u0026\\end{aligned}$$ The $J(\\theta_0, \\theta_1)$ is a convex function, which means it has only one global minimun, which means gradient descent will always hit the best fit “Batch” Gradient Descent: “Batch” means the algo is trained from all the samples every time ","date":"2016-01-29","objectID":"/2016-01-29-model-and-cost-function/:3:0","tags":["machine learning","coursera"],"title":"ML 笔记 2 - A Linear Regression Example","uri":"/2016-01-29-model-and-cost-function/"},{"categories":null,"content":"This is my notes for the open course Machine Learning from coursera. ","date":"2016-01-29","objectID":"/2016-01-29-supervised-learning/:0:0","tags":["machine learning","coursera"],"title":"ML 笔记 1 - Supervised and Unsupervised Learning","uri":"/2016-01-29-supervised-learning/"},{"categories":null,"content":"Supervised learning Model given a set of data assigned with special features(experience) build a model through learning algorithm(task) predict the features through given data using the model built(performance) Regression problem predict through consecutive data Classification problem pretict between discrete data sets learning from multiple(even infinite) featurea as parameters ","date":"2016-01-29","objectID":"/2016-01-29-supervised-learning/:1:0","tags":["machine learning","coursera"],"title":"ML 笔记 1 - Supervised and Unsupervised Learning","uri":"/2016-01-29-supervised-learning/"},{"categories":null,"content":"Unsupervised learning Model given a set of data(with no features) build a model through learning algorithm like cluster, etc. classify though the model built, etc. (which means they might have some same features) Cocktail party problem learning from a combination of different audio sources(in different ways, at least 2 ways to identify) separate different sources from the combiniton ","date":"2016-01-29","objectID":"/2016-01-29-supervised-learning/:2:0","tags":["machine learning","coursera"],"title":"ML 笔记 1 - Supervised and Unsupervised Learning","uri":"/2016-01-29-supervised-learning/"},{"categories":null,"content":"some cpp fundamentals ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-2/:0:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(2) - 栈和队列继承链表","uri":"/2016-01-22-cpp-fundamental-2/"},{"categories":null,"content":"栈 #include \u003ciostream\u003e #include \"LinkedList_simplified.h\" using namespace std; template \u003ctypename T\u003e class xStack : public LinkedList\u003cT\u003e //don't miss \u003cT\u003e here { public: T\u0026 gettop() {return LinkedList\u003cT\u003e::begin();} void push(T rhs) {LinkedList\u003cT\u003e::insert(0, rhs);} bool pop(T\u0026 popout) { if (LinkedList\u003cT\u003e::size() == 1) { cout \u003c\u003c \"error:Already Empty\" \u003c\u003c endl; return false; } else { popout = LinkedList\u003cT\u003e::begin(); LinkedList\u003cT\u003e::erase(1); return true; } } }; 栈的实现是通过对单链表的继承完成的, 被继承的类称为基类. 注意5行的基类是一个模版类 在继承类中调用基类的成员最好统一加上域解析符以避免混淆 public继承表明基类的public成员在派生类中也是public成员, 故还可以不用域解析符而通过this指针来调用基类方法,见queue的实现 ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-2/:1:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(2) - 栈和队列继承链表","uri":"/2016-01-22-cpp-fundamental-2/"},{"categories":null,"content":"队列 #include \u003ciostream\u003e #include \"LinkedList_simplified.h\" using namespace std; template \u003ctypename T\u003e class xQueue : public LinkedList\u003cT\u003e //don't miss \u003cT\u003e here { public: void inQueue(T rhs) {this-\u003einsert(this-\u003esize()-1, rhs);} bool deQueue(T\u0026 out) { if (this-\u003ebegin_itr()-\u003enext == NULL) { cout \u003c\u003c \"error:Already Empty\" \u003c\u003c endl; return false; } else { out = this-\u003ebegin_itr()-\u003enext-\u003edata; this-\u003eerase(1); return true; } } }; queue和stack的实现差别仅仅在于push操作和inQueue操作的插入位置不同, 一个在链表头一个在链表尾. 而deQueue操作和pop一致 两者都利用了链表提供的简易迭代器, 这正是为何将其设定为protected的原因. ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-2/:2:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(2) - 栈和队列继承链表","uri":"/2016-01-22-cpp-fundamental-2/"},{"categories":null,"content":"some cpp fundamentals ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:0:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"类的基本语法 一个具有最基础功能的链表接口如下： template \u003ctypename T\u003e class LinkedList { protected: void init() {} //私有初始化函数 struct Node{} //链节点 int ListSize; //链长度 Node* head; //头指针 Node* locate(int index); //简易迭代器 Node* begin_itr() {return locate(0);} //返回头节点指针, 内联 Node* end_itr() {return locate(ListSize - 1);}//返回尾节点指针, 内联 public: LinkedList() {} //构造函数 LinkedList(const LinkedList \u0026 rhs) {} //拷贝构造函数 ~LinkedList() //析构函数 LinkedList\u0026 operator=(LinkedList\u0026 rhs); //操作符重载 int size() const {return ListSize;} //链长度, 内联 T\u0026 index(int index) {return locate(index)-\u003edata;} //索引函数, 内联 T\u0026 begin() {return begin_itr()-\u003enext-\u003edata;} //取头元素, 内联 T\u0026 end() {return end_itr()-\u003edata;} //取尾元素, 内联 void insert(int index, const T\u0026 rhs); //插入 void erase(int index); //删除 bool isempty() const {} //判空 void clear() {} //清空 }; 基本结构：如以上代码所示，类的成员由数据和方法组成 数据封装：public和private/protected表示了类的可见性，标示为private/protected的成员只能由类本身的方法所访问， 而public可以被任何类的任何方法访问 const变量和方法：int size() const {}中的const限制了该方法的权限，使得它只能访问类数据成员而不能修改它们.同理,19行的size()使得例如LinkedList A.size() = x;这样的代码非法，即size只可访问不可修改 ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:1:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"链节点Node struct Node { T data; Node* next; Node(const T\u0026 d=T(), Node* n=NULL) :data(d), next(n) {} }; Node结构由数据(类型)T， 次节点指针next以及重载的Node的构造函数组成 所谓构造函数，是指在实例化一个class或structure的时候自动加载的函数，包含了初始化的各项默认参数(第6行中的T()和NULL即为默认参数)，同时亦可通过重载手动指定参数，见下 第7行是一个通过初始化列表(initializer list)来手动指定初始化参数的方法，其将Node初始化时传入的参数d和n传给成员数据data和next ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:2:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"初始化函数init void init() { head = new Node; head-\u003enext = NULL; ListSize = 1; } 在LinkedList实例化之后自动赋予内部变量默认值， 不过由于它是私有变量所以需要在外部利用构造函数调用它，见构造函数和析构函数部分 ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:3:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"构造函数,复制构造函数,析构函数和operator= explicit LinkedList() {init();} LinkedList(LinkedList\u0026 rhs); ~LinkedList() { cout \u003c\u003c \"LinkedList:Destructor...\"; clear(); delete head; } LinkedList\u0026 operator=(LinkedList\u0026 rhs); 构造函数的函数名和类名一致，而析构函数只是在类名之前加上一个～符号。其中第二个构造函数称为复制构造函数, 是为了手动指定从另一个链表初始化的方法, 两者都可视为对默认隐式构造函数的重载 explicit参数是为了禁止隐式类型转换带来不必要的麻烦, 复制构造函数没有使用是因为为了下面实现A b = a这样的句法. 如果加上explici则该句式是违法的 重载的析构函数在main执行完毕之后自动调用，这里利用输出字符串来判断其是否成功启动了 关于重载默认构造函数的重要性, 特别是返回指针类成员时的必要性, 用以下实例来说明 //一个暴露默认构造函数缺陷的例子 class IntCell { public: explicit IntCell(int initialValue = 0) {storedValue = new int(initialValue);} int read() const {return *storedValue;} void write(int x) {*storedValue = x;} private: int* storedValue; }; int f() { IntCell a(2); IntCell b = a; IntCell c; c = b; a.write(4); cout \u003c\u003c a.read( ) \u003c\u003c endl \u003c\u003c b.read( ) \u003c\u003c endl \u003c\u003c c.read( ) \u003c\u003c endl; return 0; } 对以上代码, 该实现将会输出三个4, 而我们想要的结果仅仅是a为4. 这是因为内部数据成员是一个int指针, 而默认赋值过程使得b,c都获得了指向同一个int的指针, 故改变该int导致所有指向该数的指针值对应变化. 要解决此问题, 需要重载operator=以及复制构造函数, 见下 在初始化时赋值调用的是复制构造函数, 而在调用赋值操作符的时候重载的=函数 //改进默认构造函数的手段 explicit IntCell(const IntCell\u0026 rhs) //对应IntCell b = a或IntCell b(a) {storedValue = new int(*rhs.storedValue);} IntCell \u0026 operator= ( const IntCell \u0026 rhs ) // 对应 b = a { if (this != \u0026rhs) {*storedValue = *rhs.storedValue;} return *this; } 因此重载的复制构造函数和operator=应该实现深复制, 见下 //deep copy, discard explicit or assignment upon initializion is unable LinkedList(LinkedList\u0026 rhs) { init(); for (int i = 0; i \u003c rhs.size() - 1; ++i) {insert(i, rhs.index(i+1));} } LinkedList\u0026 operator=(LinkedList\u0026 rhs) { if (this != \u0026rhs) { for (int i = 0; i \u003c rhs.size() - 1; ++i) {insert(i, rhs.index(i+1));} } return *this; } 之所以复制构造函数和operator=的形参都没有加const, 是因为使用到的insert并不是const函数, 直接使用会导致违法, 想要合法必须再定义一个const的insert函数,为了简单起见这里没有应用. ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:4:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"locate函数的实现:类和接口的分离 template \u003ctypename T\u003e typename LinkedList\u003cT\u003e::Node* LinkedList\u003cT\u003e::locate(int index) { if (index \u003c 0) {cout \u003c\u003c \"error:index \u003c 0\" \u003c\u003cendl; return locate(0);} if (index \u003e ListSize - 1) {cout \u003c\u003c \"error:index overflow\" \u003c\u003cendl; return locate(ListSize - 1 );} Node* itr = head; for (int i = 0; i \u003c index; ++i) {itr = itr-\u003enext;} return itr; } locate函数返回Node指针 模板类主要应用在链表数据节点的数据类型上。此外，实现在类的声明之外, 需要在返回值类型和函数类型之前加上域解析符，表示它们是类LinkedList的成员 对输入的index做出边界溢出判定，若超过最后一个则返回链尾，若小于0则返回头节点 迭代器一般封装在private/protected域之内, 而给用户访问/修改元素需要另外设计接口, 即本例的index, begin和end方法. 完整的public迭代器的实现比较复杂, 需要对权限作严格限制, 见带迭代器的单链表 ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:5:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"其他方法的实现 //从index后插入一个节点 template \u003ctypename T\u003e void LinkedList\u003cT\u003e::insert(int index, const T\u0026 rhs) { Node *p = locate(index); Node *newNode = new Node(rhs, p-\u003enext); p-\u003enext = newNode; ListSize++; } //删除index后一个节点 template \u003ctypename T\u003e void LinkedList\u003cT\u003e::erase(int index) { Node *p = locate(index); locate(index - 1)-\u003enext = p-\u003enext; delete p; ListSize--; } ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:6:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"测试文件 //这是一个类实例 #include \u003cstring\u003e #define CLASS_EXAMPLE_H #ifdef CLASS_EXAMPLE_H using namespace std; class Player { private: int rank; string name; public: Player(int r = 0, const string \u0026 n = \"#name\") : rank(r), name(n) {} int getrank() const {return rank;} string getname() const {return name;} }; #endif 以上用于测试的类实例代表一个游戏玩家，包括了玩家的name和排名rank，并且提供了getrank()和getname()方法用于取得名字和排名。 //测试文件 #include \u003ciostream\u003e #include \"../Class_Example.h\" #include \"LinkedList_simplified.h\" using namespace std; int main(int argc, char const *argv[]) { const int listcap = 5; Player playerlist[listcap] = { Player(1, \"Cookiezi\"), Player(2, \"WhiteWolf\"), Player(3, \"rrtyui\"), Player(4, \"powergame\"), Player(5, \"SiLviA\"), }; LinkedList\u003cPlayer\u003e players; cout \u003c\u003c \"-----------insert test-----------\" \u003c\u003cendl; for (int i = 0; i \u003c listcap; ++i) { players.insert(players.size() - 1, playerlist[i]); } for (int i = 0; i \u003c listcap; ++i) { cout \u003c\u003c players.index(i+1).getname() \u003c\u003c \", \" \u003c\u003c endl; } cout \u003c\u003c \"-----------copy test-----------\" \u003c\u003cendl; LinkedList\u003cPlayer\u003e players_copy_1 = players; for (int i = 0; i \u003c listcap; ++i) { cout \u003c\u003c players_copy_1.index(i+1).getname() \u003c\u003c \", \" \u003c\u003c endl; } cout \u003c\u003c \"-----------operator= test-----------\" \u003c\u003cendl; LinkedList\u003cPlayer\u003e players_copy_2; players_copy_2 = players_copy_1; for (int i = 0; i \u003c listcap; ++i) { cout \u003c\u003c players_copy_2.index(i+1).getname() \u003c\u003c \", \" \u003c\u003c endl; } return 0; } 将24行的players.size() - 1改成0，即可从链头插入，那么输出序列将是逆置的 ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:7:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"},{"categories":null,"content":"输出结果 -----------insert test----------- Cookiezi, WhiteWolf, rrtyui, powergame, SiLviA, -----------copy test----------- Cookiezi, WhiteWolf, rrtyui, powergame, SiLviA, -----------operator= test----------- Cookiezi, WhiteWolf, rrtyui, powergame, SiLviA, LinkedList:Destructor...LinkedList:Destructor...LinkedList:Destructor... Process finished with exit code 0 ","date":"2016-01-22","objectID":"/2016-01-22-cpp-fundamental-1/:8:0","tags":["cpp"],"title":"利用链表ADT学习C++面向对象(1) - 基础知识","uri":"/2016-01-22-cpp-fundamental-1/"}]